<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-10 03:12</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251110_0312</div>
    <div class="row"><div class="card">
<div class="title">TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models</div>
<div class="meta-line">Authors: Yao Xiao, Qiqian Fu, Heyi Tao, Yuqun Wu, Zhen Zhu, Derek Hoiem</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research, 2025</div>
<div class="meta-line">First: 2025-05-29T17:59:59+00:00 · Latest: 2025-11-06T18:59:57+00:00</div>
<div class="meta-line">Comments: Published in TMLR, with a J2C Certification</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.23769v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.23769v2">PDF</a> · <a href="https://github.com/avaxiao/TextRegion">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image-text models excel at image-level tasks but struggle with detailed
visual understanding. While these models provide strong visual-language
alignment, segmentation models like SAM2 offer precise spatial boundaries for
objects. To this end, we propose TextRegion, a simple, effective, and
training-free framework that combines the strengths of image-text models and
SAM2 to generate powerful text-aligned region tokens. These tokens enable
detailed visual understanding while preserving open-vocabulary capabilities.
They can be directly applied to various downstream tasks, including open-world
semantic segmentation, referring expression comprehension, and grounding. We
conduct extensive evaluations and consistently achieve superior or competitive
performance compared to state-of-the-art training-free methods. Additionally,
our framework is compatible with many image-text models, making it highly
practical and easily extensible as stronger models emerge. Code is available
at: https://github.com/avaxiao/TextRegion.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Image-text models excel at image-level tasks but struggle with detailed visual understanding.</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Inference Schedules for Masked Diffusion Models</div>
<div class="meta-line">Authors: Sitan Chen, Kevin Cong, Jerry Li</div>
<div class="meta-line">First: 2025-11-06T18:38:24+00:00 · Latest: 2025-11-06T18:38:24+00:00</div>
<div class="meta-line">Comments: 33 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.04647v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.04647v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A major bottleneck of standard auto-regressive large language models is that
their inference process is inherently sequential, resulting in very long and
costly inference times. To circumvent this, practitioners proposed a class of
language models called diffusion language models, of which the masked diffusion
model (MDM) is the most successful. The MDM is able to sample tokens
out-of-order and, ostensibly, many tokens at once and in parallel. However,
there is very limited rigorous understanding of how much parallel sampling
these models can perform without noticeable degradation in their sampling
performance. Prior work of Li and Cai obtained some preliminary bounds, but
these are not tight for many natural classes of distributions. In this work, we
give a new, exact characterization of the expected divergence between the true
distribution and the sampled distribution, for any distribution and any
unmasking schedule for the sampler, showing an elegant connection to the theory
of univariate function approximation.
  By leveraging this connection, we then attain a number of novel lower and
upper bounds for this problem. While the connection to function approximation
in principle gives the optimal unmasking schedule for any distribution, we show
that it is in general impossible to compete with it without strong a priori
knowledge of the distribution, even in seemingly benign settings. However, we
also demonstrate new upper bounds and new sampling schedules in terms of
well-studied information-theoretic properties of the base distribution, namely,
its total correlation and dual total correlation, which show that in some
natural settings, one can sample in $O(log n)$ steps without any visible loss
in performance, where $n$ is the total sequence length.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">A major bottleneck of standard auto-regressive large language models is that their inference process is inherently sequential, resulting in very long and costly inference times.</div>
</details>
</div>
<div class="card">
<div class="title">SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</div>
<div class="meta-line">Authors: Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar</div>
<div class="meta-line">First: 2025-11-05T00:38:31+00:00 · Latest: 2025-11-06T18:27:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03092v2">Abs</a> · <a href="http://arxiv.org/pdf/2511.03092v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches.</div>
</details>
</div>
<div class="card">
<div class="title">PixCLIP: Achieving Fine-grained Visual Language Understanding via   Any-granularity Pixel-Text Alignment Learning</div>
<div class="meta-line">Authors: Yicheng Xiao, Yu Chen, Haoxuan Ma, Jiale Hong, Caorui Li, Lingxiang Wu, Haiyun Guo, Jinqiao Wang</div>
<div class="meta-line">First: 2025-11-06T17:54:12+00:00 · Latest: 2025-11-06T17:54:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.04601v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.04601v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model&#x27;s fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP&#x27;s text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP&#x27;s original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While the Contrastive Language-Image Pretraining(CLIP) model has achieved remarkable success in a variety of downstream vison language understanding tasks, enhancing its capability for fine-grained image-text alignment remains an active research focus.</div>
</details>
</div>
<div class="card">
<div class="title">Are language models aware of the road not taken? Token-level uncertainty   and hidden state dynamics</div>
<div class="meta-line">Authors: Amir Zur, Atticus Geiger, Ekdeep Singh Lubana, Eric Bigelow</div>
<div class="meta-line">First: 2025-11-06T16:43:25+00:00 · Latest: 2025-11-06T16:43:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.04527v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.04527v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model&#x27;s uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model&#x27;s future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify.</div>
</details>
</div>
<div class="card">
<div class="title">RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within   Structured Tables</div>
<div class="meta-line">Authors: Nikhil Abhyankar, Purvi Chaurasia, Sanchit Kabra, Ananya Srivastava, Vivek Gupta, Chandan K. Reddy</div>
<div class="meta-line">First: 2025-11-06T16:10:03+00:00 · Latest: 2025-11-06T16:10:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.04491v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.04491v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models&#x27; (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models&#x27; (LLMs) reasoning abilities.</div>
</details>
</div>
<div class="card">
<div class="title">RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive   Text-to-Video Generation</div>
<div class="meta-line">Authors: Xiangjun Zhang, Litong Gong, Yinglin Zheng, Yansong Liu, Wentao Jiang, Mingyi Xu, Biao Wang, Tiezheng Ge, Ming Zeng</div>
<div class="meta-line">First: 2025-11-06T12:42:03+00:00 · Latest: 2025-11-06T12:42:03+00:00</div>
<div class="meta-line">Comments: 17 pages, 16 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.04317v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.04317v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rise-t2v.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user&#x27;s intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Most text-to-video(T2V) diffusion models depend on pre-trained text encoders for semantic alignment, yet they often fail to maintain video quality when provided with concise prompts rather than well-designed ones.</div>
</details>
</div>
<div class="card">
<div class="title">GASP: Efficient Black-Box Generation of Adversarial Suffixes for   Jailbreaking LLMs</div>
<div class="meta-line">Authors: Advik Raj Basani, Xiao Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-11-21T14:00:01+00:00 · Latest: 2025-11-06T12:34:22+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025. Project page and demos:
  https://air-ml.org/project/gasp/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2411.14133v3">Abs</a> · <a href="http://arxiv.org/pdf/2411.14133v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://air-ml.org/project/gasp/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs have shown impressive capabilities across various natural language
processing tasks, yet remain vulnerable to input prompts, known as jailbreak
attacks, carefully designed to bypass safety guardrails and elicit harmful
responses. Traditional methods rely on manual heuristics but suffer from
limited generalizability. Despite being automatic, optimization-based attacks
often produce unnatural prompts that can be easily detected by safety filters
or require high computational costs due to discrete token optimization. In this
paper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel
automated framework that can efficiently generate human-readable jailbreak
prompts in a fully black-box setting. In particular, GASP leverages latent
Bayesian optimization to craft adversarial suffixes by efficiently exploring
continuous latent embedding spaces, gradually optimizing the suffix prompter to
improve attack efficacy while balancing prompt coherence via a targeted
iterative refinement procedure. Through comprehensive experiments, we show that
GASP can produce natural adversarial prompts, significantly improving jailbreak
success over baselines, reducing training times, and accelerating inference
speed, thus making it an efficient and scalable solution for red-teaming LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLMs have shown impressive capabilities across various natural language processing tasks, yet remain vulnerable to input prompts, known as jailbreak attacks, carefully designed to bypass safety guardrails and elicit harmful responses.</div>
</details>
</div>
<div class="card">
<div class="title">Advanced Sign Language Video Generation with Compressed and Quantized   Multi-Condition Tokenization</div>
<div class="meta-line">Authors: Cong Wang, Zexuan Deng, Zhiwei Jiang, Yafeng Yin, Fei Shen, Zifeng Cheng, Shiping Ge, Shiwei Gan, Qing Gu</div>
<div class="meta-line">First: 2025-06-19T02:56:06+00:00 · Latest: 2025-11-06T11:55:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.15980v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.15980v2">PDF</a> · <a href="https://github.com/umnooob/signvip/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sign Language Video Generation (SLVG) seeks to generate identity-preserving
sign language videos from spoken language texts. Existing methods primarily
rely on the single coarse condition (\eg, skeleton sequences) as the
intermediary to bridge the translation model and the video generation model,
which limits both the naturalness and expressiveness of the generated videos.
To overcome these limitations, we propose SignViP, a novel SLVG framework that
incorporates multiple fine-grained conditions for improved generation fidelity.
Rather than directly translating error-prone high-dimensional conditions,
SignViP adopts a discrete tokenization paradigm to integrate and represent
fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP
contains three core components. (1) Sign Video Diffusion Model is jointly
trained with a multi-condition encoder to learn continuous embeddings that
encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization
(FSQ) Autoencoder is further trained to compress and quantize these embeddings
into discrete tokens for compact representation of the conditions. (3)
Multi-Condition Token Translator is trained to translate spoken language text
to discrete multi-condition tokens. During inference, Multi-Condition Token
Translator first translates the spoken language text into discrete
multi-condition tokens. These tokens are then decoded to continuous embeddings
by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion
Model to guide video generation. Experimental results show that SignViP
achieves state-of-the-art performance across metrics, including video quality,
temporal coherence, and semantic fidelity. The code is available at
https://github.com/umnooob/signvip/.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts.</div>
</details>
</div>
<div class="card">
<div class="title">CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the   Biomedical Field</div>
<div class="meta-line">Authors: Doria Bonzi, Alexandre Guiggi, Frédéric Béchet, Carlos Ramisch, Benoit Favre</div>
<div class="meta-line">First: 2025-11-05T13:02:06+00:00 · Latest: 2025-11-06T11:06:10+00:00</div>
<div class="meta-line">Comments: Preprint submitted to LREC 2026 (under review) To access the dataset,
  see https://github.com/bonzid/CareMedEval</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03441v2">Abs</a> · <a href="http://arxiv.org/pdf/2511.03441v2">PDF</a> · <a href="https://github.com/bonzid/CareMedEval">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Critical appraisal of scientific literature is an essential skill in the biomedical field.</div>
</details>
</div>
<div class="card">
<div class="title">Datasets, Documents, and Repetitions: The Practicalities of Unequal Data   Quality</div>
<div class="meta-line">Authors: Alex Fang, Hadi Pouransari, Matt Jordan, Alexander Toshev, Vaishaal Shankar, Ludwig Schmidt, Tom Gunter</div>
<div class="meta-line">First: 2025-03-10T21:51:17+00:00 · Latest: 2025-11-06T09:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.07879v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.07879v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data filtering has become a powerful tool for improving model performance
while reducing computational cost. However, as large language model compute
budgets continue to grow, the limited data volume provided by heavily filtered
and deduplicated datasets will become a practical constraint. In efforts to
better understand how to proceed, we study model performance at various compute
budgets and across multiple pre-training datasets created through data
filtering and deduplication. We find that, given appropriate modifications to
the training recipe, repeating existing aggressively filtered datasets for up
to ten epochs can outperform training on the ten times larger superset for a
single epoch across multiple compute budget orders of magnitude. While this
finding relies on repeating the dataset for many epochs, we also investigate
repeats within these datasets at the document level. We find that not all
documents within a dataset are equal, and we can create better datasets
relative to a token budget by explicitly manipulating the counts of individual
documents. We conclude by arguing that even as large language models scale,
data filtering remains an important direction of research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Data filtering has become a powerful tool for improving model performance while reducing computational cost.</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Models Hallucinate More: Factuality-Aware Reinforcement   Learning for Large Reasoning Models</div>
<div class="meta-line">Authors: Junyi Li, Hwee Tou Ng</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-30T14:23:32+00:00 · Latest: 2025-11-06T09:04:26+00:00</div>
<div class="meta-line">Comments: accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.24630v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.24630v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have significantly advanced in reasoning tasks
through reinforcement learning (RL) optimization, achieving impressive
capabilities across various challenging benchmarks. However, our empirical
analysis reveals a critical drawback: reasoning-oriented RL fine-tuning
significantly increases the prevalence of hallucinations. We theoretically
analyze the RL training dynamics, identifying high-variance gradient,
entropy-induced randomness, and susceptibility to spurious local optima as key
factors leading to hallucinations. To address this drawback, we propose
Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL
fine-tuning algorithm incorporating explicit factuality verification at each
reasoning step. FSPO leverages automated verification against given evidence to
dynamically adjust token-level advantage values, incentivizing factual
correctness throughout the reasoning process. Experiments across mathematical
reasoning and hallucination benchmarks using Qwen2.5 and Llama models
demonstrate that FSPO effectively reduces hallucinations while enhancing
reasoning accuracy, substantially improving both reliability and performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks.</div>
</details>
</div>
<div class="card">
<div class="title">Training Large Language Models To Reason In Parallel With Global Forking   Tokens</div>
<div class="meta-line">Authors: Sheng Jia, Xiao Wang, Shiva Prasad Kasiviswanathan</div>
<div class="meta-line">First: 2025-10-01T02:48:39+00:00 · Latest: 2025-11-06T07:00:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.05132v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.05132v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although LLMs have demonstrated improved performance by scaling parallel
test-time compute, doing so relies on generating reasoning paths that are both
diverse and accurate. For challenging problems, the forking tokens that trigger
diverse yet correct reasoning modes are typically deep in the sampling tree.
Consequently, common strategies to encourage diversity, such as temperature
scaling, encounter a worsened trade-off between diversity and accuracy.
Motivated by this challenge, we treat parallel reasoning as a
set-of-next-token-prediction problem, and incorporate a set-based global loss
into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching
between our global forking tokens and unique reasoning traces. We observe that,
while naive fine-tuning with multiple reasoning traces collapses these unique
reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT),
preserves these modes and produces emergent global forking tokens. Experiments
on multiple reasoning benchmarks show that our SSFT consistently outperforms
SFT under both Pass@1 and Cons@k metrics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate.</div>
</details>
</div>
<div class="card">
<div class="title">Poutine: Vision-Language-Trajectory Pre-Training and Reinforcement   Learning Post-Training Enable Robust End-to-End Autonomous Driving</div>
<div class="meta-line">Authors: Luke Rowe, Rodrigue de Schaetzen, Roger Girgis, Christopher Pal, Liam Paull</div>
<div class="meta-line">First: 2025-06-12T19:14:00+00:00 · Latest: 2025-11-06T05:07:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.11234v4">Abs</a> · <a href="http://arxiv.org/pdf/2506.11234v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Maintaining good driving behavior in out-of-distribution scenarios remains a
critical challenge in autonomous driving. A promising direction is to leverage
the generalist knowledge and reasoning capabilities of large-language models by
treating unusual driving scenarios as a logical reasoning task. In this work,
we present Poutine, a method that uses an off-the-shelf 3B-parameter
vision-language model (VLM) - without any additional components - to achieve
robust end-to-end autonomous driving via a simple and scalable training recipe.
To learn strong base driving capabilities, we first train Poutine-Base using
self-supervised next-token prediction over vision, language, and trajectory
(VLT) tokens, leveraging both nominal and long-tail driving data. In the second
stage, we fine-tune Poutine-Base using Group Relative Policy Optimization
(GRPO) with a small set of human preference-labeled examples. We evaluated our
approach on the Waymo end-to-end driving benchmark curated for long-tail
scenarios. The final Poutine model achieves an RFS of 7.99 on the test set,
placing 1st in the 2025 Waymo Vision-Based End-to-End Driving Challenge by a
significant margin. Our results suggest that handcrafted tokenizers or custom
architectural components added to base VLMs in prior work are not necessary to
achieve strong driving performance. Instead, this work highlights the potential
of scalable VLT pretraining combined with lightweight RL fine-tuning to enable
robust and generalizable autonomous driving.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Maintaining good driving behavior in out-of-distribution scenarios remains a critical challenge in autonomous driving.</div>
</details>
</div>
<div class="card">
<div class="title">GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient   Domain Generalization</div>
<div class="meta-line">Authors: Mahmoud Soliman, Omar Abdelaziz, Ahmed Radwan, Anand, Mohamed Shehata</div>
<div class="meta-line">First: 2025-11-06T03:16:08+00:00 · Latest: 2025-11-06T03:16:08+00:00</div>
<div class="meta-line">Comments: 6 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.04008v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.04008v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Domain generalization (DG) seeks robust Vision Transformer (ViT) performance
on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;
standard fine-tuning is costly and can impair generalization. We propose
GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a
Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead
of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,
SAGE) operates on inter-patch graphs to dynamically assign patches to
specialized experts. This context-aware GNN routing leverages inter-patch
relationships for better adaptation to domain shifts. GNN-MoE achieves
state-of-the-art or competitive DG benchmark performance with high parameter
efficiency, highlighting the utility of graph-based contextual routing for
robust, lightweight DG.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Domain generalization (DG) seeks robust Vision Transformer (ViT) performance on unseen domains.</div>
</details>
</div>
<div class="card">
<div class="title">Control Barrier Function for Aligning Large Language Models</div>
<div class="meta-line">Authors: Yuya Miyaoka, Masaki Inoue</div>
<div class="meta-line">First: 2025-11-05T02:12:59+00:00 · Latest: 2025-11-06T03:06:07+00:00</div>
<div class="meta-line">Comments: This work is an extenede version of arXiv:2408.15625 and has been
  submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03121v2">Abs</a> · <a href="http://arxiv.org/pdf/2511.03121v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation.</div>
</details>
</div>
<div class="card">
<div class="title">Learning-at-Criticality in Large Language Models for Quantum Field   Theory and Beyond</div>
<div class="meta-line">Authors: Xiansheng Cai, Sihan Hu, Tao Wang, Yuan Huang, Pan Zhang, Youjin Deng, Kun Chen</div>
<div class="meta-line">First: 2025-06-04T08:35:05+00:00 · Latest: 2025-11-06T03:04:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.03703v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.03703v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fundamental physics often confronts complex symbolic problems with few
guiding exemplars or established principles. While artificial intelligence (AI)
offers promise, its typical need for vast datasets to learn from hinders its
use in these information-scarce frontiers. We introduce learning at criticality
(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models
(LLMs) to a sharp learning transition, addressing this information scarcity. At
this transition, LLMs achieve peak generalization from minimal data,
exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic
reasoning. To elucidate this peak, we analyze a minimal concept-network model
(CoNet) designed to capture the essence of how LLMs might link tokens. Trained
on a single exemplar, this model also undergoes a sharp learning transition.
This transition exhibits hallmarks of a second-order phase transition, notably
power-law distributed solution path lengths. At this critical point, the system
maximizes a ``critical thinking pattern&quot; crucial for generalization, enabled by
the underlying scale-free exploration. This suggests LLMs reach peak
performance by operating at criticality, where such explorative dynamics enable
the extraction of underlying operational rules. We demonstrate LaC in quantum
field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a
few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,
significantly outperforming far larger models. LaC thus leverages critical
phenomena, a physical principle, to empower AI for complex, data-sparse
challenges in fundamental physics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Fundamental physics often confronts complex symbolic problems with few guiding exemplars or established principles.</div>
</details>
</div>
<div class="card">
<div class="title">Memory- and Latency-Constrained Inference of Large Language Models via   Adaptive Split Computing</div>
<div class="meta-line">Authors: Mingyu Sung, Vikas Palakonda, Suhwan Im, Sunghwan Moon, Il-Min Kim, Sangseok Yun, Jae-Mo Kang</div>
<div class="meta-line">First: 2025-11-06T02:55:07+00:00 · Latest: 2025-11-06T02:55:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.04002v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.04002v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding.</div>
</details>
</div>
<div class="card">
<div class="title">Pay for The Second-Best Service: A Game-Theoretic Approach Against   Dishonest LLM Providers</div>
<div class="meta-line">Authors: Yuhan Cao, Yu Wang, Sitong Liu, Miao Li, Yixin Tao, Tianxing He</div>
<div class="meta-line">First: 2025-11-02T08:18:20+00:00 · Latest: 2025-11-06T02:40:22+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.00847v3">Abs</a> · <a href="http://arxiv.org/pdf/2511.00847v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread adoption of Large Language Models (LLMs) through Application
Programming Interfaces (APIs) induces a critical vulnerability: the potential
for dishonest manipulation by service providers. This manipulation can manifest
in various forms, such as secretly substituting a proclaimed high-performance
model with a low-cost alternative, or inflating responses with meaningless
tokens to increase billing. This work tackles the issue through the lens of
algorithmic game theory and mechanism design. We are the first to propose a
formal economic model for a realistic user-provider ecosystem, where a user can
iteratively delegate $T$ queries to multiple model providers, and providers can
engage in a range of strategic behaviors. As our central contribution, we prove
that for a continuous strategy space and any $\epsilon\in(0,\frac12)$, there
exists an approximate incentive-compatible mechanism with an additive
approximation ratio of $O(T^{1-\epsilon}\log T)$, and a guaranteed quasi-linear
second-best user utility. We also prove an impossibility result, stating that
no mechanism can guarantee an expected user utility that is asymptotically
better than our mechanism. Furthermore, we demonstrate the effectiveness of our
mechanism in simulation experiments with real-world API settings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The widespread adoption of Large Language Models (LLMs) through Application Programming Interfaces (APIs) induces a critical vulnerability: the potential for dishonest manipulation by service providers.</div>
</details>
</div>
<div class="card">
<div class="title">HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts</div>
<div class="meta-line">Authors: Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying</div>
<div class="meta-line">First: 2025-05-30T15:42:42+00:00 · Latest: 2025-11-06T01:18:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.24722v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.24722v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have shown great success in text modeling tasks
across domains. However, natural language exhibits inherent semantic
hierarchies and nuanced geometric structure, which current LLMs do not capture
completely owing to their reliance on Euclidean operations. Recent studies have
also shown that not respecting the geometry of token embeddings leads to
training instabilities and degradation of generative capabilities. These
findings suggest that shifting to non-Euclidean geometries can better align
language models with the underlying geometry of text. We thus propose to
operate fully in Hyperbolic space, known for its expansive, scale-free, and
low-distortion properties. We thus introduce HELM, a family of HypErbolic Large
Language Models, offering a geometric rethinking of the Transformer-based LLM
that addresses the representational inflexibility, missing set of necessary
operations, and poor scalability of existing hyperbolic LMs. We additionally
introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert
operates in a distinct curvature space to encode more fine-grained geometric
structure from text, as well as a dense model, HELM-D. For HELM-MICE, we
further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,
reduced-KV-cache training and inference. For both models, we develop essential
hyperbolic equivalents of rotary positional encodings and RMS normalization. We
are the first to train fully hyperbolic LLMs at billion-parameter scale, and
evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM
problem-solving, general knowledge, and commonsense reasoning. Our results show
consistent gains from our HELM architectures -- up to 4% -- over popular
Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy
and enhanced reasoning afforded by hyperbolic geometry in large-scale LM
pretraining.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) have shown great success in text modeling tasks across domains.</div>
</details>
</div>
<div class="card">
<div class="title">Direct Semantic Communication Between Large Language Models via Vector   Translation</div>
<div class="meta-line">Authors: Fu-Chun Yang, Jason Eshraghian</div>
<div class="meta-line">First: 2025-11-06T00:43:29+00:00 · Latest: 2025-11-06T00:43:29+00:00</div>
<div class="meta-line">Comments: 9 pages, 1 figure, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03945v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03945v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model&#x27;s
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In multi-agent settings, such as debate, reflection, or tool-calling, large language models (LLMs) pass messages as plain tokens, discarding most latent semantics.</div>
</details>
</div>
<div class="card">
<div class="title">PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive   Error Feedback Agentic-AI</div>
<div class="meta-line">Authors: Athma Narayanan, Mahesh Subedar, Omesh Tickoo</div>
<div class="meta-line">First: 2025-11-06T00:19:47+00:00 · Latest: 2025-11-06T00:19:47+00:00</div>
<div class="meta-line">Comments: Appeared in the Design Automation Conference (DAC) 2025, Workshop
  Poster on June 22, 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03934v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03934v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present an agentic flow consisting of multiple agents that combine specialized LLMs and hardware simulation tools to collaboratively complete the complex task of Register Transfer Level (RTL) generation without human intervention.</div>
</details>
</div>
<div class="card">
<div class="title">Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM   Inference</div>
<div class="meta-line">Authors: Donghyeon Joo, Helya Hosseini, Ramyad Hadidi, Bahar Asgari</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-28T22:32:15+00:00 · Latest: 2025-11-06T00:11:18+00:00</div>
<div class="meta-line">Comments: 20 pages, 9 figures, NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.22913v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.22913v2">PDF</a> · <a href="https://github.com/dhjoo98/mustafar">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We demonstrate that unstructured sparsity significantly improves KV cache
compression for LLMs, enabling sparsity levels up to 70% without compromising
accuracy or requiring fine-tuning. We conduct a systematic exploration of
pruning strategies and find per-token magnitude-based pruning as highly
effective for both Key and Value caches under unstructured sparsity, surpassing
prior structured pruning schemes. The Key cache benefits from prominent outlier
elements, while the Value cache surprisingly benefits from a simple
magnitude-based pruning despite its uniform distribution. KV cache size is the
major bottleneck in decode performance due to high memory overhead for large
context lengths. To address this, we use a bitmap-based sparse format and a
custom attention kernel capable of compressing and directly computing over
compressed caches pruned to arbitrary sparsity patterns, significantly
accelerating memory-bound operations in decode computations and thereby
compensating for the overhead of runtime pruning and compression. Our custom
attention kernel coupled with the bitmap-based format delivers substantial
compression of KV cache upto 45% of dense inference and thereby enables longer
context length and increased tokens/sec throughput of upto 2.23x compared to
dense inference. Our pruning mechanism and sparse attention kernel is available
at https://github.com/dhjoo98/mustafar.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We demonstrate that unstructured sparsity significantly improves KV cache compression for LLMs, enabling sparsity levels up to 70% without compromising accuracy or requiring fine-tuning.</div>
</details>
</div>
<div class="card">
<div class="title">NVIDIA Nemotron Nano V2 VL</div>
<div class="meta-line">Authors: NVIDIA, :, Amala Sanjay Deshmukh, Kateryna Chumachenko, Tuomas Rintamaki, Matthieu Le, Tyler Poon, Danial Mohseni Taheri, Ilia Karmanov, Guilin Liu, Jarno Seppanen, Guo Chen, Karan Sapra, Zhiding Yu, Adi Renduchintala, Charles Wang, Peter Jin, Arushi Goel, Mike Ranzinger, Lukas Voegtle, Philipp Fischer, Timo Roman, Wei Ping, Boxin Wang, Zhuolin Yang, Nayeon Lee, Shaokun Zhang, Fuxiao Liu, Zhiqi Li, Di Zhang, Greg Heinrich, Hongxu, Yin, Song Han, Pavlo Molchanov, Parth Mannan, Yao Xu, Jane Polak Scowcroft, Tom Balough, Subhashree Radhakrishnan, Paris Zhang, Sean Cha, Ratnesh Kumar, Zaid Pervaiz Bhat, Jian Zhang, Darragh Hanley, Pritam Biswas, Jesse Oliver, Kevin Vasques, Roger Waleffe, Duncan Riach, Oluwatobi Olabiyi, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Pritam Gundecha, Khanh Nguyen, Alexandre Milesi, Eugene Khvedchenia, Ran Zilberstein, Ofri Masad, Natan Bagrov, Nave Assaf, Tomer Asida, Daniel Afrimi, Amit Zuker, Netanel Haber, Zhiyu Cheng, Jingyu, Xin, Di, Wu, Nik Spirin, Maryam Moosaei, Roman Ageev, Vanshil Atul Shah, Yuting Wu, Daniel Korzekwa, Unnikrishnan Kizhakkemadam Sreekumar, Wanli Jiang, Padmavathy Subramanian, Alejandra Rico, Sandip Bhaskar, Saeid Motiian, Kedi Wu, Annie Surla, Chia-Chih Chen, Hayden Wolff, Matthew Feinberg, Melissa Corpuz, Marek Wawrzos, Eileen Long, Aastha Jhunjhunwala, Paul Hendricks, Farzan Memarian, Benika Hall, Xin-Yu Wang, David Mosallanezhad, Soumye Singhal, Luis Vega, Katherine Cheung, Krzysztof Pawelec, Michael Evans, Katherine Luna, Jie Lou, Erick Galinkin, Akshay Hazare, Kaustubh Purandare, Ann Guan, Anna Warno, Chen Cui, Yoshi Suhara, Shibani Likhite, Seph Mard, Meredith Price, Laya Sleiman, Saori Kaji, Udi Karpas, Kari Briski, Joey Conway, Michael Lightstone, Jan Kautz, Mohammad Shoeybi, Mostofa Patwary, Jonathen Cohen, Oleksii Kuchaiev, Andrew Tao, Bryan Catanzaro</div>
<div class="meta-line">First: 2025-11-06T00:10:19+00:00 · Latest: 2025-11-06T00:10:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03929v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03929v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Nemotron Nano V2 VL, the latest model of the Nemotron
vision-language series designed for strong real-world document understanding,
long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers
significant improvements over our previous model,
Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major
enhancements in model architecture, datasets, and training recipes. Nemotron
Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and
innovative token reduction techniques to achieve higher inference throughput in
long document and video scenarios. We are releasing model checkpoints in BF16,
FP8, and FP4 formats and sharing large parts of our datasets, recipes and
training code.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks.</div>
</details>
</div>
<div class="card">
<div class="title">AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous   Driving with Adaptive Reasoning and Reinforcement Fine-Tuning</div>
<div class="meta-line">Authors: Zewei Zhou, Tianhui Cai, Seth Z. Zhao, Yun Zhang, Zhiyu Huang, Bolei Zhou, Jiaqi Ma</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-16T17:58:50+00:00 · Latest: 2025-11-05T23:46:20+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025; Website link:https://autovla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.13757v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.13757v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://autovla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Vision-Language-Action (VLA) models have shown promise
for end-to-end autonomous driving by leveraging world knowledge and reasoning
capabilities. However, current VLA models often struggle with physically
infeasible action outputs, complex model structures, or unnecessarily long
reasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies
reasoning and action generation within a single autoregressive generation model
for end-to-end autonomous driving. AutoVLA performs semantic reasoning and
trajectory planning directly from raw visual inputs and language instructions.
We tokenize continuous trajectories into discrete, feasible actions, enabling
direct integration into the language model. For training, we employ supervised
fine-tuning to equip the model with dual thinking modes: fast thinking
(trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning).
To further enhance planning performance and efficiency, we introduce a
reinforcement fine-tuning method based on Group Relative Policy Optimization
(GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive
experiments across real-world and simulated datasets and benchmarks, including
nuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of
AutoVLA in both open-loop and closed-loop settings. Qualitative results
showcase the adaptive reasoning and accurate planning capabilities of AutoVLA
in diverse scenarios.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advancements in Vision-Language-Action (VLA) models have shown promise for end-to-end autonomous driving by leveraging world knowledge and reasoning capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">Exact Expressive Power of Transformers with Padding</div>
<div class="meta-line">Authors: William Merrill, Ashish Sabharwal</div>
<div class="meta-line">Venue: Neurips 2025</div>
<div class="meta-line">First: 2025-05-25T02:52:15+00:00 · Latest: 2025-11-05T23:01:55+00:00</div>
<div class="meta-line">Comments: Neurips 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.18948v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.18948v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain of thought is a natural inference-time method for increasing the
computational power of transformer-based large language models (LLMs), but
comes at the cost of sequential decoding. Are there more efficient alternatives
to expand a transformer&#x27;s expressive power without adding parameters? We
consider transformers with padding tokens as a form of parallelizable test-time
compute. We show that averaging-hard-attention, masked-pre-norm transformers
with polynomial padding recognize precisely the class $\mathsf{FO}$-uniform
$\mathsf{TC}^0$ of extremely parallelizable problems. While the $\mathsf{TC}^0$
upper bound was known, proving a matching lower bound had been elusive.
Further, our novel analysis reveals the precise expanded power of padded
transformers when coupled with another form of inference-time compute, namely
dynamically increasing depth via looping. Our core technical contribution is to
show how padding helps bring the notions of complete problems and reductions,
which have been a cornerstone of classical complexity theory, to the formal
study of transformers. Armed with this new tool, we prove that padded
transformers with $O(\log^d n)$ looping on inputs of length $n$ recognize
exactly the class $\mathsf{FO}$-uniform $\mathsf{TC}^d$ of moderately
parallelizable problems. Thus, padding and looping together systematically
expand transformers&#x27; expressive power: with polylogarithmic looping,
polynomially padded transformers recognize precisely the class
$\mathsf{FO}$-uniform $\mathsf{NC}$, the best that could be expected without
losing parallelism (unless $\mathsf{NC} = \mathsf{P}$). Our results thus
motivate further exploration of padding and looping as parallelizable
alternatives to chain of thought for test-time compute.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding.</div>
</details>
</div>
<div class="card">
<div class="title">GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation</div>
<div class="meta-line">Authors: Manh Nguyen, Sunil Gupta, Dai Do, Hung Le</div>
<div class="meta-line">First: 2025-11-05T22:51:16+00:00 · Latest: 2025-11-05T22:51:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03900v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03900v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Hallucination mitigation remains a persistent challenge for large language models (LLMs), even as model scales grow.</div>
</details>
</div>
<div class="card">
<div class="title">Critical Batch Size Revisited: A Simple Empirical Approach to   Large-Batch Language Model Training</div>
<div class="meta-line">Authors: William Merrill, Shane Arora, Dirk Groeneveld, Hannaneh Hajishirzi</div>
<div class="meta-line">Venue: Neurips 2025</div>
<div class="meta-line">First: 2025-05-29T19:53:39+00:00 · Latest: 2025-11-05T22:50:55+00:00</div>
<div class="meta-line">Comments: Neurips 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.23971v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.23971v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The right batch size is important when training language models at scale: a
large batch size is necessary for fast training, but a batch size that is too
large will harm token efficiency. To navigate this tradeoff, McCandlish et al.
(2018) suggest that a critical batch size (CBS), below which training will not
substantially degrade loss, can be estimated based on the gradient noise scale
during training. While their method has been adopted in practice, e.g., when
training GPT-3, strong assumptions are required to justify gradient noise as a
proxy for the CBS, which makes it unclear whether their approach should be
trusted in practice, limiting its applicability. In this paper, we introduce a
simple, empirical approach to directly measure the CBS and show how the CBS
evolves over training. Applying our approach to the OLMo models, we find that
CBS is near 0 at initialization, increases rapidly at first, and then plateaus
as training progresses. Furthermore, we find that this trend holds across
different model sizes (1B and 7B), suggesting CBS from small training runs can
inform larger-scale training runs. Our findings about how the CBS changes over
training motivate batch size warmup as a natural way to reliably train language
models at large batch size: start the batch size small and increase it as the
CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to
slightly better loss than the original training run with 43% fewer gradient
steps. This shows how our framework can be applied to reliably train language
models at larger batch sizes, increasing data parallelism without compromising
performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is too large will harm token efficiency.</div>
</details>
</div>
<div class="card">
<div class="title">DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object   Hallucination</div>
<div class="meta-line">Authors: Xuan Gong, Tianshi Ming, Xinpeng Wang, Zhihua Wei</div>
<div class="meta-line">First: 2024-10-06T15:12:09+00:00 · Latest: 2025-11-05T20:57:25+00:00</div>
<div class="meta-line">Comments: Accepted by EMNLP2024 (Main Conference), add GitHub link</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.04514v2">Abs</a> · <a href="http://arxiv.org/pdf/2410.04514v2">PDF</a> · <a href="https://github.com/coder-gx/DAMRO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the great success of Large Vision-Language Models (LVLMs), they
inevitably suffer from hallucination. As we know, both the visual encoder and
the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing
the model to extract visual information and generate text outputs via attention
mechanisms. We find that the attention distribution of LLM decoder on image
tokens is highly consistent with the visual encoder and both distributions tend
to focus on particular background tokens rather than the referred objects in
the image. We attribute to the unexpected attention distribution to an inherent
flaw in the visual encoder itself, which misguides LLMs to over emphasize the
redundant information and generate object hallucination. To address the issue,
we propose DAMRO, a novel training-free strategy that $D$ive into $A$ttention
$M$echanism of LVLM to $R$educe $O$bject Hallucination. Specifically, our
approach employs classification token (CLS) of ViT to filter out high-attention
outlier tokens scattered in the background and then eliminate their influence
during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5,
LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME
and GPT-4V Aided Evaluation. The results demonstrate that our approach
significantly reduces the impact of these outlier tokens, thus effectively
alleviating the hallucination of LVLMs. The code is released at
https://github.com/coder-gx/DAMRO.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite the great success of Large Vision-Language Models (LVLMs), they inevitably suffer from hallucination.</div>
</details>
</div>
<div class="card">
<div class="title">How Different Tokenization Algorithms Impact LLMs and Transformer Models   for Binary Code Analysis</div>
<div class="meta-line">Authors: Ahmed Mostafa, Raisul Arefin Nahid, Samuel Mulder</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2025-11-05T19:45:26+00:00 · Latest: 2025-11-05T19:45:26+00:00</div>
<div class="meta-line">Comments: Publication Notice. This paper was published in the BAR 2025 Workshop
  (with NDSS 2025) and is for research and educational use. Copyright
  \c{opyright} 2025 Internet Society. All rights reserved. Personal/classroom
  reproduction is permitted with this notice and full paper citation. All other
  uses, including commercial, require prior written permission from the
  Internet Society</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03825v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03825v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Tokenization is fundamental in assembly code analysis, impacting intrinsic characteristics like vocabulary size, semantic coverage, and extrinsic performance in downstream tasks.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251109_0313.html">20251109_0313</a>
<a href="archive/20251108_0316.html">20251108_0316</a>
<a href="archive/20251107_0319.html">20251107_0319</a>
<a href="archive/20251106_0316.html">20251106_0316</a>
<a href="archive/20251105_0315.html">20251105_0315</a>
<a href="archive/20251104_0314.html">20251104_0314</a>
<a href="archive/20251103_0313.html">20251103_0313</a>
<a href="archive/20251102_0313.html">20251102_0313</a>
<a href="archive/20251101_0314.html">20251101_0314</a>
<a href="archive/20251031_0314.html">20251031_0314</a>
<a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
