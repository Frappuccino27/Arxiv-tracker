<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-14 03:15</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251114_0315</div>
    <div class="row"><div class="card">
<div class="title">Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality</div>
<div class="meta-line">Authors: Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik</div>
<div class="meta-line">First: 2025-05-23T11:30:30+00:00 · Latest: 2025-07-29T00:55:14+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/ZLKong/Awesome-Collection-Token-Reduction</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2505.18227v2">Abs</a> · <a href="https://github.com/ZLKong/Awesome-Collection-Token-Reduction">Code1</a> · <a href="https://github.com/ZLKong/awesome-token-compression-reduction">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input&#x27;s essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate &quot;overthinking&quot; and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks.</div>
</details>
</div>
<div class="card">
<div class="title">Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey</div>
<div class="meta-line">Authors: Liang Chen, Zekun Wang, Shuhuai Ren, Lei Li, Haozhe Zhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang, Yizhe Xiong, Yichi Zhang, Ruoyu Wu, Qingxiu Dong, Ge Zhang, Jian Yang, Lingwei Meng, Shujie Hu, Yulong Chen, Junyang Lin, Shuai Bai, Andreas Vlachos, Xu Tan, Minjia Zhang, Wen Xiao, Aaron Yee, Tianyu Liu, Baobao Chang</div>
<div class="meta-line">First: 2024-12-16T05:02:25+00:00 · Latest: 2024-12-31T01:56:04+00:00</div>
<div class="meta-line">Comments: 69 papes, 18 figures, repo at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2412.18619v2">Abs</a> · <a href="https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction">Code1</a> · <a href="https://github.com/kakaobrain/coyo-dataset">Code2</a> · <a href="https://github.com/togethercomputer/RedPajama-Data">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets \&amp; evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success.</div>
</details>
</div>
<div class="card">
<div class="title">Token Bottleneck: One Token to Remember Dynamics</div>
<div class="meta-line">Authors: Taekyung Kim, Dongyoon Han, Byeongho Heo, Jeongeun Park, Sangdoo Yun</div>
<div class="meta-line">First: 2025-07-09T04:57:29+00:00 · Latest: 2025-07-10T00:19:47+00:00</div>
<div class="meta-line">Comments: 17 pages, 9 figures, 8 tables, project page: https://token-bottleneck.github.io, code: https://github.com/naver-ai/tobo</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2507.06543v1">Abs</a> · <a href="https://github.com/naver-ai/tobo">Code1</a> · <a href="https://token-bottleneck.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation.</div>
</details>
</div>
<div class="card">
<div class="title">Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models</div>
<div class="meta-line">Authors: Xu Ma, Peize Sun, Haoyu Ma, Hao Tang, Chih-Yao Ma, Jialiang Wang, Kunpeng Li, Xiaoliang Dai, Yujun Shi, Xuan Ju, Yushi Hu, Artsiom Sanakoyeu, Felix Juefei-Xu, Ji Hou, Junjiao Tian, Tao Xu, Tingbo Hou, Yen-Cheng Liu, Zecheng He, Zijian He, Matt Feiszli, Peizhao Zhang, Peter Vajda, Sam Tsai, Yun Fu</div>
<div class="meta-line">First: 2025-04-24T17:59:56+00:00 · Latest: 2025-04-29T01:08:34+00:00</div>
<div class="meta-line">Comments: Project Page: https://ma-xu.github.io/token-shuffle/ Add related works</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2504.17789v2">Abs</a> · <a href="https://ma-xu.github.io/token-shuffle/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training and inference efficiency, as well as image resolution. To address this, we present Token-Shuffle, a novel yet simple method that reduces the number of image tokens in Transformer. Our key insight is the dimensional redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs), where low-dimensional visual codes from visual encoder are directly mapped to high-dimensional language vocabularies. Leveraging this, we consider two key operations: token-shuffle, which merges spatially local tokens along channel dimension to decrease the input token number, and token-unshuffle, which untangles the inferred tokens after Transformer blocks to restore the spatial arrangement for output. Jointly training with textual prompts, our strategy requires no additional pretrained text-encoder and enables MLLMs to support extremely high-resolution image synthesis in a unified next-token prediction way while maintaining efficient training and inference. For the first time, we push the boundary of AR text-to-image generation to a resolution of 2048x2048 with gratifying generation performance. In GenAI-benchmark, our 2.7B model achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human evaluations also demonstrate our prominent image generation ability in terms of text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle can serve as a foundational design for efficient high-resolution image generation within MLLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models.</div>
</details>
</div>
<div class="card">
<div class="title">Token-Efficient RL for LLM Reasoning</div>
<div class="meta-line">Authors: Alan Lee, Harry Tong</div>
<div class="meta-line">First: 2025-04-29T14:58:43+00:00 · Latest: 2025-06-13T00:03:03+00:00</div>
<div class="meta-line">Comments: Title updated to &quot;Token-Efficient RL for LLM Reasoning&quot; to better reflect algorithmic focus. Revised abstract, intro, and conclusion. Paper shortened and typos fixed</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2504.20834v4">Abs</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose reinforcement learning (RL) strategies tailored for reasoning in large language models (LLMs) under strict memory and compute limits, with a particular focus on compatibility with LoRA fine-tuning. Building on early policy gradient methods with baseline subtraction, we design critic-free methods that operate on a small, informative subset of output tokens to reduce memory usage and stabilize training. We introduce S-GRPO, a stochastic variant of Group Relative Policy Optimization, and T-SPMO, a token-level prefix matching approach for fine-grained credit assignment. Applied to Qwen2-1.5B, our methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show strong performance on multi-digit multiplication. Surprisingly, full-token GRPO under LoRA fails to improve over the base model, suggesting that selective token-level optimization may act as an implicit regularizer in low-parameter training regimes.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose reinforcement learning (RL) strategies tailored for reasoning in large language models (LLMs) under strict memory and compute limits, with a particular focus on compatibility with LoRA fine-tuning.</div>
</details>
</div>
<div class="card">
<div class="title">Token Pruning in Audio Transformers: Optimizing Performance and Decoding Patch Importance</div>
<div class="meta-line">Authors: Taehan Lee, Hyukjun Lee</div>
<div class="meta-line">First: 2025-04-02T12:44:38+00:00 · Latest: 2025-10-27T01:03:42+00:00</div>
<div class="meta-line">Comments: Accepted at the 28th European Conference on Artificial Intelligence (ECAI 2025). Source code is available at https://github.com/andylee-24/token-pruning-audio-transformer</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2504.01690v2">Abs</a> · <a href="https://github.com/andylee-24/token-pruning-audio-transformer">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers (ViTs) have achieved state-of-the-art performance across various computer vision tasks, but their high computational cost remains a challenge. Token pruning has been proposed to reduce this cost by selectively removing less important tokens. While effective in vision tasks by discarding non-object regions, applying this technique to audio tasks presents unique challenges, as distinguishing relevant from irrelevant regions in time-frequency representations is less straightforward. In this study, for the first time, we applied token pruning to ViT-based audio classification models using Mel-spectrograms and analyzed the trade-offs between model performance and computational cost: TopK token pruning can reduce MAC operations of AudioMAE and AST by 30-40%, with less than a 1% drop in accuracy. Our analysis reveals that while high-intensity or high-variation tokens contribute significantly to model accuracy, low-intensity or low variation tokens also remain important when token pruning is applied; pruning solely based on the intensity or variation of signals in a patch leads to a noticeable drop in accuracy. We support our claim by measuring high correlation between attention scores and these statistical features and by showing retained tokens consistently receive distinct attention compared to pruned ones. We also show that AudioMAE retains more low-intensity tokens than AST. This can be explained by AudioMAE&#x27;s self-supervised reconstruction objective, which encourages attention to all patches, whereas AST&#x27;s supervised training focuses on label-relevant tokens.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision Transformers (ViTs) have achieved state-of-the-art performance across various computer vision tasks, but their high computational cost remains a challenge.</div>
</details>
</div>
<div class="card">
<div class="title">When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios</div>
<div class="meta-line">Authors: Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, Huan Wang</div>
<div class="meta-line">First: 2025-07-27T09:33:56+00:00 · Latest: 2025-08-29T00:24:33+00:00</div>
<div class="meta-line">Comments: For ongoing updates and to track the latest advances in this promising area, we maintain a public repository: https://github.com/cokeshao/Awesome-Multimodal-Token-Compression</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2507.20198v4">Abs</a> · <a href="https://github.com/cokeshao/Awesome-Multimodal-Token-Compression">Code1</a> · <a href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have made remarkable strides, largely driven by their ability to process increasingly long and complex contexts, such as high-resolution images, extended video sequences, and lengthy audio input. While this ability significantly enhances MLLM capabilities, it introduces substantial computational challenges, primarily due to the quadratic complexity of self-attention mechanisms with numerous input tokens. To mitigate these bottlenecks, token compression has emerged as an auspicious and critical approach, efficiently reducing the number of tokens during both training and inference. In this paper, we present the first systematic survey and synthesis of the burgeoning field of multimodal long context token compression. Recognizing that effective compression strategies are deeply tied to the unique characteristics and redundancies of each modality, we categorize existing approaches by their primary data focus, enabling researchers to quickly access and learn methods tailored to their specific area of interest: (1) image-centric compression, which addresses spatial redundancy in visual data; (2) video-centric compression, which tackles spatio-temporal redundancy in dynamic sequences; and (3) audio-centric compression, which handles temporal and spectral redundancy in acoustic signals. Beyond this modality-driven categorization, we further dissect methods based on their underlying mechanisms, including transformation-based, similarity-based, attention-based, and query-based approaches. By providing a comprehensive and structured overview, this survey aims to consolidate current progress, identify key challenges, and inspire future research directions in this rapidly evolving domain. We also maintain a public repository to continuously track and update the latest advances in this promising area.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) have made remarkable strides, largely driven by their ability to process increasingly long and complex contexts, such as high-resolution images, extended video sequences, and lengthy audio input.</div>
</details>
</div>
<div class="card">
<div class="title">Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling</div>
<div class="meta-line">Authors: Xianzhen Luo, Yixuan Wang, Qingfu Zhu, Zhiming Zhang, Xuanyu Zhang, Qing Yang, Dongliang Xu</div>
<div class="meta-line">First: 2024-08-16T12:20:56+00:00 · Latest: 2025-05-27T00:59:42+00:00</div>
<div class="meta-line">Comments: Accepted by ACL2025. Code is [here](https://github.com/Luowaterbi/TokenRecycling). Token Recycling has already merged into [SpecBench](https://github.com/hemingkx/Spec-Bench)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2408.08696v3">Abs</a> · <a href="https://github.com/Luowaterbi/TokenRecycling">Code1</a> · <a href="https://github.com/hemingkx/Spec-Bench">Code2</a> · <a href="https://github.com/tatsu-lab/alpaca_eval">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Massive parameters of LLMs have made inference latency a fundamental bottleneck. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm. Some methods rely on additional architectures to guess draft tokens, which need extra training before use. Alternatively, retrieval-based training-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. It stores candidate tokens in an adjacency matrix and employs a breadth-first-search (BFS)-like algorithm to construct a draft tree, which is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\% and even a widely recognized training method by 25\%.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Massive parameters of LLMs have made inference latency a fundamental bottleneck.</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Tokenization</div>
<div class="meta-line">Authors: Renato Lui Geh, Zilei Shao, Guy Van den Broeck</div>
<div class="meta-line">Venue: ACL 2025</div>
<div class="meta-line">First: 2025-03-04T01:31:17+00:00 · Latest: 2025-06-09T00:11:22+00:00</div>
<div class="meta-line">Comments: Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, ACL 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2503.02174v2">Abs</a> · <a href="https://github.com/RenatoGeh/advtok">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current LLM pipelines account for only one possible tokenization for a given string, ignoring exponentially many alternative tokenizations during training and inference. For example, the standard Llama3 tokenization of penguin is [p,enguin], yet [peng,uin] is another perfectly valid alternative. In this paper, we show that despite LLMs being trained solely on one tokenization, they still retain semantic understanding of other tokenizations, raising questions about their implications in LLM safety. Put succinctly, we answer the following question: can we adversarially tokenize an obviously malicious string to evade safety and alignment restrictions? We show that not only is adversarial tokenization an effective yet previously neglected axis of attack, but it is also competitive against existing state-of-the-art adversarial approaches without changing the text of the harmful request. We empirically validate this exploit across three state-of-the-art LLMs and adversarial datasets, revealing a previously unknown vulnerability in subword models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current LLM pipelines account for only one possible tokenization for a given string, ignoring exponentially many alternative tokenizations during training and inference.</div>
</details>
</div>
<div class="card">
<div class="title">Token Fusion: Bridging the Gap between Token Pruning and Token Merging</div>
<div class="meta-line">Authors: Minchul Kim, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, Hongxia Jin</div>
<div class="meta-line">Venue: WACV 2024</div>
<div class="meta-line">First: 2023-12-02T04:29:19+00:00 · Latest: 2023-12-05T01:10:20+00:00</div>
<div class="meta-line">Comments: To appear in WACV 2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2312.01026v1">Abs</a> · <a href="https://github.com/facebookresearch/fvcore">Code1</a> · <a href="https://github.com/rwightman/pytorch-image-models">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers (ViTs) have emerged as powerful backbones in computer vision, outperforming many traditional CNNs. However, their computational overhead, largely attributed to the self-attention mechanism, makes deployment on resource-constrained edge devices challenging. Multiple solutions rely on token pruning or token merging. In this paper, we introduce &quot;Token Fusion&quot; (ToFu), a method that amalgamates the benefits of both token pruning and token merging. Token pruning proves advantageous when the model exhibits sensitivity to input interpolations, while token merging is effective when the model manifests close to linear responses to inputs. We combine this to propose a new scheme called Token Fusion. Moreover, we tackle the limitations of average merging, which doesn&#x27;t preserve the intrinsic feature norm, resulting in distributional shifts. To mitigate this, we introduce MLERP merging, a variant of the SLERP technique, tailored to merge multiple tokens while maintaining the norm distribution. ToFu is versatile, applicable to ViTs with or without additional training. Our empirical evaluations indicate that ToFu establishes new benchmarks in both classification and image generation tasks concerning computational efficiency and model accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision Transformers (ViTs) have emerged as powerful backbones in computer vision, outperforming many traditional CNNs.</div>
</details>
</div>
<div class="card">
<div class="title">OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference</div>
<div class="meta-line">Authors: Seungjun Shin, Jaehoon Oh, Dokwan Oh</div>
<div class="meta-line">Venue: ICML 2025</div>
<div class="meta-line">First: 2025-07-05T02:29:23+00:00 · Latest: 2025-08-19T00:22:38+00:00</div>
<div class="meta-line">Comments: ICML 2025 (final version)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2507.03865v2">Abs</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Attention mechanisms are central to the success of large language models (LLMs), enabling them to capture intricate token dependencies and implicitly assign importance to each token. Recent studies have revealed the sink token, which receives disproportionately high attention despite their limited semantic role. In this paper, we first expand the relationship between the sink token and other tokens, moving beyond attention to explore their similarity in hidden states, considering the layer depth. We observe that as the layers get deeper, the cosine similarity between the normalized hidden states of the sink token and those of other tokens increases, and that the normalized hidden states of the sink token exhibit negligible changes. These imply that other tokens consistently are directed toward the sink token throughout the layers. Next, we propose a dynamic token selection method, called OrthoRank, using these findings to select important tokens. Specifically, in a certain layer, we define token importance by the speed at which the token moves toward the sink token. This is converted into orthogonality with the sink token, meaning that tokens that are more orthogonal to the sink token are assigned greater importance. Finally, through extensive experiments, we demonstrated that our method results in lower perplexity and higher zero-shot accuracy compared to layer pruning methods at the same sparsity ratio with comparable throughput, while also achieving superior performance on LongBench.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Attention mechanisms are central to the success of large language models (LLMs), enabling them to capture intricate token dependencies and implicitly assign importance to each token.</div>
</details>
</div>
<div class="card">
<div class="title">Visual Concepts Tokenization</div>
<div class="meta-line">Authors: Tao Yang, Yuwang Wang, Yan Lu, Nanning Zheng</div>
<div class="meta-line">Venue: NeurIPS 2022</div>
<div class="meta-line">First: 2022-05-20T11:25:31+00:00 · Latest: 2022-10-14T00:10:28+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2022</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2205.10093v2">Abs</a> · <a href="https://github.com/thomasmry/VCT">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Obtaining the human-like perception ability of abstracting visual concepts from concrete pixels has always been a fundamental and important target in machine learning research fields such as disentangled representation learning and scene decomposition. Towards this goal, we propose an unsupervised transformer-based Visual Concepts Tokenization framework, dubbed VCT, to perceive an image into a set of disentangled visual concept tokens, with each concept token responding to one type of independent visual concept. Particularly, to obtain these concept tokens, we only use cross-attention to extract visual information from the image tokens layer by layer without self-attention between concept tokens, preventing information leakage across concept tokens. We further propose a Concept Disentangling Loss to facilitate that different concept tokens represent independent visual concepts. The cross-attention and disentangling loss play the role of induction and mutual exclusion for the concept tokens, respectively. Extensive experiments on several popular datasets verify the effectiveness of VCT on the tasks of disentangled representation learning and scene decomposition. VCT achieves the state of the art results by a large margin.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Obtaining the human-like perception ability of abstracting visual concepts from concrete pixels has always been a fundamental and important target in machine learning research fields such as disentangled representation learning and scene decomposition.</div>
</details>
</div>
<div class="card">
<div class="title">Language-Guided Image Tokenization for Generation</div>
<div class="meta-line">Authors: Kaiwen Zha, Lijun Yu, Alireza Fathi, David A. Ross, Cordelia Schmid, Dina Katabi, Xiuye Gu</div>
<div class="meta-line">Venue: CVPR 2025 Oral</div>
<div class="meta-line">First: 2024-12-08T03:18:17+00:00 · Latest: 2025-04-08T00:40:50+00:00</div>
<div class="meta-line">Comments: CVPR 2025 Oral. Project page: https://kaiwenzha.github.io/textok/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2412.05796v2">Abs</a> · <a href="https://kaiwenzha.github.io/textok/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide a compact, high-level semantic representation. By conditioning the tokenization process on descriptive text captions, TexTok simplifies semantic learning, allowing more learning capacity and token space to be allocated to capture fine-grained visual details, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2% and 48.1% on ImageNet-256 and -512 benchmarks respectively, across varying numbers of tokens. These tokenization improvements consistently translate to 16.3% and 34.3% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve a 93.5x inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok&#x27;s superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization. Project page is at: https://kaiwenzha.github.io/textok/.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation.</div>
</details>
</div>
<div class="card">
<div class="title">VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models</div>
<div class="meta-line">Authors: Haichao Zhang, Yun Fu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-03-21T09:46:31+00:00 · Latest: 2025-09-30T01:25:43+00:00</div>
<div class="meta-line">Comments: Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2503.16980v6">Abs</a> · <a href="https://github.com/Hai-chao-Zhang/VQToken">Code1</a> · <a href="https://huggingface.co/haichaozhang/VQ-Token-llava-ov-0.5b">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content. However, existing token reduction techniques, such as pruning and merging, often disrupt essential positional embeddings and rely on continuous visual tokens sampled from nearby pixels with similar spatial-temporal locations. By removing only a small fraction of tokens, these methods still produce relatively lengthy continuous sequences, which falls short of the extreme compression required to balance computational efficiency and token count in video LLMs. In this paper, we introduce the novel task of Extreme Short Token Reduction, which aims to represent entire videos using a minimal set of discrete tokens. We propose VQToken, a neural discrete token representation framework that (i) applies adaptive vector quantization to continuous ViT embeddings to learn a compact codebook and (ii) preserves spatial-temporal positions via a token hash function by assigning each grid-level token to its nearest codebook entry. On the Extreme Short Token Reduction task, our VQToken compresses sequences to just 0.07 percent of their original length while incurring only a 0.66 percent drop in accuracy on the NextQA-MC benchmark. It also achieves comparable performance on ActNet-QA, Long Video Bench, and VideoMME. We further introduce the Token Information Density (TokDense) metric and formalize fixed-length and adaptive-length subtasks, achieving state-of-the-art results in both settings. Our approach dramatically lowers theoretical complexity, increases information density, drastically reduces token counts, and enables efficient video LLMs in resource-constrained environments.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content.</div>
</details>
</div>
<div class="card">
<div class="title">Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs</div>
<div class="meta-line">Authors: Aaditya K. Singh, DJ Strouse</div>
<div class="meta-line">First: 2024-02-22T18:14:09+00:00 · Latest: 2024-02-26T01:02:51+00:00</div>
<div class="meta-line">Comments: 21 pages, 18 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2402.14903v1">Abs</a> · <a href="https://huggingface.co/docs/transformers/main/en/model_doc/mistral">Code1</a> · <a href="https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py">Code2</a> · <a href="https://github.com/aadityasingh/TokenizationCounts">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases. Historically, LLMs have relied on byte pair encoding, without care to specific input domains. With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers. In this work, we study the effect this choice has on numerical reasoning through the use of arithmetic tasks. We consider left-to-right and right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left tokenization (enforced by comma separating numbers at inference time) leads to largely improved performance. Furthermore, we find that model errors when using standard left-to-right tokenization follow stereotyped error patterns, suggesting that model computations are systematic rather than approximate. We show that the model is able to convert between tokenizations easily, thus allowing chain-of-thought-inspired approaches to recover performance on left-to-right tokenized inputs. We also find the gap between tokenization directions decreases when models are scaled, possibly indicating that larger models are better able to override this tokenization-dependent inductive bias. In summary, our work performs the first study of how number tokenization choices lead to differences in model performance on arithmetic tasks, accompanied by a thorough analysis of error patterns. We hope this work inspires practitioners to more carefully ablate number tokenization-related choices when working towards general models of numerical reasoning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Tokenization, the division of input text into input tokens, is an often overlooked aspect of the large language model (LLM) pipeline and could be the source of useful or harmful inductive biases.</div>
</details>
</div>
<div class="card">
<div class="title">One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory</div>
<div class="meta-line">Authors: Chenhao Zheng, Jieyu Zhang, Mohammadreza Salehi, Ziqi Gao, Vishnu Iyengar, Norimasa Kobori, Quan Kong, Ranjay Krishna</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-05-29T16:25:35+00:00 · Latest: 2025-07-11T00:02:42+00:00</div>
<div class="meta-line">Comments: ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2505.23617v2">Abs</a> · <a href="https://github.com/RAIVNLab/trajvit">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 18x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Effective video tokenization is critical for scaling transformer models for long videos.</div>
</details>
</div>
<div class="card">
<div class="title">Token-Label Alignment for Vision Transformers</div>
<div class="meta-line">Authors: Han Xiao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, Jiwen Lu</div>
<div class="meta-line">First: 2022-10-12T17:54:32+00:00 · Latest: 2022-11-30T01:10:49+00:00</div>
<div class="meta-line">Comments: Source code available at https://github.com/Euphoria16/TL-Align</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2210.06455v2">Abs</a> · <a href="https://github.com/Euphoria16/TL-Align">Code1</a> · <a href="https://github.com/rwightman/pytorch-image-models">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data mixing strategies (e.g., CutMix) have shown the ability to greatly improve the performance of convolutional neural networks (CNNs). They mix two images as inputs for training and assign them with a mixed label with the same ratio. While they are shown effective for vision transformers (ViTs), we identify a token fluctuation phenomenon that has suppressed the potential of data mixing strategies. We empirically observe that the contributions of input tokens fluctuate as forward propagating, which might induce a different mixing ratio in the output tokens. The training target computed by the original data mixing strategy can thus be inaccurate, resulting in less effective training. To address this, we propose a token-label alignment (TL-Align) method to trace the correspondence between transformed tokens and the original tokens to maintain a label for each token. We reuse the computed attention at each layer for efficient token-label alignment, introducing only negligible additional training costs. Extensive experiments demonstrate that our method improves the performance of ViTs on image classification, semantic segmentation, objective detection, and transfer learning tasks. Code is available at: https://github.com/Euphoria16/TL-Align.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Data mixing strategies (e.g., CutMix) have shown the ability to greatly improve the performance of convolutional neural networks (CNNs).</div>
</details>
</div>
<div class="card">
<div class="title">The Token Tax: Systematic Bias in Multilingual Tokenization</div>
<div class="meta-line">Authors: Jessica M. Lundin, Ada Zhang, Nihal Karim, Hamza Louzan, Victor Wei, David Adelani, Cody Carroll</div>
<div class="meta-line">First: 2025-09-05T20:20:51+00:00 · Latest: 2025-09-09T00:06:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2509.05486v1">Abs</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenization inefficiency imposes structural disadvantages on morphologically complex, low-resource languages, inflating compute resources and depressing accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA items; 5 subjects; 16 African languages) and show that fertility (tokens/word) reliably predicts accuracy. Higher fertility consistently predicts lower accuracy across all models and subjects. We further find that reasoning models (DeepSeek, o1) consistently outperform non-reasoning peers across high and low resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in prior generations. Finally, translating token inflation to economics, a doubling in tokens results in quadrupled training cost and time, underscoring the token tax faced by many languages. These results motivate morphologically aware tokenization, fair pricing, and multilingual benchmarks for equitable natural language processing (NLP).</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Tokenization inefficiency imposes structural disadvantages on morphologically complex, low-resource languages, inflating compute resources and depressing accuracy.</div>
</details>
</div>
<div class="card">
<div class="title">Soft Tokens, Hard Truths</div>
<div class="meta-line">Authors: Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, Yann Ollivier</div>
<div class="meta-line">First: 2025-09-23T15:43:47+00:00 · Latest: 2025-09-25T00:43:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2509.19170v2">Abs</a> · <a href="https://huggingface.co/datasets/HuggingFaceH4/MATH-500">Code1</a> · <a href="https://github.com/huggingface/math-verify">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.
  This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use &quot;soft&quot; tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the &quot;soft&quot; models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously.</div>
</details>
</div>
<div class="card">
<div class="title">Spectral Image Tokenizer</div>
<div class="meta-line">Authors: Carlos Esteves, Mohammed Suhail, Ameesh Makadia</div>
<div class="meta-line">First: 2024-12-12T18:59:31+00:00 · Latest: 2025-06-12T01:03:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2412.09607v2">Abs</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image tokenizers map images to sequences of discrete tokens, and are a crucial component of autoregressive transformer-based image generation. The tokens are typically associated with spatial locations in the input image, arranged in raster scan order, which is not ideal for autoregressive modeling. In this paper, we propose to tokenize the image spectrum instead, obtained from a discrete wavelet transform (DWT), such that the sequence of tokens represents the image in a coarse-to-fine fashion. Our tokenizer brings several advantages: 1) it leverages that natural images are more compressible at high frequencies, 2) it can take and reconstruct images of different resolutions without retraining, 3) it improves the conditioning for next-token prediction -- instead of conditioning on a partial line-by-line reconstruction of the image, it takes a coarse reconstruction of the full image, 4) it enables partial decoding where the first few generated tokens can reconstruct a coarse version of the image, 5) it enables autoregressive models to be used for image upsampling. We evaluate the tokenizer reconstruction metrics as well as multiscale image generation, text-guided image upsampling and editing.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Image tokenizers map images to sequences of discrete tokens, and are a crucial component of autoregressive transformer-based image generation.</div>
</details>
</div>
<div class="card">
<div class="title">Tokenization of Gaze Data</div>
<div class="meta-line">Authors: Tim Rolff, Jurik Karimian, Niklas Hypki, Susanne Schmidt, Markus Lappe, Frank Steinicke</div>
<div class="meta-line">First: 2025-03-28T04:41:09+00:00 · Latest: 2025-03-31T01:22:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2503.22145v1">Abs</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A considerable part of the performance of today&#x27;s large language models (LLM&#x27;s) and multimodal large language models (MLLM&#x27;s) depends on their tokenization strategies. While tokenizers are extensively researched for textual and visual input, there is no research on tokenization strategies for gaze data due to its nature. However, a corresponding tokenization strategy would allow using the vision capabilities of pre-trained MLLM&#x27;s for gaze data, for example, through fine-tuning.
  In this paper, we aim to close this research gap by analyzing five different tokenizers for gaze data on three different datasets for the forecasting and generation of gaze data through LLMs (cf.~\cref{fig:teaser}). We evaluate the tokenizers regarding their reconstruction and compression abilities. Further, we train an LLM for each tokenization strategy, measuring its generative and predictive performance. Overall, we found that a quantile tokenizer outperforms all others in predicting the gaze positions and k-means is best when predicting gaze velocities.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">A considerable part of the performance of today&#x27;s large language models (LLM&#x27;s) and multimodal large language models (MLLM&#x27;s) depends on their tokenization strategies.</div>
</details>
</div>
<div class="card">
<div class="title">The pitfalls of next-token prediction</div>
<div class="meta-line">Authors: Gregor Bachmann, Vaishnavh Nagarajan</div>
<div class="meta-line">Venue: ICML 2024</div>
<div class="meta-line">First: 2024-03-11T17:47:30+00:00 · Latest: 2025-07-30T00:18:00+00:00</div>
<div class="meta-line">Comments: ICML 2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2403.06963v3">Abs</a> · <a href="https://github.com/gregorbachmann/Next-Token-Failures">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can a mere next-token predictor faithfully model human intelligence? We crystallize this emerging concern and correct popular misconceptions surrounding it, and advocate a simple multi-token objective.
  As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn.
  Finally, we provide preliminary evidence that this failure can be resolved using _teacherless_ training, a simple modification using dummy tokens that predicts multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under https://github.com/gregorbachmann/Next-Token-Failures</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Can a mere next-token predictor faithfully model human intelligence?</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Attentive Tokens: Incorporating Token Importance and Diversity for Efficient Vision Transformers</div>
<div class="meta-line">Authors: Sifan Long, Zhen Zhao, Jimin Pi, Shengsheng Wang, Jingdong Wang</div>
<div class="meta-line">First: 2022-11-21T09:57:11+00:00 · Latest: 2022-11-22T01:26:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2211.11315v1">Abs</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision transformers have achieved significant improvements on various vision tasks but their quadratic interactions between tokens significantly reduce computational efficiency. Many pruning methods have been proposed to remove redundant tokens for efficient vision transformers recently. However, existing studies mainly focus on the token importance to preserve local attentive tokens but completely ignore the global token diversity. In this paper, we emphasize the cruciality of diverse global semantics and propose an efficient token decoupling and merging method that can jointly consider the token importance and diversity for token pruning. According to the class token attention, we decouple the attentive and inattentive tokens. In addition to preserving the most discriminative local tokens, we merge similar inattentive tokens and match homogeneous attentive tokens to maximize the token diversity. Despite its simplicity, our method obtains a promising trade-off between model complexity and classification accuracy. On DeiT-S, our method reduces the FLOPs by 35% with only a 0.2% accuracy drop. Notably, benefiting from maintaining the token diversity, our method can even improve the accuracy of DeiT-T by 0.1% after reducing its FLOPs by 40%.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision transformers have achieved significant improvements on various vision tasks but their quadratic interactions between tokens significantly reduce computational efficiency.</div>
</details>
</div>
<div class="card">
<div class="title">Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies</div>
<div class="meta-line">Authors: Anaelia Ovalle, Ninareh Mehrabi, Palash Goyal, Jwala Dhamala, Kai-Wei Chang, Richard Zemel, Aram Galstyan, Yuval Pinter, Rahul Gupta</div>
<div class="meta-line">Venue: NAACL 2024</div>
<div class="meta-line">First: 2023-12-19T01:28:46+00:00 · Latest: 2024-04-09T00:24:54+00:00</div>
<div class="meta-line">Comments: Accepted to NAACL 2024 findings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2312.11779v3">Abs</a> · <a href="https://github.com/EleutherAI/pythia">Code1</a> · <a href="https://huggingface.co/datasets/wiki_bio">Code2</a> · <a href="https://huggingface.co/EleutherAI/pythia-12b-deduped">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae). While data scarcity is a known culprit, the precise mechanisms through which scarcity affects this behavior remain underexplored. We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLMs. Unlike binary pronouns, BPE overfragments neopronouns, a direct consequence of data scarcity during tokenizer training. This disparate tokenization mirrors tokenizer limitations observed in multilingual and low-resource NLP, unlocking new misgendering mitigation strategies. We propose two techniques: (1) pronoun tokenization parity, a method to enforce consistent tokenization across gendered pronouns, and (2) utilizing pre-existing LLM pronoun knowledge to improve neopronoun proficiency. Our proposed methods outperform finetuning with standard BPE, improving neopronoun accuracy from 14.1% to 58.4%. Our paper is the first to link LLM misgendering to tokenization and deficient neopronoun grammar, indicating that LLMs unable to correctly treat neopronouns as pronouns are more prone to misgender.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae).</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Autoencoders are Scalable Image Tokenizers</div>
<div class="meta-line">Authors: Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, Ishan Misra</div>
<div class="meta-line">First: 2025-01-30T18:59:37+00:00 · Latest: 2025-01-31T01:54:11+00:00</div>
<div class="meta-line">Comments: Project page: https://yinboc.github.io/dito/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2501.18593v1">Abs</a> · <a href="https://yinboc.github.io/dito/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenizing images into compact visual representations is a key step in learning efficient and high-quality image generative models. We present a simple diffusion tokenizer (DiTo) that learns compact visual representations for image generation models. Our key insight is that a single learning objective, diffusion L2 loss, can be used for training scalable image tokenizers. Since diffusion is already widely used for image generation, our insight greatly simplifies training such tokenizers. In contrast, current state-of-the-art tokenizers rely on an empirically found combination of heuristics and losses, thus requiring a complex training recipe that relies on non-trivially balancing different losses and pretrained supervised models. We show design decisions, along with theoretical grounding, that enable us to scale DiTo for learning competitive image representations. Our results show that DiTo is a simpler, scalable, and self-supervised alternative to the current state-of-the-art image tokenizer which is supervised. DiTo achieves competitive or better quality than state-of-the-art in image reconstruction and downstream image generation tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Tokenizing images into compact visual representations is a key step in learning efficient and high-quality image generative models.</div>
</details>
</div>
<div class="card">
<div class="title">No Token Left Behind: Efficient Vision Transformer via Dynamic Token Idling</div>
<div class="meta-line">Authors: Xuwei Xu, Changlin Li, Yudong Chen, Xiaojun Chang, Jiajun Liu, Sen Wang</div>
<div class="meta-line">First: 2023-10-09T12:10:41+00:00 · Latest: 2023-11-07T01:26:17+00:00</div>
<div class="meta-line">Comments: Accepted to AJCAI2023</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2310.05654v2">Abs</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers (ViTs) have demonstrated outstanding performance in computer vision tasks, yet their high computational complexity prevents their deployment in computing resource-constrained environments. Various token pruning techniques have been introduced to alleviate the high computational burden of ViTs by dynamically dropping image tokens. However, some undesirable pruning at early stages may result in permanent loss of image information in subsequent layers, consequently hindering model performance. To address this problem, we propose IdleViT, a dynamic token-idle-based method that achieves an excellent trade-off between performance and efficiency. Specifically, in each layer, IdleViT selects a subset of the image tokens to participate in computations while keeping the rest of the tokens idle and directly passing them to this layer&#x27;s output. By allowing the idle tokens to be re-selected in the following layers, IdleViT mitigates the negative impact of improper pruning in the early stages. Furthermore, inspired by the normalized graph cut, we devise a token cut loss on the attention map as regularization to improve IdleViT&#x27;s token selection ability. Our method is simple yet effective and can be extended to pyramid ViTs since no token is completely dropped. Extensive experimental results on various ViT architectures have shown that IdleViT can diminish the complexity of pretrained ViTs by up to 33\% with no more than 0.2\% accuracy decrease on ImageNet, after finetuning for only 30 epochs. Notably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art EViT on DeiT-S by 0.5\% higher accuracy and even faster inference speed. The source code is available in the supplementary material.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision Transformers (ViTs) have demonstrated outstanding performance in computer vision tasks, yet their high computational complexity prevents their deployment in computing resource-constrained environments.</div>
</details>
</div>
<div class="card">
<div class="title">Learning Graph Quantized Tokenizers</div>
<div class="meta-line">Authors: Limei Wang, Kaveh Hassani, Si Zhang, Dongqi Fu, Baichuan Yuan, Weilin Cong, Zhigang Hua, Hao Wu, Ning Yao, Bo Long</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2024-10-17T17:38:24+00:00 · Latest: 2025-04-03T00:23:15+00:00</div>
<div class="meta-line">Comments: ICLR 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2410.13798v2">Abs</a> · <a href="https://github.com/limei0307/GQT">Code1</a> · <a href="https://github.com/lucidrains/vector-quantize-pytorch">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains. Graph Transformers (GTs) have recently emerged as leading models in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph learning tasks. However, the development of tokenizers for graphs has lagged behind other modalities. To address this, we introduce GQT (\textbf{G}raph \textbf{Q}uantized \textbf{T}okenizer), which decouples tokenizer training from Transformer training by leveraging multi-task graph self-supervised learning, yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens, resulting in significantly reduced memory requirements and improved generalization capabilities. By combining the GQT with token modulation, a Transformer encoder achieves state-of-the-art performance on 20 out of 22 benchmarks, including large-scale homophilic and heterophilic datasets.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains.</div>
</details>
</div>
<div class="card">
<div class="title">Tokenization for Molecular Foundation Models</div>
<div class="meta-line">Authors: Alexius Wadell, Anoushka Bhutani, Venkatasubramanian Viswanathan</div>
<div class="meta-line">First: 2024-09-19T02:36:04+00:00 · Latest: 2025-07-10T00:07:45+00:00</div>
<div class="meta-line">Comments: 26 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2409.15370v3">Abs</a> · <a href="https://github.com/BattModels/Smirk">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-based foundation models have become an important part of scientific discovery, with molecular foundation models accelerating advancements in material science and molecular design.However, existing models are constrained by closed-vocabulary tokenizers that capture only a fraction of molecular space. In this work, we systematically evaluate 34 tokenizers, including 19 chemistry-specific ones, and reveal significant gaps in their coverage of the SMILES molecular representation. To assess the impact of tokenizer choice, we introduce n-gram language models as a low-cost proxy and validate their effectiveness by pretraining and finetuning 18 RoBERTa-style encoders for molecular property prediction. To overcome the limitations of existing tokenizers, we propose two new tokenizers -- Smirk and Smirk-GPE -- with full coverage of the OpenSMILES specification. The proposed tokenizers systematically integrate nuclear, electronic, and geometric degrees of freedom; facilitating applications in pharmacology, agriculture, biology, and energy storage. Our results highlight the need for open-vocabulary modeling and chemically diverse benchmarks in cheminformatics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-based foundation models have become an important part of scientific discovery, with molecular foundation models accelerating advancements in material science and molecular design.However, existing models are constrained by closed-vocabulary tokenizers that capture only a fraction of molecular space.</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal Medical Code Tokenizer</div>
<div class="meta-line">Authors: Xiaorui Su, Shvat Messica, Yepeng Huang, Ruth Johnson, Lukas Fesser, Shanghua Gao, Faryad Sahneh, Marinka Zitnik</div>
<div class="meta-line">Venue: ICML</div>
<div class="meta-line">First: 2025-02-06T06:58:09+00:00 · Latest: 2025-07-01T00:36:06+00:00</div>
<div class="meta-line">Comments: ICML&#x27;25</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2502.04397v3">Abs</a> · <a href="https://github.com/mims-harvard/MedTok">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items. Existing tokenizers treat medical codes from EHRs as isolated textual tokens. However, each medical code is defined by its textual description, its position in ontological hierarchies, and its relationships to other codes, such as disease co-occurrences and drug-treatment associations. Medical vocabularies contain more than 600,000 codes with critical information for clinical reasoning. We introduce MedTok, a multimodal medical code tokenizer that uses the text descriptions and relational context of codes. MedTok processes text using a language model encoder and encodes the relational structure with a graph encoder. It then quantizes both modalities into a unified token space, preserving modality-specific and cross-modality information. We integrate MedTok into five EHR models and evaluate it on operational and clinical tasks across in-patient and out-patient datasets, including outcome prediction, diagnosis classification, drug recommendation, and risk stratification. Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate using MedTok tokenizer with medical QA systems. Our results demonstrate the potential of MedTok as a unified tokenizer for medical codes, improving tokenization for medical foundation models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items.</div>
</details>
</div>
<div class="card">
<div class="title">The Foundations of Tokenization: Statistical and Computational Concerns</div>
<div class="meta-line">Authors: Juan Luis Gastaldi, John Terilla, Luca Malagutti, Brian DuSell, Tim Vieira, Ryan Cotterell</div>
<div class="meta-line">First: 2024-07-16T11:12:28+00:00 · Latest: 2025-04-04T00:54:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/pdf/2407.11606v4">Abs</a> · <a href="https://github.com/huggingface/tokenizers">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenization - the practice of converting strings of characters from an alphabet into sequences of tokens over a vocabulary - is a critical step in the NLP pipeline. The use of token representations is widely credited with increased model performance but is also the source of many undesirable behaviors, such as spurious ambiguity or inconsistency. Despite its recognized importance as a standard representation method in NLP, the theoretical underpinnings of tokenization are not yet fully understood. In particular, the impact of tokenization on language model estimation has been investigated primarily through empirical means. The present paper contributes to addressing this theoretical gap by proposing a unified formal framework for representing and analyzing tokenizer models. Based on the category of stochastic maps, this framework enables us to establish general conditions for a principled use of tokenizers and, most importantly, the necessary and sufficient conditions for a tokenizer model to preserve the consistency of statistical estimators. In addition, we discuss statistical and computational concerns crucial for designing and implementing tokenizer models, such as inconsistency, ambiguity, finiteness, and sequentiality. The framework and results advanced in this paper contribute to building robust theoretical foundations for representations in neural language modeling that can inform future theoretical and empirical research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Tokenization - the practice of converting strings of characters from an alphabet into sequences of tokens over a vocabulary - is a critical step in the NLP pipeline.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251113_0316.html">20251113_0316</a>
<a href="archive/20251112_0315.html">20251112_0315</a>
<a href="archive/20251111_0314.html">20251111_0314</a>
<a href="archive/20251110_0312.html">20251110_0312</a>
<a href="archive/20251109_0313.html">20251109_0313</a>
<a href="archive/20251108_0316.html">20251108_0316</a>
<a href="archive/20251107_0319.html">20251107_0319</a>
<a href="archive/20251106_0316.html">20251106_0316</a>
<a href="archive/20251105_0315.html">20251105_0315</a>
<a href="archive/20251104_0314.html">20251104_0314</a>
<a href="archive/20251103_0313.html">20251103_0313</a>
<a href="archive/20251102_0313.html">20251102_0313</a>
<a href="archive/20251101_0314.html">20251101_0314</a>
<a href="archive/20251031_0314.html">20251031_0314</a>
<a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
