<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-30 03:17</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251030_0317</div>
    <div class="row"><div class="card">
<div class="title">Uniform Discrete Diffusion with Metric Path for Video Generation</div>
<div class="meta-line">Authors: Haoge Deng, Ting Pan, Fan Zhang, Yang Liu, Zhuoyan Luo, Yufeng Cui, Wenxuan Wang, Chunhua Shen, Shiguang Shan, Zhaoxiang Zhang, Xinlong Wang</div>
<div class="meta-line">First: 2025-10-28T17:59:57+00:00 · Latest: 2025-10-28T17:59:57+00:00</div>
<div class="meta-line">Comments: 19 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24717v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24717v1">PDF</a> · <a href="https://github.com/baaivision/URSA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency.</div>
</details>
</div>
<div class="card">
<div class="title">Memory Mosaics at scale</div>
<div class="meta-line">Authors: Jianyu Zhang, Léon Bottou</div>
<div class="meta-line">Venue: NeurIPS 2025 Oral</div>
<div class="meta-line">First: 2025-07-04T04:23:03+00:00 · Latest: 2025-10-28T17:59:36+00:00</div>
<div class="meta-line">Comments: Oral @ NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.03285v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.03285v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory Mosaics [Zhang et al., 2025], networks of associative memories, have
demonstrated appealing compositional and in-context learning capabilities on
medium-scale networks (GPT-2 scale) and synthetic small datasets. This work
shows that these favorable properties remain when we scale memory mosaics to
large language model sizes (llama-8B scale) and real-world datasets.
  To this end, we scale memory mosaics to 10B size, we train them on one
trillion tokens, we introduce a couple architectural modifications (&quot;Memory
Mosaics v2&quot;), we assess their capabilities across three evaluation dimensions:
training-knowledge storage, new-knowledge storage, and in-context learning.
  Throughout the evaluation, memory mosaics v2 match transformers on the
learning of training knowledge (first dimension) and significantly outperforms
transformers on carrying out new tasks at inference time (second and third
dimensions). These improvements cannot be easily replicated by simply
increasing the training data for transformers. A memory mosaics v2 trained on
one trillion tokens still perform better on these tasks than a transformer
trained on eight trillion tokens.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Memory Mosaics [Zhang et al., 2025], networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets.</div>
</details>
</div>
<div class="card">
<div class="title">Routing Matters in MoE: Scaling Diffusion Transformers with Explicit   Routing Guidance</div>
<div class="meta-line">Authors: Yujie Wei, Shiwei Zhang, Hangjie Yuan, Yujin Han, Zhekai Chen, Jiayu Wang, Difan Zou, Xihui Liu, Yingya Zhang, Yu Liu, Hongming Shan</div>
<div class="meta-line">First: 2025-10-28T17:59:02+00:00 · Latest: 2025-10-28T17:59:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24711v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency.</div>
</details>
</div>
<div class="card">
<div class="title">Tongyi DeepResearch Technical Report</div>
<div class="meta-line">Authors: Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang</div>
<div class="meta-line">First: 2025-10-28T17:53:02+00:00 · Latest: 2025-10-28T17:53:02+00:00</div>
<div class="meta-line">Comments: https://tongyi-agent.github.io/blog</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24701v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24701v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tongyi-agent.github.io/blog">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Tongyi DeepResearch, an agentic large language model, which is
specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is
developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent
interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total
parameters, with only 3.3 billion activated per token, achieves
state-of-the-art performance across a range of agentic deep research
benchmarks, including Humanity&#x27;s Last Exam, BrowseComp, BrowseComp-ZH,
WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We
open-source the model, framework, and complete solutions to empower the
community.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks.</div>
</details>
</div>
<div class="card">
<div class="title">ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking</div>
<div class="meta-line">Authors: Baixuan Li, Dingchu Zhang, Jialong Wu, Wenbiao Yin, Zhengwei Tao, Yida Zhao, Liwen Zhang, Haiyang Shen, Runnan Fang, Pengjun Xie, Jingren Zhou, Yong Jiang</div>
<div class="meta-line">First: 2025-10-28T17:51:50+00:00 · Latest: 2025-10-28T17:51:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24698v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parallel thinking expands exploration breadth, complementing the deep
exploration of information-seeking (IS) agents to further enhance
problem-solving capability. However, conventional parallel thinking faces two
key challenges in this setting: inefficiency from repeatedly rolling out from
scratch, and difficulty in integrating long-horizon reasoning trajectories
during answer generation, as limited context capacity prevents full
consideration of the reasoning process. To address these issues, we propose
ParallelMuse, a two-stage paradigm designed for deep IS agents. The first
stage, Functionality-Specified Partial Rollout, partitions generated sequences
into functional regions and performs uncertainty-guided path reuse and
branching to enhance exploration efficiency. The second stage, Compressed
Reasoning Aggregation, exploits reasoning redundancy to losslessly compress
information relevant to answer derivation and synthesize a coherent final
answer. Experiments across multiple open-source agents and benchmarks
demonstrate up to 62% performance improvement with a 10--30% reduction in
exploratory token consumption.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability.</div>
</details>
</div>
<div class="card">
<div class="title">Group Relative Attention Guidance for Image Editing</div>
<div class="meta-line">Authors: Xuanpu Zhang, Xuesong Niu, Ruidong Chen, Dan Song, Jianhao Zeng, Penghui Du, Haoxiang Cao, Kai Wu, An-an Liu</div>
<div class="meta-line">First: 2025-10-28T17:22:44+00:00 · Latest: 2025-10-28T17:22:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24657v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24657v1">PDF</a> · <a href="https://github.com/little-misfit/GRAG-Image-Editing">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model&#x27;s inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development.</div>
</details>
</div>
<div class="card">
<div class="title">RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video   Captioning</div>
<div class="meta-line">Authors: Yunchuan Ma, Laiyun Qing, Guorong Li, Yuankai Qi, Amin Beheshti, Quan Z. Sheng, Qingming Huang</div>
<div class="meta-line">First: 2024-05-11T16:22:00+00:00 · Latest: 2025-10-28T16:43:19+00:00</div>
<div class="meta-line">Comments: Published in Pattern Recognition</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2405.07046v3">Abs</a> · <a href="http://arxiv.org/pdf/2405.07046v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the significant progress of fully-supervised video captioning,
zero-shot methods remain much less explored. In this paper, we propose a novel
zero-shot video captioning framework named Retrieval-Enhanced Test-Time
Adaptation (RETTA), which takes advantage of existing pretrained large-scale
vision and language models to directly generate captions with test-time
adaptation. Specifically, we bridge video and text using four key models: a
general video-text retrieval model XCLIP, a general image-text matching model
CLIP, a text alignment model AnglE, and a text generation model GPT-2, due to
their source-code availability. The main challenge is how to enable the text
generation model to be sufficiently aware of the content in a given video so as
to generate corresponding captions. To address this problem, we propose using
learnable tokens as a communication medium among these four frozen models
GPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains
these tokens with training data, we propose to learn these tokens with soft
targets of the inference data under several carefully crafted loss functions,
which enable the tokens to absorb video information catered for GPT-2. This
procedure can be efficiently done in just a few iterations (we use 16
iterations in the experiments) and does not require ground truth data.
Extensive experimental results on three widely used datasets, MSR-VTT, MSVD,
and VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric
CIDEr compared to several state-of-the-art zero-shot video captioning methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite the significant progress of fully-supervised video captioning, zero-shot methods remain much less explored.</div>
</details>
</div>
<div class="card">
<div class="title">BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents</div>
<div class="meta-line">Authors: Litu Ou, Kuan Li, Huifeng Yin, Liwen Zhang, Zhongwang Zhang, Xixi Wu, Rui Ye, Zile Qiao, Pengjun Xie, Jingren Zhou, Yong Jiang</div>
<div class="meta-line">First: 2025-10-27T15:58:51+00:00 · Latest: 2025-10-28T16:23:04+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23458v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.23458v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Confidence in LLMs is a useful indicator of model uncertainty and answer reliability.</div>
</details>
</div>
<div class="card">
<div class="title">Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with   Leading Short-Context Accuracy</div>
<div class="meta-line">Authors: Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Yi-Fan Zhang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Shaohui Lin, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Ran He, Caifeng Shan, Rongrong Ji, Xing Sun</div>
<div class="meta-line">First: 2025-02-07T18:59:56+00:00 · Latest: 2025-10-28T16:02:48+00:00</div>
<div class="meta-line">Comments: https://github.com/VITA-MLLM/Long-VITA</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.05177v3">Abs</a> · <a href="http://arxiv.org/pdf/2502.05177v3">PDF</a> · <a href="https://github.com/VITA-MLLM/Long-VITA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Long-VITA, a simple yet effective large multi-modal model for
long-context visual-language understanding tasks. It is adept at concurrently
processing and analyzing modalities of image, video, and text over 4K frames or
1M tokens while delivering advanced performances on short-context multi-modal
tasks. We propose an effective multi-modal training schema that starts with
large language models and proceeds through vision-language alignment, general
knowledge learning, and two sequential stages of long-sequence fine-tuning. We
further implement context-parallelism distributed inference and logits-masked
language modeling head to scale Long-VITA to infinitely long inputs of images
and texts during model inference. Regarding training data, Long-VITA is built
on a mix of 17M samples from public datasets only and demonstrates
state-of-the-art performance on various multi-modal benchmarks, compared
against recent cutting-edge models with internal data. Long-VITA is fully
open-source and reproducible.. By leveraging our inference designs, Long-VITA
models achieve a remarkable 2x prefill speedup and 4x context length extension
in a single node with 8 GPUs. We hope Long-VITA can serve as a competitive
baseline and offer valuable insights for the open-source community in advancing
long-context multi-modal understanding.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce Long-VITA, a simple yet effective large multi-modal model for long-context visual-language understanding tasks.</div>
</details>
</div>
<div class="card">
<div class="title">TokenTiming: A Dynamic Alignment Method for Universal Speculative   Decoding Model Pairs</div>
<div class="meta-line">Authors: Sibo Xiao, Jinyuan Fu, Zhongle Xie, Lidan Shou</div>
<div class="meta-line">First: 2025-10-17T11:25:36+00:00 · Latest: 2025-10-28T15:23:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15545v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.15545v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accelerating the inference of large language models (LLMs) has been a
critical challenge in generative AI. Speculative decoding (SD) substantially
improves LLM inference efficiency. However, its utility is limited by a
fundamental constraint: the draft and target models must share the same
vocabulary, thus limiting the herd of available draft models and often
necessitating the training of a new model from scratch. Inspired by Dynamic
Time Warping (DTW), a classic algorithm for aligning time series, we propose
the algorithm TokenTiming for universal speculative decoding. It operates by
re-encoding the draft token sequence to get a new target token sequence, and
then uses DTW to build a mapping to transfer the probability distributions for
speculative sampling. Benefiting from this, our method accommodates mismatched
vocabularies and works with any off-the-shelf models without retraining and
modification. We conduct comprehensive experiments on various tasks,
demonstrating 1.57x speedup. This work enables a universal approach for draft
model selection, making SD a more versatile and practical tool for LLM
acceleration.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accelerating the inference of large language models (LLMs) has been a critical challenge in generative AI.</div>
</details>
</div>
<div class="card">
<div class="title">Exploration of Summarization by Generative Language Models for Automated   Scoring of Long Essays</div>
<div class="meta-line">Authors: Haowei Hua, Hong Jiao, Xinyi Wang</div>
<div class="meta-line">First: 2025-10-26T20:59:22+00:00 · Latest: 2025-10-28T14:43:58+00:00</div>
<div class="meta-line">Comments: 19 pages, 5 Tables 7 Figures, Presentation at Artificial Intelligence
  in Measurement and Education Conference (AIME-Con)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22830v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.22830v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">BERT and its variants are extensively explored for automated scoring.</div>
</details>
</div>
<div class="card">
<div class="title">AutoJudge: Judge Decoding Without Manual Annotation</div>
<div class="meta-line">Authors: Roman Garipov, Fedor Velikonivtsev, Ivan Ermakov, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin</div>
<div class="meta-line">First: 2025-04-28T17:59:28+00:00 · Latest: 2025-10-28T14:35:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.20039v3">Abs</a> · <a href="http://arxiv.org/pdf/2504.20039v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce AutoJudge, a method that accelerates large language model (LLM)
inference with task-specific lossy speculative decoding. Instead of matching
the original model output distribution token-by-token, we identify which of the
generated tokens affect the downstream quality of the response, relaxing the
distribution match guarantee so that the &quot;unimportant&quot; tokens can be generated
faster. Our approach relies on a semi-greedy search algorithm to test which of
the mismatches between target and draft models should be corrected to preserve
quality and which ones may be skipped. We then train a lightweight classifier
based on existing LLM embeddings to predict, at inference time, which
mismatching tokens can be safely accepted without compromising the final answer
quality. We evaluate the effectiveness of AutoJudge with multiple draft/target
model pairs on mathematical reasoning and programming benchmarks, achieving
significant speedups at the cost of a minor accuracy reduction. Notably, on
GSM8k with the Llama 3.1 70B target model, our approach achieves up to
$\approx2\times$ speedup over speculative decoding at the cost of $\le 1\%$
drop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge
automatically detects programming-specific important tokens, accepting $\ge 25$
tokens per speculation cycle at $2\%$ drop in Pass@1. Our approach requires no
human annotation and is easy to integrate with modern LLM inference frameworks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce AutoJudge, a method that accelerates large language model (LLM) inference with task-specific lossy speculative decoding.</div>
</details>
</div>
<div class="card">
<div class="title">OmniResponse: Online Multimodal Conversational Response Generation in   Dyadic Interactions</div>
<div class="meta-line">Authors: Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem</div>
<div class="meta-line">First: 2025-05-27T20:12:46+00:00 · Latest: 2025-10-28T14:26:23+00:00</div>
<div class="meta-line">Comments: 25 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.21724v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.21724v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task designed to produce synchronized verbal and
non-verbal listener feedback online, based on the speaker&#x27;s multimodal inputs.
OMCRG captures natural dyadic interactions and introduces new challenges in
aligning generated audio with listeners&#x27; facial responses. To tackle these
challenges, we incorporate text as an intermediate modality to connect audio
and facial responses. We propose OmniResponse, a Multimodal Large Language
Model (MLLM) that autoregressively generates accurate multimodal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two core
components: Chrono-Text Markup, which precisely timestamps generated text
tokens, and TempoVoice, a controllable online text-to-speech (TTS) module that
outputs speech synchronized with facial responses. To advance OMCRG research,
we offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring
synchronized split-screen videos, multichannel audio, transcripts, and
annotated facial behaviors. Comprehensive evaluations on ResponseNet
demonstrate that OmniResponse outperforms baseline models in terms of semantic
speech content, audio-visual synchronization, and generation quality. Our
dataset, code, and models are publicly available.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task designed to produce synchronized verbal and non-verbal listener feedback online, based on the speaker&#x27;s multimodal inputs.</div>
</details>
</div>
<div class="card">
<div class="title">Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding   in Vision-Language-Action Policies</div>
<div class="meta-line">Authors: Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Tian Nian, Liuao Pei, Shunbo Zhou, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo</div>
<div class="meta-line">First: 2025-08-27T17:39:11+00:00 · Latest: 2025-10-28T14:22:20+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.20072v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.20072v2">PDF</a> · <a href="https://github.com/Liang-ZX/DiscreteDiffusionVLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions into robot actions. However, prevailing VLAs either
generate actions auto-regressively in a fixed left-to-right order or attach
separate MLP or diffusion heads outside the backbone, leading to fragmented
information pathways and specialized training requirements that hinder a
unified, scalable architecture. We present Discrete Diffusion VLA, a
unified-transformer policy that models discretized action chunks with discrete
diffusion. The design retains diffusion&#x27;s progressive refinement paradigm while
remaining natively compatible with the discrete token interface of VLMs. Our
method achieves an adaptive decoding order that resolves easy action elements
before harder ones and uses secondary re-masking to revisit uncertain
predictions across refinement rounds, which improves consistency and enables
robust error correction. This unified decoder preserves pre-trained
vision-language priors, supports parallel decoding, breaks the autoregressive
bottleneck, and reduces the number of function evaluations. Discrete Diffusion
VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on
SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge, improving over
autoregressive, MLP decoder and continuous diffusion baselines. These findings
indicate that discrete-diffusion VLA supports precise action modeling and
consistent training, laying groundwork for scaling VLA to larger models and
datasets. Our project page is https://github.com/Liang-ZX/DiscreteDiffusionVLA</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions.</div>
</details>
</div>
<div class="card">
<div class="title">Improving LLM Reasoning via Dependency-Aware Query Decomposition and   Logic-Parallel Content Expansion</div>
<div class="meta-line">Authors: Xianjun Gao, Jianchun Liu, Hongli Xu, Liusheng Huang</div>
<div class="meta-line">First: 2025-10-28T13:05:23+00:00 · Latest: 2025-10-28T13:05:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24390v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24390v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The integration of Large Language Models (LLMs) into real-time Web applications, such as AI-powered search and conversational agents, presents a fundamental Web infrastructure challenge: reconciling the demand for high-quality, complex reasoning with the stringent low-latency and high-throughput requirements of interactive services.</div>
</details>
</div>
<div class="card">
<div class="title">LongWeave: A Long-Form Generation Benchmark Bridging Real-World   Relevance and Verifiability</div>
<div class="meta-line">Authors: Zikai Xiao, Fei Huang, Jianhong Tu, Jianhui Wei, Wen Ma, Yuxuan Zhou, Jian Wu, Bowen Yu, Zuozhu Liu, Junyang Lin</div>
<div class="meta-line">Venue: EMNLP</div>
<div class="meta-line">First: 2025-10-28T12:11:12+00:00 · Latest: 2025-10-28T12:11:12+00:00</div>
<div class="meta-line">Comments: EMNLP Findings 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24345v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24345v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating long, informative, and factual outputs remains a major challenge
for Large Language Models (LLMs). Existing benchmarks for long-form generation
typically assess real-world queries with hard-to-verify metrics or use
synthetic setups that ease evaluation but overlook real-world intricacies. In
this paper, we introduce \textbf{LongWeave}, which balances real-world and
verifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval
constructs tasks by first defining verifiable targets within real-world
scenarios, then systematically generating corresponding queries, textual
materials, and constraints based on these targets. This ensures that tasks are
both realistic and objectively assessable, enabling rigorous assessment of
model capabilities in meeting complex real-world constraints. LongWeave
supports customizable input/output lengths (up to 64K/8K tokens) across seven
distinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models
encounter significant challenges in long-form generation as real-world
complexity and output length increase.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Generating long, informative, and factual outputs remains a major challenge for Large Language Models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM   Reasoning</div>
<div class="meta-line">Authors: Aman Sharma, Paras Chopra</div>
<div class="meta-line">First: 2025-10-09T12:33:16+00:00 · Latest: 2025-10-28T10:58:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.08146v3">Abs</a> · <a href="http://arxiv.org/pdf/2510.08146v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a simple, yet novel entropy-based framework to drive token
efficiency in large language models during reasoning tasks. Our approach uses
Shannon entropy from token-level logprobs as a confidence signal to enable
early stopping, achieving 25-50% computational savings while maintaining task
accuracy. Crucially, we demonstrate that entropy-based confidence calibration
represents an emergent property of advanced post-training optimization present
in modern reasoning models but notably absent in standard instruction-tuned and
pre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop
reasoning varies from model to model but can be calculated easily in one shot
using only a few examples from existing reasoning datasets. Our results
indicate that advanced reasoning models often know that they&#x27;ve gotten a
correct answer early on, and that this emergent confidence awareness can be
exploited to save tokens and reduce latency. The framework demonstrates
consistent performance across reasoning-optimized model families with 25-50%
computational cost reduction while preserving accuracy, revealing that
confidence mechanisms represent a distinguishing characteristic of modern
post-trained reasoning systems versus their predecessors.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce a simple, yet novel entropy-based framework to drive token efficiency in large language models during reasoning tasks.</div>
</details>
</div>
<div class="card">
<div class="title">SALS: Sparse Attention in Latent Space for KV cache Compression</div>
<div class="meta-line">Authors: Junlin Mu, Hantao Huang, Jihang Zhang, Minghui Yu, Tao Wang, Yidong Li</div>
<div class="meta-line">First: 2025-10-28T10:32:52+00:00 · Latest: 2025-10-28T10:32:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24273v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24273v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models capable of handling extended contexts are in high
demand, yet their inference remains challenging due to substantial Key-Value
cache size and high memory bandwidth requirements. Previous research has
demonstrated that KV cache exhibits low-rank characteristics within the hidden
dimension, suggesting the potential for effective compression. However, due to
the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive
low-rank compression suffers severe accuracy degradation or creates a new speed
bottleneck, as the low-rank cache must first be reconstructed in order to apply
RoPE. In this paper, we introduce two key insights: first, the application of
RoPE to the key vectors increases their variance, which in turn results in a
higher rank; second, after the key vectors are transformed into the latent
space, they largely maintain their representation across most layers. Based on
these insights, we propose the Sparse Attention in Latent Space framework. SALS
projects the KV cache into a compact latent space via low-rank projection, and
performs sparse token selection using RoPE-free query-key interactions in this
space. By reconstructing only a small subset of important tokens, it avoids the
overhead of full KV cache reconstruction. We comprehensively evaluate SALS on
various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and
additionally verify its scalability on the RULER-128k benchmark with
LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA
performance by maintaining competitive accuracy. Under different settings, SALS
achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention
operator compared to FlashAttention2 on the 4K sequence. For the end-to-end
throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared
to GPT-fast on 4k and 32K sequences, respectively.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models capable of handling extended contexts are in high demand, yet their inference remains challenging due to substantial Key-Value cache size and high memory bandwidth requirements.</div>
</details>
</div>
<div class="card">
<div class="title">SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel   LLMs</div>
<div class="meta-line">Authors: Jinhong Deng, Wen Li, Joey Tianyi Zhou, Yang He</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-28T09:29:37+00:00 · Latest: 2025-10-28T09:29:37+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24214v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24214v1">PDF</a> · <a href="https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE">Code1</a> · <a href="https://github.com/kinredon/SCOPE">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) typically process a large number of visual tokens, leading to considerable computational overhead, even though many of these tokens are redundant.</div>
</details>
</div>
<div class="card">
<div class="title">MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive   Visual Generation Acceleration</div>
<div class="meta-line">Authors: Junhyuk So, Hyunho Kook, Chaeyeon Jang, Eunhyeok Park</div>
<div class="meta-line">First: 2025-10-28T09:26:27+00:00 · Latest: 2025-10-28T09:26:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24211v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24211v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While autoregressive (AR) modeling has recently emerged as a new paradigm in
visual generation, its practical adoption is severely constrained by the slow
inference speed of per-token generation, which often requires thousands of
steps to produce a single sample. To address this challenge, we propose MC-SJD,
a training-free, lossless parallel decoding framework designed to accelerate AR
visual generation by extending the recently introduced Speculative Jacobi
Decoding (SJD). Although SJD shows strong potential for accelerating AR
generation, we demonstrate that token instability across iterations
significantly reduces the acceptance rate, a limitation that primarily arises
from the independent sampling process used during draft token generation. To
overcome this, we introduce MC-SJD, an information-theoretic approach based on
coupling, which substantially accelerates standard SJD by maximizing the
probability of sampling identical draft tokens across consecutive iterations,
all while preserving its lossless property. Remarkably, this method requires
only a single-line modification to the existing algorithm, yet achieves
substantial performance gains, delivering up to a ~4.2x acceleration in image
generation and ~13.3x acceleration in video generation compared to standard AR
decoding, without any degradation in output quality.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While autoregressive (AR) modeling has recently emerged as a new paradigm in visual generation, its practical adoption is severely constrained by the slow inference speed of per-token generation, which often requires thousands of steps to produce a single sample.</div>
</details>
</div>
<div class="card">
<div class="title">EddyFormer: Accelerated Neural Simulations of Three-Dimensional   Turbulence at Scale</div>
<div class="meta-line">Authors: Yiheng Du, Aditi S. Krishnapriyan</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-28T08:27:37+00:00 · Latest: 2025-10-28T08:27:37+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24173v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24173v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computationally resolving turbulence remains a central challenge in fluid
dynamics due to its multi-scale interactions. Fully resolving large-scale
turbulence through direct numerical simulation (DNS) is computationally
prohibitive, motivating data-driven machine learning alternatives. In this
work, we propose EddyFormer, a Transformer-based spectral-element (SEM)
architecture for large-scale turbulence simulation that combines the accuracy
of spectral methods with the scalability of the attention mechanism. We
introduce an SEM tokenization that decomposes the flow into grid-scale and
subgrid-scale components, enabling capture of both local and global features.
We create a new three-dimensional isotropic turbulence dataset and train
EddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x
speedup over DNS. When applied to unseen domains up to 4x larger than in
training, EddyFormer preserves accuracy on physics-invariant metrics-energy
spectra, correlation functions, and structure functions-showing domain
generalization. On The Well benchmark suite of diverse turbulent flows,
EddyFormer resolves cases where prior ML models fail to converge, accurately
reproducing complex dynamics across a wide range of physical conditions.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Computationally resolving turbulence remains a central challenge in fluid dynamics due to its multi-scale interactions.</div>
</details>
</div>
<div class="card">
<div class="title">Context-level Language Modeling by Learning Predictive Context   Embeddings</div>
<div class="meta-line">Authors: Beiya Dai, Yuliang Liu, Daozheng Xue, Qipeng Guo, Kai Chen, Xinbing Wang, Bowen Zhou, Zhouhan Lin</div>
<div class="meta-line">First: 2025-10-23T07:09:45+00:00 · Latest: 2025-10-28T07:35:34+00:00</div>
<div class="meta-line">Comments: 16pages,6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20280v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.20280v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Next-token prediction (NTP) is the cornerstone of modern large language
models (LLMs) pretraining, driving their unprecedented capabilities in text
generation, reasoning, and instruction following. However, the token-level
prediction limits the model&#x27;s capacity to capture higher-level semantic
structures and long-range contextual relationships. To overcome this
limitation, we introduce \textbf{ContextLM}, a framework that augments standard
pretraining with an inherent \textbf{next-context prediction} objective. This
mechanism trains the model to learn predictive representations of multi-token
contexts, leveraging error signals derived from future token chunks. Crucially,
ContextLM achieves this enhancement while remaining fully compatible with the
standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).
Extensive experiments on the GPT2 and Pythia model families, scaled up to
$1.5$B parameters, show that ContextLM delivers consistent improvements in both
perplexity and downstream task performance. Our analysis indicates that
next-context prediction provides a scalable and efficient pathway to stronger
language modeling, yielding better long-range coherence and more effective
attention allocation with minimal computational overhead.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Next-token prediction (NTP) is the cornerstone of modern large language models (LLMs) pretraining, driving their unprecedented capabilities in text generation, reasoning, and instruction following.</div>
</details>
</div>
<div class="card">
<div class="title">Mixture-of-Experts Meets In-Context Reinforcement Learning</div>
<div class="meta-line">Authors: Wenhao Wu, Fuhong Liu, Haoru Li, Zican Hu, Daoyi Dong, Chunlin Chen, Zhi Wang</div>
<div class="meta-line">First: 2025-06-05T06:29:14+00:00 · Latest: 2025-10-28T06:55:14+00:00</div>
<div class="meta-line">Comments: 28 pages, 13 figures, 17 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.05426v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.05426v3">PDF</a> · <a href="https://github.com/NJU-RL/T2MIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In-context reinforcement learning (ICRL) has emerged as a promising paradigm
for adapting RL agents to downstream tasks through prompt conditioning.
However, two notable challenges remain in fully harnessing in-context learning
within RL domains: the intrinsic multi-modality of the state-action-reward data
and the diverse, heterogeneous nature of decision tasks. To tackle these
challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an
innovative framework that introduces architectural advances of
mixture-of-experts (MoE) into transformer-based decision models. T2MIR
substitutes the feedforward layer with two parallel layers: a token-wise MoE
that captures distinct semantics of input tokens across multiple modalities,
and a task-wise MoE that routes diverse tasks to specialized experts for
managing a broad task distribution with alleviated gradient conflicts. To
enhance task-wise routing, we introduce a contrastive learning method that
maximizes the mutual information between the task and its router
representation, enabling more precise capture of task-relevant information. The
outputs of two MoE components are concatenated and fed into the next layer.
Comprehensive experiments show that T2MIR significantly facilitates in-context
learning capacity and outperforms various types of baselines. We bring the
potential and promise of MoE to ICRL, offering a simple and scalable
architectural enhancement to advance ICRL one step closer toward achievements
in language and vision communities. Our code is available at
https://github.com/NJU-RL/T2MIR.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning.</div>
</details>
</div>
<div class="card">
<div class="title">Task-Agnostic Fusion of Time Series and Imagery for Earth Observation</div>
<div class="meta-line">Authors: Gianfranco Basile, Johannes Jakubik, Benedikt Blumenstiel, Thomas Brunschwiler, Juan Bernabe Moreno</div>
<div class="meta-line">First: 2025-10-27T08:38:52+00:00 · Latest: 2025-10-28T06:10:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23118v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.23118v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a task-agnostic framework for multimodal fusion of time series and
single timestamp images, enabling cross-modal generation and robust downstream
performance. Our approach explores deterministic and learned strategies for
time series quantization and then leverages a masked correlation learning
objective, aligning discrete image and time series tokens in a unified
representation space. Instantiated in the Earth observation domain, the
pretrained model generates consistent global temperature profiles from
satellite imagery and is validated through counterfactual experiments. Across
downstream tasks, our task-agnostic pretraining outperforms task-specific
fusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by
50\% in R$^2$ and 12\% in RMSE. Finally, we analyze gradient sensitivity across
modalities, providing insights into model robustness. Code, data, and weights
will be released under a permissive license.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose a task-agnostic framework for multimodal fusion of time series and single timestamp images, enabling cross-modal generation and robust downstream performance.</div>
</details>
</div>
<div class="card">
<div class="title">OmniText: A Training-Free Generalist for Controllable Text-Image   Manipulation</div>
<div class="meta-line">Authors: Agus Gunawan, Samuel Teodoro, Yun Chen, Soo Ye Kim, Jihyong Oh, Munchurl Kim</div>
<div class="meta-line">First: 2025-10-28T06:06:52+00:00 · Latest: 2025-10-28T06:06:52+00:00</div>
<div class="meta-line">Comments: The first two authors contributed equally to this work. The last two
  authors are co-corresponding authors</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24093v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24093v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model&#x27;s tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advancements in diffusion-based text synthesis have demonstrated significant performance in inserting and editing text within images via inpainting.</div>
</details>
</div>
<div class="card">
<div class="title">Switchable Token-Specific Codebook Quantization For Face Image   Compression</div>
<div class="meta-line">Authors: Yongbo Wang, Haonan Wang, Guodong Mu, Ruixin Zhang, Jiaqi Chen, Jingyun Zhang, Jun Wang, Yuan Xie, Zhizhong Zhang, Shouhong Ding</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-27T02:56:17+00:00 · Latest: 2025-10-28T05:57:57+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 accepted</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22943v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.22943v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the ever-increasing volume of visual data, the efficient and lossless
transmission, along with its subsequent interpretation and understanding, has
become a critical bottleneck in modern information systems. The emerged
codebook-based solution utilize a globally shared codebook to quantize and
dequantize each token, controlling the bpp by adjusting the number of tokens or
the codebook size. However, for facial images, which are rich in attributes,
such global codebook strategies overlook both the category-specific
correlations within images and the semantic differences among tokens, resulting
in suboptimal performance, especially at low bpp. Motivated by these
observations, we propose a Switchable Token-Specific Codebook Quantization for
face image compression, which learns distinct codebook groups for different
image categories and assigns an independent codebook to each token. By
recording the codebook group to which each token belongs with a small number of
bits, our method can reduce the loss incurred when decreasing the size of each
codebook group. This enables a larger total number of codebooks under a lower
overall bpp, thereby enhancing the expressive capability and improving
reconstruction performance. Owing to its generalizable design, our method can
be integrated into any existing codebook-based representation learning approach
and has demonstrated its effectiveness on face recognition datasets, achieving
an average accuracy of 93.51% for reconstructed images at 0.05 bpp.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the ever-increasing volume of visual data, the efficient and lossless transmission, along with its subsequent interpretation and understanding, has become a critical bottleneck in modern information systems.</div>
</details>
</div>
<div class="card">
<div class="title">MDP3: A Training-free Approach for List-wise Frame Selection in   Video-LLMs</div>
<div class="meta-line">Authors: Hui Sun, Shiyin Lu, Huanyu Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Ming Li</div>
<div class="meta-line">First: 2025-01-06T09:55:55+00:00 · Latest: 2025-10-28T04:18:29+00:00</div>
<div class="meta-line">Comments: 26 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.02885v2">Abs</a> · <a href="http://arxiv.org/pdf/2501.02885v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video large language models (Video-LLMs) have made significant progress in
understanding videos. However, processing multiple frames leads to lengthy
visual token sequences, presenting challenges such as the limited context
length cannot accommodate the entire video, and the inclusion of irrelevant
frames hinders visual perception. Hence, effective frame selection is crucial.
This paper emphasizes that frame selection should follow three key principles:
query relevance, list-wise diversity, and sequentiality. Existing methods, such
as uniform frame sampling and query-frame matching, do not capture all of these
principles. Thus, we propose Markov decision determinantal point process with
dynamic programming (MDP3) for frame selection, a training-free and
model-agnostic method that can be seamlessly integrated into existing
Video-LLMs. Our method first estimates frame similarities conditioned on the
query using a conditional Gaussian kernel within the reproducing kernel Hilbert
space~(RKHS). We then apply the determinantal point process~(DPP) to the
similarity matrix to capture both query relevance and list-wise diversity. To
incorporate sequentiality, we segment the video and apply DPP within each
segment, conditioned on the preceding segment selection, modeled as a Markov
decision process~(MDP) for allocating selection sizes across segments.
Theoretically, MDP3 provides a \((1 - 1/e)\)-approximate solution to the
NP-hard list-wise frame selection problem with pseudo-polynomial time
complexity, demonstrating its efficiency. Empirically, MDP3 significantly
outperforms existing methods, verifying its effectiveness and robustness.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video large language models (Video-LLMs) have made significant progress in understanding videos.</div>
</details>
</div>
<div class="card">
<div class="title">VSA: Faster Video Diffusion with Trainable Sparse Attention</div>
<div class="meta-line">Authors: Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, Hao Zhang</div>
<div class="meta-line">Venue: Neurips 2025</div>
<div class="meta-line">First: 2025-05-19T17:30:13+00:00 · Latest: 2025-10-28T04:13:18+00:00</div>
<div class="meta-line">Comments: Accepted by Neurips 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.13389v5">Abs</a> · <a href="http://arxiv.org/pdf/2505.13389v5">PDF</a> · <a href="https://github.com/hao-ai-lab/FastVideo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D
attention, even though most of the attention mass concentrates on a small
subset of positions. We turn this observation into VSA, a trainable,
hardware-efficient sparse attention that replaces full attention at \emph{both}
training and inference. In VSA, a lightweight coarse stage pools tokens into
tiles and identifies high-weight \emph{critical tokens}; a fine stage computes
token-level attention only inside those tiles subjecting to block computing
layout to ensure hard efficiency. This leads to a single differentiable kernel
that trains end-to-end, requires no post-hoc profiling, and sustains 85\% of
FlashAttention3 MFU. We perform a large sweep of ablation studies and
scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA
reaches a Pareto point that cuts training FLOPS by 2.53$\times$ with no drop in
diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention
time by 6$\times$ and lowers end-to-end generation time from 31s to 18s with
comparable quality. These results establish trainable sparse attention as a
practical alternative to full attention and a key enabler for further scaling
of video diffusion models. Code will be available at
https://github.com/hao-ai-lab/FastVideo.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D attention, even though most of the attention mass concentrates on a small subset of positions.</div>
</details>
</div>
<div class="card">
<div class="title">FastKV: KV Cache Compression for Fast Long-Context Processing with   Token-Selective Propagation</div>
<div class="meta-line">Authors: Dongwon Jo, Jiwon Song, Yulhwa Kim, Jae-Joon Kim</div>
<div class="meta-line">First: 2025-02-03T05:25:09+00:00 · Latest: 2025-10-28T04:00:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.01068v4">Abs</a> · <a href="http://arxiv.org/pdf/2502.01068v4">PDF</a> · <a href="https://github.com/dongwonjo/FastKV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While large language models (LLMs) excel at handling long-context sequences,
they require substantial prefill computation and key-value (KV) cache, which
can heavily burden computational efficiency and memory usage in both prefill
and decoding stages. Recent works that compress KV caches with prefill
acceleration reduce this cost but inadvertently tie the prefill compute
reduction to the decoding KV budget. This coupling arises from overlooking the
layer-dependent variation of critical context, often leading to accuracy
degradation. To address this issue, we introduce FastKV, a KV cache compression
framework designed to reduce latency in both prefill and decoding by leveraging
the stabilization of token importance in later layers. FastKV performs
full-context computation until a Token-Selective Propagation (TSP) layer, which
forwards only the most informative tokens to subsequent layers. From these
propagated tokens, FastKV independently selects salient KV entries for caching,
thereby decoupling KV budget from the prefill compute reduction based on the
TSP decision. This independent control of the TSP rate and KV retention rate
enables flexible optimization of efficiency and accuracy. Experimental results
show that FastKV achieves speedups of up to 1.82$\times$ in prefill and
2.87$\times$ in decoding compared to the full-context baseline, while matching
the accuracy of the baselines that only accelerate the decoding stage. Our code
is available at https://github.com/dongwonjo/FastKV.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages.</div>
</details>
</div>
<div class="card">
<div class="title">JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model</div>
<div class="meta-line">Authors: Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, Benjamin Wild</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-22T20:10:55+00:00 · Latest: 2025-10-28T03:53:33+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.17257v4">Abs</a> · <a href="http://arxiv.org/pdf/2505.17257v4">PDF</a> · <a href="https://github.com/Qihao-Duan/JanusDNA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have revolutionized natural language processing
and are increasingly applied to other sequential data types, including genetic
sequences. However, adapting LLMs to genomics presents significant challenges.
Capturing complex genomic interactions requires modeling long-range
dependencies within DNA sequences, where interactions often span over 10,000
base pairs, even within a single gene, posing substantial computational burdens
under conventional model architectures and training paradigms. Moreover,
standard LLM training approaches are suboptimal for DNA: autoregressive
training, while efficient, supports only unidirectional understanding. However,
DNA is inherently bidirectional, e.g., bidirectional promoters regulate
transcription in both directions and account for nearly 11% of human gene
expression. Masked language models (MLMs) allow bidirectional understanding but
are inefficient, as only masked tokens contribute to the loss per step. To
address these limitations, we introduce JanusDNA, the first bidirectional DNA
foundation model built upon a novel pretraining paradigm that combines the
optimization efficiency of autoregressive modeling with the bidirectional
comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and
Mixture of Experts (MoE) architecture, combining long-range modeling of
Attention with efficient sequential learning of Mamba. MoE layers further scale
model capacity via sparse activation while keeping computational cost low.
Notably, JanusDNA processes up to 1 million base pairs at single nucleotide
resolution on a single 80GB GPU. Extensive experiments and ablations show
JanusDNA achieves new SOTA results on three genomic representation benchmarks,
outperforming models with 250x more activated parameters. Code:
https://github.com/Qihao-Duan/JanusDNA</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
