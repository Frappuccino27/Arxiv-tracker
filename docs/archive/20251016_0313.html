<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-16 03:13</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251016_0313</div>
    <div class="row"><div class="card">
<div class="title">Detect Anything via Next Point Prediction</div>
<div class="meta-line">Authors: Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, Lei Zhang</div>
<div class="meta-line">First: 2025-10-14T17:59:54+00:00 · Latest: 2025-10-14T17:59:54+00:00</div>
<div class="meta-line">Comments: homepage: https://rex-omni.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12798v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12798v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rex-omni.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object detection has long been dominated by traditional coordinate
regression-based models, such as YOLO, DETR, and Grounding DINO. Although
recent efforts have attempted to leverage MLLMs to tackle this task, they face
challenges like low recall rate, duplicate predictions, coordinate
misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a
3B-scale MLLM that achieves state-of-the-art object perception performance. On
benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or
exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot
setting. This is enabled by three key designs: 1) Task Formulation: we use
special tokens to represent quantized coordinates from 0 to 999, reducing the
model&#x27;s learning difficulty and improving token efficiency for coordinate
prediction; 2) Data Engines: we construct multiple data engines to generate
high-quality grounding, referring, and pointing data, providing semantically
rich supervision for training; \3) Training Pipelines: we employ a two-stage
training process, combining supervised fine-tuning on 22 million data with
GRPO-based reinforcement post-training. This RL post-training leverages
geometry-aware rewards to effectively bridge the discrete-to-continuous
coordinate prediction gap, improve box accuracy, and mitigate undesirable
behaviors like duplicate predictions that stem from the teacher-guided nature
of the initial SFT stage. Beyond conventional detection, Rex-Omni&#x27;s inherent
language understanding enables versatile capabilities such as object referring,
pointing, visual prompting, GUI grounding, spatial referring, OCR and
key-pointing, all systematically evaluated on dedicated benchmarks. We believe
that Rex-Omni paves the way for more versatile and language-aware visual
perception systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO.</div>
</details>
</div>
<div class="card">
<div class="title">DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</div>
<div class="meta-line">Authors: Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, Lue Fan, Zhaoxiang Zhang</div>
<div class="meta-line">First: 2025-10-14T17:59:47+00:00 · Latest: 2025-10-14T17:59:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12796v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12796v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling Vision-Language-Action (VLA) models on large-scale data offers a
promising path to achieving a more generalized driving intelligence. However,
VLA models are limited by a ``supervision deficit&#x27;&#x27;: the vast model capacity is
supervised by sparse, low-dimensional actions, leaving much of their
representational power underutilized. To remedy this, we propose
\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to
predict future images. This task generates a dense, self-supervised signal that
compels the model to learn the underlying dynamics of the driving environment.
We showcase the paradigm&#x27;s versatility by instantiating it for two dominant VLA
archetypes: an autoregressive world model for VLAs that use discrete visual
tokens, and a diffusion world model for those operating on continuous visual
features. Building on the rich representations learned from world modeling, we
introduce a lightweight action expert to address the inference latency for
real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a
680x larger in-house dataset demonstrate that DriveVLA-W0 significantly
outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling
law, showing that performance gains accelerate as the training dataset size
increases.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence.</div>
</details>
</div>
<div class="card">
<div class="title">ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</div>
<div class="meta-line">Authors: Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, Wenhai Wang</div>
<div class="meta-line">First: 2025-10-14T17:58:10+00:00 · Latest: 2025-10-14T17:58:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12793v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12793v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Multimodal Large Language Models (MLLMs) suffer from increased
inference costs due to the additional vision tokens introduced by image inputs.
In this work, we propose Visual Consistency Learning (ViCO), a novel training
algorithm that enables the model to represent images of varying semantic
complexities using different numbers of vision tokens. The key idea behind our
method is to employ multiple MLP connectors, each with a different image
compression ratio, to downsample the vision tokens based on the semantic
complexity of the image. During training, we minimize the KL divergence between
the responses conditioned on different MLP connectors. At inference time, we
introduce an image router, termed Visual Resolution Router (ViR), that
automatically selects the appropriate compression rate for each image patch.
Compared with existing dynamic high-resolution strategies, which adjust the
number of visual tokens based on image resolutions, our method dynamically
adapts the number of visual tokens according to semantic complexity.
Experimental results demonstrate that our method can reduce the number of
vision tokens by up to 50% while maintaining the model&#x27;s perception, reasoning,
and OCR capabilities. We hope this work will contribute to the development of
more efficient MLLMs. The code and models will be released to facilitate future
research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs.</div>
</details>
</div>
<div class="card">
<div class="title">UniFusion: Vision-Language Model as Unified Encoder in Image Generation</div>
<div class="meta-line">Authors: Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale</div>
<div class="meta-line">First: 2025-10-14T17:57:56+00:00 · Latest: 2025-10-14T17:57:56+00:00</div>
<div class="meta-line">Comments: Project page at https://thekevinli.github.io/unifusion/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12789v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://thekevinli.github.io/unifusion/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although recent advances in visual generation have been remarkable, most
existing architectures still depend on distinct encoders for images and text.
This separation constrains diffusion models&#x27; ability to perform cross-modal
reasoning and knowledge transfer. Prior attempts to bridge this gap often use
the last layer information from VLM, employ multiple visual encoders, or train
large unified models jointly for text and image generation, which demands
substantial computational resources and large-scale data, limiting its
accessibility.We present UniFusion, a diffusion-based generative model
conditioned on a frozen large vision-language model (VLM) that serves as a
unified multimodal encoder. At the core of UniFusion is the Layerwise Attention
Pooling (LAP) mechanism that extracts both high level semantics and low level
details from text and visual tokens of a frozen VLM to condition a diffusion
generative model. We demonstrate that LAP outperforms other shallow fusion
architectures on text-image alignment for generation and faithful transfer of
visual information from VLM to the diffusion model which is key for editing. We
propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),
which conditions a diffusion transformer (DiT) only on the text tokens
generated by the VLM during in-model prompt rewriting. VERIFI combines the
alignment of the conditioning distribution with the VLM&#x27;s reasoning
capabilities for increased capabilities and flexibility at inference. In
addition, finetuning on editing task not only improves text-image alignment for
generation, indicative of cross-modality knowledge transfer, but also exhibits
tremendous generalization capabilities. Our model when trained on single image
editing, zero-shot generalizes to multiple image references further motivating
the unified encoder design of UniFusion.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text.</div>
</details>
</div>
<div class="card">
<div class="title">Dr.LLM: Dynamic Layer Routing in LLMs</div>
<div class="meta-line">Authors: Ahmed Heakl, Martin Gubri, Salman Khan, Sangdoo Yun, Seong Joon Oh</div>
<div class="meta-line">First: 2025-10-14T17:51:26+00:00 · Latest: 2025-10-14T17:51:26+00:00</div>
<div class="meta-line">Comments: 17 pages, Under submission</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12773v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12773v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning.</div>
</details>
</div>
<div class="card">
<div class="title">CARVQ: Corrective Adaptor with Group Residual Vector Quantization for   LLM Embedding Compression</div>
<div class="meta-line">Authors: Dayin Gou, Sanghyun Byun, Nilesh Malpeddi, Gabrielle De Micheli, Prathamesh Vaste, Jacob Song, Woo Seong Chung</div>
<div class="meta-line">Venue: EMNLP</div>
<div class="meta-line">First: 2025-10-14T17:00:13+00:00 · Latest: 2025-10-14T17:00:13+00:00</div>
<div class="meta-line">Comments: Accepted at EMNLP Findings 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12721v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12721v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) typically rely on a large number of parameters
for token embedding, leading to substantial storage requirements and memory
footprints. In particular, LLMs deployed on edge devices are memory-bound, and
reducing the memory footprint by compressing the embedding layer not only frees
up the memory bandwidth but also speeds up inference. To address this, we
introduce CARVQ, a post-training novel Corrective Adaptor combined with group
Residual Vector Quantization. CARVQ relies on the composition of both linear
and non-linear maps and mimics the original model embedding to compress to
approximately 1.6 bits without requiring specialized hardware to support
lower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,
LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B
and Phi-4, evaluating on common generative, discriminative, math and reasoning
tasks. We show that in most cases, CARVQ can achieve lower average
bitwidth-per-parameter while maintaining reasonable perplexity and accuracy
compared to scalar quantization. Our contributions include a novel compression
technique that is compatible with state-of-the-art transformer quantization
methods and can be seamlessly integrated into any hardware supporting 4-bit
memory to reduce the model&#x27;s memory footprint in memory-constrained devices.
This work demonstrates a crucial step toward the efficient deployment of LLMs
on edge devices.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) typically rely on a large number of parameters for token embedding, leading to substantial storage requirements and memory footprints.</div>
</details>
</div>
<div class="card">
<div class="title">Investigating Faithfulness in Large Audio Language Models</div>
<div class="meta-line">Authors: Lovenya Jain, Pooneh Mousavi, Mirco Ravanelli, Cem Subakan</div>
<div class="meta-line">First: 2025-09-26T13:58:22+00:00 · Latest: 2025-10-14T16:24:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.22363v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.22363v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Faithfulness measures whether chain-of-thought (CoT) representations
accurately reflect a model&#x27;s decision process and can be used as reliable
explanations. Prior work has shown that CoTs from text-based LLMs are often
unfaithful. This question has not been explored for large audio-language models
(LALMs), where faithfulness is critical for safety-sensitive applications.
Reasoning in LALMs is also more challenging, as models must first extract
relevant clues from audio before reasoning over them. In this paper, we
investigate the faithfulness of CoTs produced by several LALMs by applying
targeted interventions, including paraphrasing, filler token injection, early
answering, and introducing mistakes, on two challenging reasoning datasets:
SAKURA and MMAR. After going through the aforementioned interventions across
several datasets and tasks, our experiments suggest that, LALMs generally
produce CoTs that appear to be faithful to their underlying decision processes.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Faithfulness measures whether chain-of-thought (CoT) representations accurately reflect a model&#x27;s decision process and can be used as reliable explanations.</div>
</details>
</div>
<div class="card">
<div class="title">Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and   No-Think?</div>
<div class="meta-line">Authors: Shouren Wang, Wang Yang, Xianxuan Long, Qifan Wang, Vipin Chaudhary, Xiaotian Han</div>
<div class="meta-line">First: 2025-10-14T16:19:44+00:00 · Latest: 2025-10-14T16:19:44+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12680v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12680v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid thinking enables LLMs to switch between reasoning and direct
answering, offering a balance between efficiency and reasoning capability. Yet
our experiments reveal that current hybrid thinking LLMs only achieve partial
mode separation: reasoning behaviors often leak into the no-think mode. To
understand and mitigate this, we analyze the factors influencing
controllability and identify four that matter most: (1) larger data scale, (2)
using think and no-think answers from different questions rather than the same
question, (3) a moderate increase in no-think data number, and (4) a two-phase
strategy that first trains reasoning ability and then applies hybrid think
training. Building on these findings, we propose a practical recipe that,
compared to standard training, can maintain accuracy in both modes while
significantly reducing no-think output length (from $1085$ to $585$ on MATH500)
and occurrences of reasoning-supportive tokens such as ``\texttt{wait}&#x27;&#x27; (from
$5917$ to $522$ on MATH500). Our findings highlight the limitations of current
hybrid thinking and offer directions for strengthening its controllability.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Hybrid thinking enables LLMs to switch between reasoning and direct answering, offering a balance between efficiency and reasoning capability.</div>
</details>
</div>
<div class="card">
<div class="title">SG-XDEAT: Sparsity-Guided Cross-Dimensional and Cross-Encoding Attention   with Target-Aware Conditioning in Tabular Learning</div>
<div class="meta-line">Authors: Chih-Chuan Cheng, Yi-Ju Tseng</div>
<div class="meta-line">First: 2025-10-14T15:56:40+00:00 · Latest: 2025-10-14T15:56:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12659v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SG-XDEAT (Sparsity-Guided Cross Dimensional and Cross-Encoding
Attention with Target Aware Conditioning), a novel framework designed for
supervised learning on tabular data. At its core, SG-XDEAT employs a
dual-stream encoder that decomposes each input feature into two parallel
representations: a raw value stream and a target-conditioned (label-aware)
stream. These dual representations are then propagated through a hierarchical
stack of attention-based modules. SG-XDEAT integrates three key components: (i)
Cross-Dimensional self-attention, which captures intra-view dependencies among
features within each stream; (ii) Cross-Encoding self-attention, which enables
bidirectional interaction between raw and target-aware representations; and
(iii) an Adaptive Sparse Self-Attention (ASSA) mechanism, which dynamically
suppresses low-utility tokens by driving their attention weights toward
zero--thereby mitigating the impact of noise. Empirical results on multiple
public benchmarks show consistent gains over strong baselines, confirming that
jointly modeling raw and target-aware views--while adaptively filtering
noise--yields a more robust deep tabular learner.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose SG-XDEAT (Sparsity-Guided Cross Dimensional and Cross-Encoding Attention with Target Aware Conditioning), a novel framework designed for supervised learning on tabular data.</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging Importance Sampling to Detach Alignment Modules from Large   Language Models</div>
<div class="meta-line">Authors: Yi Liu, Dianqing Liu, Mingye Zhu, Junbo Guo, Yongdong Zhang, Zhendong Mao</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-26T08:53:02+00:00 · Latest: 2025-10-14T13:57:53+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025, 28 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.19700v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.19700v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread adoption of large language models (LLMs) across industries has
increased the demand for high-quality and customizable outputs. However,
traditional alignment methods often require retraining large pretrained models,
making it difficult to quickly adapt and optimize LLMs for diverse
applications. To address this limitation, we propose a novel \textit{Residual
Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type
of importance sampling. In this framework, the unaligned upstream model serves
as the proposal distribution, while the alignment process is framed as
secondary sampling based on an autoregressive alignment module that acts as an
estimator of the importance weights. This design enables a natural detachment
of the alignment module from the target aligned model, improving flexibility
and scalability. Based on this model, we derive an efficient sequence-level
training strategy for the alignment module, which operates independently of the
proposal module. Additionally, we develop a resampling algorithm with iterative
token-level decoding to address the common first-token latency issue in
comparable methods. Experimental evaluations on two leading open-source LLMs
across diverse tasks, including instruction following, domain adaptation, and
preference optimization, demonstrate that our approach consistently outperforms
baseline models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The widespread adoption of large language models (LLMs) across industries has increased the demand for high-quality and customizable outputs.</div>
</details>
</div>
<div class="card">
<div class="title">Neural Guided Sampling for Quantum Circuit Optimization</div>
<div class="meta-line">Authors: Bodo Rosenhahn, Tobias J. Osborne, Christoph Hirche</div>
<div class="meta-line">First: 2025-10-14T12:09:05+00:00 · Latest: 2025-10-14T12:09:05+00:00</div>
<div class="meta-line">Comments: 12 pages, 9 Figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12430v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12430v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Translating a general quantum circuit on a specific hardware topology with a
reduced set of available gates, also known as transpilation, comes with a
substantial increase in the length of the equivalent circuit. Due to
decoherence, the quality of the computational outcome can degrade seriously
with increasing circuit length. Thus, there is major interest to reduce a
quantum circuit to an equivalent circuit which is in its gate count as short as
possible. One method to address efficient transpilation is based on approaches
known from stochastic optimization, e.g. by using random sampling and token
replacement strategies. Here, a core challenge is that these methods can suffer
from sampling efficiency, causing long and energy consuming optimization time.
As a remedy, we propose in this work 2D neural guided sampling. Thus, given a
2D representation of a quantum circuit, a neural network predicts groups of
gates in the quantum circuit, which are likely reducible. Thus, it leads to a
sampling prior which can heavily reduce the compute time for quantum circuit
reduction. In several experiments, we demonstrate that our method is superior
to results obtained from different qiskit or BQSKit optimization levels.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Translating a general quantum circuit on a specific hardware topology with a reduced set of available gates, also known as transpilation, comes with a substantial increase in the length of the equivalent circuit.</div>
</details>
</div>
<div class="card">
<div class="title">Tokenization Disparities as Infrastructure Bias: How Subword Systems   Create Inequities in LLM Access and Efficiency</div>
<div class="meta-line">Authors: Hailay Kidu Teklehaymanot, Wolfgang Nejdl</div>
<div class="meta-line">First: 2025-10-14T11:14:38+00:00 · Latest: 2025-10-14T11:14:38+00:00</div>
<div class="meta-line">Comments: 6 pages 4 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12389v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12389v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Tokenization disparities pose a significant barrier to achieving equitable access to artificial intelligence across linguistically diverse populations.</div>
</details>
</div>
<div class="card">
<div class="title">MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise   Aggregation</div>
<div class="meta-line">Authors: Maike Behrendt, Stefan Sylvius Wagner, Stefan Harmeling</div>
<div class="meta-line">First: 2025-05-21T16:10:02+00:00 · Latest: 2025-10-14T10:59:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.15696v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.15696v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The [CLS] token in BERT is commonly used as a fixed-length representation for
classification tasks, yet prior work has shown that both other tokens and
intermediate layers encode valuable contextual information. In this work, we
study lightweight extensions to BERT that refine the [CLS] representation by
aggregating information across layers and tokens. Specifically, we explore
three modifications: (i) max-pooling the [CLS] token across multiple layers,
(ii) enabling the [CLS] token to attend over the entire final layer using an
additional multi-head attention (MHA) layer, and (iii) combining max-pooling
across the full sequence with MHA. Our approach, called MaxPoolBERT, enhances
BERT&#x27;s classification accuracy (especially on low-resource tasks) without
requiring new pre-training or significantly increasing model size. Experiments
on the GLUE benchmark show that MaxPoolBERT consistently achieves a better
performance than the standard BERT base model on low resource tasks of the GLUE
benchmark.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The [CLS] token in BERT is commonly used as a fixed-length representation for classification tasks, yet prior work has shown that both other tokens and intermediate layers encode valuable contextual information.</div>
</details>
</div>
<div class="card">
<div class="title">CiteBART: Learning to Generate Citations for Local Citation   Recommendation</div>
<div class="meta-line">Authors: Ege Yiğit Çelik, Selma Tekir</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2024-12-23T12:58:30+00:00 · Latest: 2025-10-14T10:29:53+00:00</div>
<div class="meta-line">Comments: This paper has been accepted to the EMNLP 2025 Main Conference. (19
  pages, 3 figures, 11 tables)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.17534v3">Abs</a> · <a href="http://arxiv.org/pdf/2412.17534v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Local citation recommendation (LCR) suggests a set of papers for a citation
placeholder within a given context. The task has evolved as generative
approaches have become more promising than the traditional pre-fetch and
re-rank-based state-of-the-art approaches. This paper introduces
citation-specific pre-training within an encoder-decoder architecture, where
author-date citation tokens are masked to learn to reconstruct them to fulfill
LCR. There are two variants for this pre-training. In the local context-only
base scheme (CiteBART-Base), the citation token in a local context is masked to
learn to predict the citation. The global version (CiteBART-Global) extends the
local context with the citing paper&#x27;s title and abstract to enrich the learning
signal. CiteBART-Global achieves state-of-the-art performance on LCR benchmarks
except for the FullTextPeerRead dataset, which is quite small to see the
advantage of generative pre-training. The effect is significant in the larger
benchmarks, e.g., Refseer and ArXiv., with the Refseer benchmark-trained model
emerging as the best-performing model. We perform comprehensive experiments,
including an ablation study, a qualitative analysis, and a taxonomy of
hallucinations with detailed statistics. Our analyses confirm that
CiteBART-Global has a cross-dataset generalization capability; the macro
hallucination rate (MaHR) at the top-3 predictions is 4\%, and when the
ground-truth is in the top-k prediction list, the hallucination tendency in the
other predictions drops significantly.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Local citation recommendation (LCR) suggests a set of papers for a citation placeholder within a given context.</div>
</details>
</div>
<div class="card">
<div class="title">Traveling Salesman-Based Token Ordering Improves Stability in   Homomorphically Encrypted Language Models</div>
<div class="meta-line">Authors: Donghwan Rho, Sieun Seo, Hyewon Sung, Chohong Min, Ernest K. Ryu</div>
<div class="meta-line">First: 2025-10-14T09:56:50+00:00 · Latest: 2025-10-14T09:56:50+00:00</div>
<div class="meta-line">Comments: 34 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12343v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12343v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As users increasingly interact with large language models (LLMs) using
private information, secure and encrypted communication becomes essential.
Homomorphic encryption (HE) provides a principled solution by enabling
computation directly on encrypted data. Although prior work has explored
aspects of running LLMs under HE, the challenge of text generation,
particularly next-token prediction, has received limited attention and remains
a key obstacle to practical encrypted interaction. In this work, we propose a
TSP-based token reordering strategy to address the difficulties of encrypted
text generation, together with a post-processing step that further reduces
approximation error. Theoretical analysis and experimental results demonstrate
that our method prevents collapse, improves coherence in generated text, and
preserves data privacy throughout. Overall, our contributions advance the
feasibility of practical and privacy-preserving LLM inference.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As users increasingly interact with large language models (LLMs) using private information, secure and encrypted communication becomes essential.</div>
</details>
</div>
<div class="card">
<div class="title">LazyEviction: Lagged KV Eviction with Attention Pattern Observation for   Efficient Long Reasoning</div>
<div class="meta-line">Authors: Haoyue Zhang, Hualei Zhang, Xiaosong Ma, Jie Zhang, Song Guo</div>
<div class="meta-line">First: 2025-06-19T02:25:04+00:00 · Latest: 2025-10-14T09:14:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.15969v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.15969v2">PDF</a> · <a href="https://github.com/Halo-949/LazyEviction">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) exhibit enhanced capabilities by
Chain-of-Thought reasoning. However, the extended reasoning sequences introduce
significant GPU memory overhead due to increased key-value (KV) cache. Existing
KV cache compression methods mitigate memory bottlenecks but struggle in long
reasoning tasks. In this paper, we analyze attention patterns in reasoning
tasks and reveal a \textbf{Token Importance Recurrence} phenomenon: a large
proportion of tokens regain high attention after multiple decoding steps, which
is failed to capture by existing works and may lead to unpredictable eviction
on such periodically critical tokens. To address this, we propose
\textbf{LazyEviction}, an observation window-based lagged eviction framework
retaining latent recurring tokens by prioritized eviction based on tokens&#x27;
recurrence patterns. Extensive experiments demonstrate that LazyEviction
reduces KV cache by 50\%\textasciitilde70\% while maintaining comparable
accuracy, outperforming existing KV cache compression baselines. Our
implementation code can be found at https://github.com/Halo-949/LazyEviction.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) exhibit enhanced capabilities by Chain-of-Thought reasoning.</div>
</details>
</div>
<div class="card">
<div class="title">Learning Adaptive and Temporally Causal Video Tokenization in a 1D   Latent Space</div>
<div class="meta-line">Authors: Yan Li, Changyao Tian, Renqiu Xia, Ning Liao, Weiwei Guo, Junchi Yan, Hongsheng Li, Jifeng Dai, Hao Li, Xue Yang</div>
<div class="meta-line">First: 2025-05-22T17:59:02+00:00 · Latest: 2025-10-14T09:11:06+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/VisionXLab/AdapTok</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.17011v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.17011v2">PDF</a> · <a href="https://github.com/VisionXLab/AdapTok">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose AdapTok, an adaptive temporal causal video tokenizer that can
flexibly allocate tokens for different frames based on video content. AdapTok
is equipped with a block-wise masking strategy that randomly drops tail tokens
of each block during training, and a block causal scorer to predict the
reconstruction quality of video frames using different numbers of tokens.
During inference, an adaptive token allocation strategy based on integer linear
programming is further proposed to adjust token usage given predicted scores.
Such design allows for sample-wise, content-aware, and temporally dynamic token
allocation under a controllable overall budget. Extensive experiments for video
reconstruction and generation on UCF-101 and Kinetics-600 demonstrate the
effectiveness of our approach. Without additional image data, AdapTok
consistently improves reconstruction quality and generation performance under
different token budgets, allowing for more scalable and token-efficient
generative video modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose AdapTok, an adaptive temporal causal video tokenizer that can flexibly allocate tokens for different frames based on video content.</div>
</details>
</div>
<div class="card">
<div class="title">BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals</div>
<div class="meta-line">Authors: Qinfan Xiao, Ziyun Cui, Chi Zhang, Siqi Chen, Wen Wu, Andrew Thwaites, Alexandra Woolgar, Bowen Zhou, Chao Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-18T14:07:14+00:00 · Latest: 2025-10-14T09:01:41+00:00</div>
<div class="meta-line">Comments: Accepted by the 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.18185v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.18185v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural
activity non-invasively by capturing electromagnetic fields generated by
dendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit
distinct signal patterns, further complicated by variations in sensor
configurations across modalities and recording devices. Existing approaches
typically rely on separate, modality- and dataset-specific models, which limits
the performance and cross-domain scalability. This paper proposes BrainOmni,
the first brain foundation model that generalises across heterogeneous EEG and
MEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the
first tokenizer that quantises spatiotemporal brain activity into discrete
representations. Central to BrainTokenizer is a novel Sensor Encoder that
encodes sensor properties such as spatial layout, orientation, and type,
enabling compatibility across devices and modalities. Building upon the
discrete representations, BrainOmni learns unified semantic embeddings of brain
signals by self-supervised pretraining. To the best of our knowledge, it is the
first foundation model to support both EEG and MEG signals, as well as the
first to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG
and 656 hours of MEG data are curated and standardised from publicly available
sources for pretraining. Experiments show that BrainOmni outperforms both
existing foundation models and state-of-the-art task-specific models on a range
of downstream tasks. It also demonstrates strong generalisation to unseen EEG
and MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training
yields consistent improvements across both modalities. Code and model
checkpoints will be released upon acceptance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural activity non-invasively by capturing electromagnetic fields generated by dendritic currents.</div>
</details>
</div>
<div class="card">
<div class="title">Chinese ModernBERT with Whole-Word Masking</div>
<div class="meta-line">Authors: Zeyu Zhao, Ningtao Wang, Xing Fu, Yu Cheng</div>
<div class="meta-line">First: 2025-10-14T08:41:22+00:00 · Latest: 2025-10-14T08:41:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12285v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12285v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -&gt; 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Encoder-only Transformers have advanced along three axes -- architecture, data, and systems -- yielding Pareto gains in accuracy, speed, and memory efficiency.</div>
</details>
</div>
<div class="card">
<div class="title">HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain   Generalization</div>
<div class="meta-line">Authors: Ziyi Han, Huanyu Wang, Zeyu Zhang, Xiangxiang Dai, Xutong Liu, John C. S. Lui</div>
<div class="meta-line">First: 2025-10-14T08:19:13+00:00 · Latest: 2025-10-14T08:19:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12266v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12266v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Rank Adaptation (LoRA) has emerged as a widely used technique for
adapting large language models (LLMs) to new domains, due to its modular design
and broad availability on platforms such as HuggingFace. This availability has
motivated efforts to reuse existing LoRAs for domain generalization.
  However, existing methods often rely on explicit task labels or additional
training, which are impractical for deployment. Moreover, they typically
activate a fixed number of entire LoRA modules, leading to parameter redundancy
or insufficiency that degrade performance.
  In this paper, we propose \texttt{HiLoRA}, a training-free framework that
performs adaptive hierarchical routing over LoRA pools. Drawing on structural
properties of LoRA, we define rank-one components (ROCs), in which each rank
parameter is regarded as an independent unit. For a given input sequence,
\texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their
ROC allocation based on Gaussian likelihoods at the sequence level. At the
token level, it further refines routing by activating only the most informative
ROCs.
  We further provide theoretical guarantees that \texttt{HiLoRA} selects the
most relevant LoRAs with high probability.
  Extensive experiments show that \texttt{HiLoRA} achieves substantial
improvements in domain generalization, with accuracy gains of up to {\small
$55\%$} over state-of-the-art baselines, while maintaining comparable inference
throughput.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Low-Rank Adaptation (LoRA) has emerged as a widely used technique for adapting large language models (LLMs) to new domains, due to its modular design and broad availability on platforms such as HuggingFace.</div>
</details>
</div>
<div class="card">
<div class="title">$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for   Active Reasoning</div>
<div class="meta-line">Authors: Deyu Zou, Yongqiang Chen, Jianxiang Wang, Haochen Yang, Mufei Li, James Cheng, Pan Li, Yu Gong</div>
<div class="meta-line">First: 2025-10-14T08:14:49+00:00 · Latest: 2025-10-14T08:14:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12264v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12264v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Active reasoning requires large language models (LLMs) to interact with external sources and strategically gather information to solve problems.</div>
</details>
</div>
<div class="card">
<div class="title">BIGFix: Bidirectional Image Generation with Token Fixing</div>
<div class="meta-line">Authors: Victor Besnier, David Hurych, Andrei Bursuc, Eduardo Valle</div>
<div class="meta-line">First: 2025-10-14T07:34:44+00:00 · Latest: 2025-10-14T07:34:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12231v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12231v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in image and video generation have raised significant
interest from both academia and industry. A key challenge in this field is
improving inference efficiency, as model size and the number of inference steps
directly impact the commercial viability of generative models while also posing
fundamental scientific challenges. A promising direction involves combining
auto-regressive sequential token modeling with multi-token prediction per step,
reducing inference time by up to an order of magnitude. However, predicting
multiple tokens in parallel can introduce structural inconsistencies due to
token incompatibilities, as capturing complex joint dependencies during
training remains challenging. Traditionally, once tokens are sampled, there is
no mechanism to backtrack and refine erroneous predictions. We propose a method
for self-correcting image generation by iteratively refining sampled tokens. We
achieve this with a novel training scheme that injects random tokens in the
context, improving robustness and enabling token fixing during sampling. Our
method preserves the efficiency benefits of parallel token prediction while
significantly enhancing generation quality. We evaluate our approach on image
generation using the ImageNet-256 and CIFAR-10 datasets, as well as on video
generation with UCF-101 and NuScenes, demonstrating substantial improvements
across both modalities.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in image and video generation have raised significant interest from both academia and industry.</div>
</details>
</div>
<div class="card">
<div class="title">DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early   Time-Series Classification</div>
<div class="meta-line">Authors: Tao Xie, Zexi Tan, Haoyi Xiao, Binbin Sun, Yiqun Zhang</div>
<div class="meta-line">First: 2025-10-14T07:10:05+00:00 · Latest: 2025-10-14T07:10:05+00:00</div>
<div class="meta-line">Comments: Accepted to IEEE BIBM 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12214v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12214v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early time-series classification (ETSC) in medical applications is crucial
for time-sensitive scenarios such as sepsis prediction in intensive care units
(ICUs), where a large number of deaths are caused by delayed prediction. ETSC
can significantly improve ICU resource utilization efficiency and healthcare
precision. However, it faces conflicting goals of accuracy and earliness, with
existing methods often trading one for the other, struggling to capture subtle
early-stage patterns due to weak initial signals and class imbalance. The key
to solve these challenges is to find shapelets, which are discriminative
subsequences (or shapes) with high interpretability in time-series
classification. This paper proposes Dual-Enhanced Soft-Sparse-Shape Learning
for Medical Early Time-Series Classification (DE3S), which introduces a novel
Dual-Enhanced Soft-Shape Learning framework to figure out shapelets precisely
through three innovations: (1) a comprehensive dual-enhancement strategy
combines traditional temporal augmentation with attention-based global temporal
enhancement for robust representation learning, (2) an attention-score-based
soft shapelet sparsification mechanism dynamically preserves discriminative
patterns while aggregating less important shapelets into representative tokens,
and (3) a dual-path Mixture of Experts Network (MoE) and Inception modules
fusion architecture where MoE performs local learning within shapelets and
multi-scale Inception modules capture global patterns across shapelets. The
framework employs weighted cross-entropy loss for class imbalance handling and
demonstrates robustness on subject-consistency datasets. Extensive experiments
on six real-world medical datasets show state-of-the-art performance, with
ablation studies confirming component efficacy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Early time-series classification (ETSC) in medical applications is crucial for time-sensitive scenarios such as sepsis prediction in intensive care units (ICUs), where a large number of deaths are caused by delayed prediction.</div>
</details>
</div>
<div class="card">
<div class="title">DiSTAR: Diffusion over a Scalable Token Autoregressive Representation   for Speech Generation</div>
<div class="meta-line">Authors: Yakun Song, Xiaobin Zhuang, Jiawei Chen, Zhikang Niu, Guanrou Yang, Chenpeng Du, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen</div>
<div class="meta-line">First: 2025-10-14T07:03:29+00:00 · Latest: 2025-10-14T07:03:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12210v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent attempts to interleave autoregressive (AR) sketchers with
diffusion-based refiners over continuous speech representations have shown
promise, but they remain brittle under distribution shift and offer limited
levers for controllability. We introduce DISTAR, a zero-shot text-to-speech
framework that operates entirely in a discrete residual vector quantization
(RVQ) code space and tightly couples an AR language model with a masked
diffusion model, without forced alignment or a duration predictor. Concretely,
DISTAR drafts block-level RVQ tokens with an AR language model and then
performs parallel masked-diffusion infilling conditioned on the draft to
complete the next block, yielding long-form synthesis with blockwise
parallelism while mitigating classic AR exposure bias. The discrete code space
affords explicit control at inference: DISTAR produces high-quality audio under
both greedy and sample-based decoding using classifier-free guidance, supports
trade-offs between robustness and diversity, and enables variable bit-rate and
controllable computation via RVQ layer pruning at test time. Extensive
experiments and ablations demonstrate that DISTAR surpasses state-of-the-art
zero-shot TTS systems in robustness, naturalness, and speaker/style
consistency, while maintaining rich output diversity. Audio samples are
provided on https://anonymous.4open.science/w/DiSTAR_demo.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability.</div>
</details>
</div>
<div class="card">
<div class="title">State Space Prompting via Gathering and Spreading Spatio-Temporal   Information for Video Understanding</div>
<div class="meta-line">Authors: Jiahuan Zhou, Kai Zhu, Zhenyu Cui, Zichen Liu, Xu Zou, Gang Hua</div>
<div class="meta-line">First: 2025-10-14T05:30:36+00:00 · Latest: 2025-10-14T05:30:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12160v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, pre-trained state space models have shown great potential for video
classification, which sequentially compresses visual tokens in videos with
linear complexity, thereby improving the processing efficiency of video data
while maintaining high performance. To apply powerful pre-trained models to
downstream tasks, prompt learning is proposed to achieve efficient downstream
task adaptation with only a small number of fine-tuned parameters. However, the
sequentially compressed visual prompt tokens fail to capture the spatial and
temporal contextual information in the video, thus limiting the effective
propagation of spatial information within a video frame and temporal
information between frames in the state compression model and the extraction of
discriminative information. To tackle the above issue, we proposed a State
Space Prompting (SSP) method for video understanding, which combines
intra-frame and inter-frame prompts to aggregate and propagate key
spatiotemporal information in the video. Specifically, an Intra-Frame Gathering
(IFG) module is designed to aggregate spatial key information within each
frame. Besides, an Inter-Frame Spreading (IFS) module is designed to spread
discriminative spatio-temporal information across different frames. By
adaptively balancing and compressing key spatio-temporal information within and
between frames, our SSP effectively propagates discriminative information in
videos in a complementary manner. Extensive experiments on four video benchmark
datasets verify that our SSP significantly outperforms existing SOTA methods by
2.76% on average while reducing the overhead of fine-tuning parameters.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently, pre-trained state space models have shown great potential for video classification, which sequentially compresses visual tokens in videos with linear complexity, thereby improving the processing efficiency of video data while maintaining high performance.</div>
</details>
</div>
<div class="card">
<div class="title">Massive Activations are the Key to Local Detail Synthesis in Diffusion   Transformers</div>
<div class="meta-line">Authors: Chaofan Gan, Zicheng Zhao, Yuanpeng Tu, Xi Chen, Ziran Qin, Tieyuan Chen, Mehrtash Harandi, Weiyao Lin</div>
<div class="meta-line">First: 2025-10-13T15:39:13+00:00 · Latest: 2025-10-14T04:40:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11538v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.11538v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) have recently emerged as a powerful backbone
for visual generation. Recent observations reveal \emph{Massive Activations}
(MAs) in their internal feature maps, yet their function remains poorly
understood. In this work, we systematically investigate these activations to
elucidate their role in visual generation. We found that these massive
activations occur across all spatial tokens, and their distribution is
modulated by the input timestep embeddings. Importantly, our investigations
further demonstrate that these massive activations play a key role in local
detail synthesis, while having minimal impact on the overall semantic content
of output. Building on these insights, we propose \textbf{D}etail
\textbf{G}uidance (\textbf{DG}), a MAs-driven, training-free self-guidance
strategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG
constructs a degraded ``detail-deficient&#x27;&#x27; model by disrupting MAs and
leverages it to guide the original network toward higher-quality detail
synthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG),
enabling further refinements of fine-grained details. Extensive experiments
demonstrate that our DG consistently improves fine-grained detail quality
across various pre-trained DiTs (\eg, SD3, SD3.5, and Flux).</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for visual generation.</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Language Models Know the Answer Before Decoding</div>
<div class="meta-line">Authors: Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu</div>
<div class="meta-line">First: 2025-08-27T15:40:25+00:00 · Latest: 2025-10-14T03:42:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.19982v3">Abs</a> · <a href="http://arxiv.org/pdf/2508.19982v3">PDF</a> · <a href="https://github.com/pixeli99/Prophet">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) have recently emerged as an alternative to
autoregressive approaches, offering parallel sequence generation and flexible
token orders. However, their inference remains slower than that of
autoregressive models, primarily due to the cost of bidirectional attention and
the large number of refinement steps required for high quality outputs. In this
work, we highlight and leverage an overlooked property of DLMs early answer
convergence: in many cases, the correct answer can be internally identified by
half steps before the final decoding step, both under semi-autoregressive and
random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%
of instances, respectively, can be decoded correctly using only half of the
refinement steps. Building on this observation, we introduce Prophet, a
training-free fast decoding paradigm that enables early commit decoding.
Specifically, Prophet dynamically decides whether to continue refinement or to
go &quot;all-in&quot; (i.e., decode all remaining tokens in one step), using the
confidence gap between the top-2 prediction candidates as the criterion. It
integrates seamlessly into existing DLM implementations, incurs negligible
overhead, and requires no additional training. Empirical evaluations of
LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the
number of decoding steps by up to 3.4x while preserving high generation
quality. These results recast DLM decoding as a problem of when to stop
sampling, and demonstrate that early decode convergence provides a simple yet
powerful mechanism for accelerating DLM inference, complementary to existing
speedup techniques. Our code is publicly available at
https://github.com/pixeli99/Prophet.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders.</div>
</details>
</div>
<div class="card">
<div class="title">Understanding the Modality Gap: An Empirical Study on the Speech-Text   Alignment Mechanism of Large Speech Language Models</div>
<div class="meta-line">Authors: Bajian Xiang, Shuaijiang Zhao, Tingwei Guo, Wei Zou</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-10-14T03:34:38+00:00 · Latest: 2025-10-14T03:34:38+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025 (Main Conference)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12116v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12116v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive conversational generation abilities, yet consistently fall short of traditional pipeline systems on semantic understanding benchmarks.</div>
</details>
</div>
<div class="card">
<div class="title">Influence Dynamics and Stagewise Data Attribution</div>
<div class="meta-line">Authors: Jin Hwa Lee, Matthew Smith, Maxwell Adam, Jesse Hoogland</div>
<div class="meta-line">First: 2025-10-14T02:21:23+00:00 · Latest: 2025-10-14T02:21:23+00:00</div>
<div class="meta-line">Comments: 28 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12071v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12071v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current training data attribution (TDA) methods treat the influence one
sample has on another as static, but neural networks learn in distinct stages
that exhibit changing patterns of influence. In this work, we introduce a
framework for stagewise data attribution grounded in singular learning theory.
We predict that influence can change non-monotonically, including sign flips
and sharp peaks at developmental transitions. We first validate these
predictions analytically and empirically in a toy model, showing that dynamic
shifts in influence directly map to the model&#x27;s progressive learning of a
semantic hierarchy. Finally, we demonstrate these phenomena at scale in
language models, where token-level influence changes align with known
developmental stages.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current training data attribution (TDA) methods treat the influence one sample has on another as static, but neural networks learn in distinct stages that exhibit changing patterns of influence.</div>
</details>
</div>
<div class="card">
<div class="title">Your VAR Model is Secretly an Efficient and Explainable Generative   Classifier</div>
<div class="meta-line">Authors: Yi-Chung Chen, David I. Inouye, Jing Gao</div>
<div class="meta-line">First: 2025-10-14T01:59:01+00:00 · Latest: 2025-10-14T01:59:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12060v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.12060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative classifiers, which leverage conditional generative models for
classification, have recently demonstrated desirable properties such as
robustness to distribution shifts. However, recent progress in this area has
been largely driven by diffusion-based models, whose substantial computational
cost severely limits scalability. This exclusive focus on diffusion-based
methods has also constrained our understanding of generative classifiers. In
this work, we propose a novel generative classifier built on recent advances in
visual autoregressive (VAR) modeling, which offers a new perspective for
studying generative classifiers. To further enhance its performance, we
introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a
superior trade-off between accuracy and inference speed, thereby significantly
improving practical applicability. Moreover, we show that the VAR-based method
exhibits fundamentally different properties from diffusion-based methods. In
particular, due to its tractable likelihood, the VAR-based classifier enables
visual explainability via token-wise mutual information and demonstrates
inherent resistance to catastrophic forgetting in class-incremental learning
tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Generative classifiers, which leverage conditional generative models for classification, have recently demonstrated desirable properties such as robustness to distribution shifts.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
