<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-29 03:15</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251029_0315</div>
    <div class="row"><div class="card">
<div class="title">Variational Masked Diffusion Models</div>
<div class="meta-line">Authors: Yichi Zhang, Alex Schwing, Zhizhen Zhao</div>
<div class="meta-line">First: 2025-10-27T17:59:57+00:00 · Latest: 2025-10-27T17:59:57+00:00</div>
<div class="meta-line">Comments: Project Page: https://riccizz.github.io/VMD</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23606v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23606v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://riccizz.github.io/VMD">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked diffusion models have recently emerged as a flexible framework for
discrete generative modeling. However, a key limitation of standard masked
diffusion is its inability to effectively capture dependencies among tokens
that are predicted concurrently, leading to degraded generation quality when
dependencies among tokens are important. To explicitly model dependencies among
tokens, we propose Variational Masked Diffusion (VMD), a framework that
introduces latent variables into the masked diffusion process. Through
controlled experiments on synthetic datasets, we demonstrate that VMD
successfully learns dependencies that conventional masked diffusion fails to
capture. We further validate the effectiveness of our approach on Sudoku
puzzles and text datasets, where learning of dependencies among tokens improves
global consistency. Across these domains, VMD enhances both generation quality
and dependency awareness, highlighting the value of integrating variational
inference into masked diffusion. Our code is available at:
https://riccizz.github.io/VMD.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling.</div>
</details>
</div>
<div class="card">
<div class="title">PixelRefer: A Unified Framework for Spatio-Temporal Object Referring   with Arbitrary Granularity</div>
<div class="meta-line">Authors: Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi</div>
<div class="meta-line">First: 2025-10-27T17:59:32+00:00 · Latest: 2025-10-27T17:59:32+00:00</div>
<div class="meta-line">Comments: 22 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23603v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23603v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have demonstrated strong
general-purpose capabilities in open-world visual comprehension. However, most
existing MLLMs primarily focus on holistic, scene-level understanding, often
overlooking the need for fine-grained, object-centric reasoning. In this paper,
we present PixelRefer, a unified region-level MLLM framework that enables
advanced fine-grained understanding over user-specified regions across both
images and videos. Motivated by the observation that LLM attention
predominantly focuses on object-level tokens, we propose a Scale-Adaptive
Object Tokenizer (SAOT) to generate compact and semantically rich object
representations from free-form regions. Our analysis reveals that global visual
tokens contribute mainly in early LLM layers, inspiring the design of
PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion
module to pre-fuse global context into object tokens. This yields a lightweight
Object-Only Framework that substantially reduces computational cost while
maintaining high semantic fidelity. To facilitate fine-grained instruction
tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction
dataset. Extensive experiments across a range of benchmarks validate that
PixelRefer achieves leading performance with fewer training samples, while
PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension.</div>
</details>
</div>
<div class="card">
<div class="title">Alita-G: Self-Evolving Generative Agent for Agent Generation</div>
<div class="meta-line">Authors: Jiahao Qiu, Xuan Qi, Hongru Wang, Xinzhe Juan, Yimin Wang, Zelin Zhao, Jiayi Geng, Jiacheng Guo, Peihang Li, Jingzhe Shi, Shilong Liu, Mengdi Wang</div>
<div class="meta-line">First: 2025-10-27T17:59:14+00:00 · Latest: 2025-10-27T17:59:14+00:00</div>
<div class="meta-line">Comments: 15 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23601v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23601v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool&#x27;s descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity&#x27;s Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) have been shown to perform better when scaffolded into agents with memory, tools, and feedback.</div>
</details>
</div>
<div class="card">
<div class="title">Token Is All You Price</div>
<div class="meta-line">Authors: Weijie Zhong</div>
<div class="meta-line">First: 2025-10-10T20:49:31+00:00 · Latest: 2025-10-27T17:57:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09859v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.09859v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We build a mechanism design framework where a platform designs GenAI models
to screen users who obtain instrumental value from the generated conversation
and privately differ in their preference for latency. We show that the
revenue-optimal mechanism is simple: deploy a single aligned (user-optimal)
model and use token cap as the only instrument to screen the user. The design
decouples model training from pricing, is readily implemented with token
metering, and mitigates misalignment pressures.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We build a mechanism design framework where a platform designs GenAI models to screen users who obtain instrumental value from the generated conversation and privately differ in their preference for latency.</div>
</details>
</div>
<div class="card">
<div class="title">AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented   Generation</div>
<div class="meta-line">Authors: Yixiong Fang, Tianran Sun, Yuling Shi, Xiaodong Gu</div>
<div class="meta-line">First: 2025-03-13T08:22:28+00:00 · Latest: 2025-10-27T16:55:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.10720v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.10720v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While RAG demonstrates remarkable capabilities in LLM applications, its
effectiveness is hindered by the ever-increasing length of retrieved contexts,
which introduces information redundancy and substantial computational overhead.
Existing context pruning methods, such as LLMLingua, lack contextual awareness
and offer limited flexibility in controlling compression rates, often resulting
in either insufficient pruning or excessive information loss. In this paper, we
propose AttentionRAG, an attention-guided context pruning method for RAG
systems. The core idea of AttentionRAG lies in its attention focus mechanism,
which reformulates RAG queries into a next-token prediction paradigm. This
mechanism isolates the query&#x27;s semantic focus to a single token, enabling
precise and efficient attention calculation between queries and retrieved
contexts. Extensive experiments on LongBench and Babilong benchmarks show that
AttentionRAG achieves up to 6.3$\times$ context compression while outperforming
LLMLingua methods by around 10\% in key metrics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While RAG demonstrates remarkable capabilities in LLM applications, its effectiveness is hindered by the ever-increasing length of retrieved contexts, which introduces information redundancy and substantial computational overhead.</div>
</details>
</div>
<div class="card">
<div class="title">Topology Sculptor, Shape Refiner: Discrete Diffusion Model for   High-Fidelity 3D Meshes Generation</div>
<div class="meta-line">Authors: Kaiyu Song, Hanjiang Lai, Yaqing Zhang, Chuangjian Cai, Yan Pan Kun Yue, Jian Yin</div>
<div class="meta-line">First: 2025-10-24T08:51:48+00:00 · Latest: 2025-10-27T16:38:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.21264v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.21264v2">PDF</a> · <a href="https://github.com/psky1111/Tencent-TSSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we introduce Topology Sculptor, Shape Refiner (TSSR), a novel
method for generating high-quality, artist-style 3D meshes based on Discrete
Diffusion Models (DDMs). Our primary motivation for TSSR is to achieve highly
accurate token prediction while enabling parallel generation, a significant
advantage over sequential autoregressive methods. By allowing TSSR to &quot;see&quot; all
mesh tokens concurrently, we unlock a new level of efficiency and control. We
leverage this parallel generation capability through three key innovations: 1)
Decoupled Training and Hybrid Inference, which distinctly separates the
DDM-based generation into a topology sculpting stage and a subsequent shape
refinement stage. This strategic decoupling enables TSSR to effectively capture
both intricate local topology and overarching global shape. 2) An Improved
Hourglass Architecture, featuring bidirectional attention enriched by
face-vertex-sequence level Rotational Positional Embeddings (RoPE), thereby
capturing richer contextual information across the mesh structure. 3) A novel
Connection Loss, which acts as a topological constraint to further enhance the
realism and fidelity of the generated meshes. Extensive experiments on complex
datasets demonstrate that TSSR generates high-quality 3D artist-style meshes,
capable of achieving up to 10,000 faces at a remarkable spatial resolution of
$1024^3$. The code will be released at:
https://github.com/psky1111/Tencent-TSSR.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we introduce Topology Sculptor, Shape Refiner (TSSR), a novel method for generating high-quality, artist-style 3D meshes based on Discrete Diffusion Models (DDMs).</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Reason Efficiently with Discounted Reinforcement Learning</div>
<div class="meta-line">Authors: Alex Ayoub, Kavosh Asadi, Dale Schuurmans, Csaba Szepesvári, Karim Bouyarmane</div>
<div class="meta-line">First: 2025-10-27T16:17:45+00:00 · Latest: 2025-10-27T16:17:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23486v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23486v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) often consume excessive tokens, inflating
computational cost and latency. We challenge the assumption that longer
responses improve accuracy. By penalizing reasoning tokens using a discounted
reinforcement learning setup (interpretable as a small token cost) and
analyzing Blackwell optimality in restricted policy classes, we encourage
concise yet accurate reasoning. Experiments confirm our theoretical results
that this approach shortens chains of thought while preserving accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large reasoning models (LRMs) often consume excessive tokens, inflating computational cost and latency.</div>
</details>
</div>
<div class="card">
<div class="title">MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal   Understanding</div>
<div class="meta-line">Authors: Xin Jin, Siyuan Li, Siyong Jian, Kai Yu, Huan Wang</div>
<div class="meta-line">First: 2025-10-27T16:12:40+00:00 · Latest: 2025-10-27T16:12:40+00:00</div>
<div class="meta-line">Comments: Code Link: https://github.com/JinXins/MergeMix</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23479v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23479v1">PDF</a> · <a href="https://github.com/JinXins/MergeMix">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language alignment in multi-modal large language models (MLLMs)
typically relies on supervised fine-tuning (SFT) or reinforcement learning
(RL). SFT is stable and efficient but requires large-scale human annotations
and cannot capture subtle preferences, while RL brings in a reward signal for
training, but suffers from overhead and instability. These limitations
highlight a trade-off between scalability, robustness, and alignment quality.
To address this, we propose MergeMix, a training-time augmentation paradigm
that bridges SFT and RL. It first applies an attention-aware image mixing via
token merge with more cluster representation and spatial context, and then
presents a preference-driven training paradigm for MLLMs by building preference
pairs with mixed images and raw images, and optimizing via SimPO loss. As a
mixup augmentation, MergeMix enhances attention consistency and efficiency,
surpassing other heuristic-based methods in classification. Extensive
experiments demonstrate that MergeMix achieves competitive accuracy with
improved efficiency, providing a scalable approach to preference alignment in
classification and MLLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL).</div>
</details>
</div>
<div class="card">
<div class="title">BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents</div>
<div class="meta-line">Authors: Litu Ou, Kuan Li, Huifeng Yin, Liwen Zhang, Zhongwang Zhang, Xixi Wu, Rui Ye, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</div>
<div class="meta-line">First: 2025-10-27T15:58:51+00:00 · Latest: 2025-10-27T15:58:51+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23458v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Confidence in LLMs is a useful indicator of model uncertainty and answer reliability.</div>
</details>
</div>
<div class="card">
<div class="title">How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of   Target Language Text?</div>
<div class="meta-line">Authors: Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras</div>
<div class="meta-line">First: 2024-06-17T12:42:34+00:00 · Latest: 2025-10-27T15:19:39+00:00</div>
<div class="meta-line">Comments: Accepted to Computational Linguistics</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2406.11477v3">Abs</a> · <a href="http://arxiv.org/pdf/2406.11477v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have shown remarkable capabilities in many
languages beyond English. Yet, LLMs require more inference steps when
generating non-English text due to their reliance on English-centric tokenizers
and vocabulary, resulting in higher usage costs to non-English speakers.
Vocabulary expansion with target language tokens is a widely used cross-lingual
vocabulary adaptation approach to remedy this issue. Despite its effectiveness
in inference speedup, previous work on vocabulary expansion has focused on
high-resource settings assuming access to a substantial amount of target
language data to effectively initialize the embeddings of the new tokens and
adapt the LLM to the target language. However, vocabulary expansion in
low-resource settings has yet to be explored. In this article, we investigate
vocabulary expansion in low-resource settings by considering embedding
initialization methods and continual pre-training strategies. Through extensive
experiments across typologically diverse languages, tasks and models, we
establish a set of strategies to perform vocabulary expansion for faster
inference, while striving to maintain competitive downstream performance to
baselines. This is achieved with only 30K sentences ($\sim$0.01GB text data)
from the target language.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) have shown remarkable capabilities in many languages beyond English.</div>
</details>
</div>
<div class="card">
<div class="title">Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising   from A Foundation Model Lens</div>
<div class="meta-line">Authors: Jiahao Ji, Tianyu Wang, Yeshu Li, Yushen Huo, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng</div>
<div class="meta-line">Venue: KDD 2025</div>
<div class="meta-line">First: 2025-10-27T15:15:01+00:00 · Latest: 2025-10-27T15:15:01+00:00</div>
<div class="meta-line">Comments: 12 pages, KDD 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23410v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23410v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Auto-bidding is crucial in facilitating online advertising by automatically
providing bids for advertisers. While previous work has made great efforts to
model bidding environments for better ad performance, it has limitations in
generalizability across environments since these models are typically tailored
for specific bidding scenarios. To this end, we approach the
scenario-independent principles through a unified function that estimates the
achieved effect under specific bids, such as budget consumption, gross
merchandise volume (GMV), page views, etc. Then, we propose a bidding
foundation model Bid2X to learn this fundamental function from data in various
scenarios. Our Bid2X is built over uniform series embeddings that encode
heterogeneous data through tailored embedding methods. To capture complex
inter-variable and dynamic temporal dependencies in bidding data, we propose
two attention mechanisms separately treating embeddings of different variables
and embeddings at different times as attention tokens for representation
learning. On top of the learned variable and temporal representations, a
variable-aware fusion module is used to perform adaptive bidding outcome
prediction. To model the unique bidding data distribution, we devise a
zero-inflated projection module to incorporate the estimated non-zero
probability into its value prediction, which makes up a joint optimization
objective containing classification and regression. The objective is proven to
converge to the zero-inflated distribution. Our model has been deployed on the
ad platform in Taobao, one of the world&#x27;s largest e-commerce platforms. Offline
evaluation on eight datasets exhibits Bid2X&#x27;s superiority compared to various
baselines and its generality across different scenarios. Bid2X increased GMV by
4.65% and ROI by 2.44% in online A/B tests, paving the way for bidding
foundation model in computational advertising.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Auto-bidding is crucial in facilitating online advertising by automatically providing bids for advertisers.</div>
</details>
</div>
<div class="card">
<div class="title">Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient   Multimodal Inference on Battery-Powered Small Devices</div>
<div class="meta-line">Authors: Yilong Li, Shuai Zhang, Yijing Zeng, Hao Zhang, Xinmiao Xiong, Jingyu Liu, Pan Hu, Suman Banerjee</div>
<div class="meta-line">First: 2025-09-25T22:28:44+00:00 · Latest: 2025-10-27T14:17:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.05109v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.05109v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Multimodal Models (LMMs) are inherently modular, consisting of vision
and audio encoders, projectors, and large language models. Yet, they are almost
always executed monolithically, which underutilizes the heterogeneous
accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end
latency. In this paper, we present NANOMIND, a hardware--software co-design
inference framework for Large Multimodal Models (LMMs) that breaks large models
into modular ``bricks&#x27;&#x27; (vision, language, audio, etc.) and maps each to its
ideal accelerator. The key insight is that large models can be broken into
modular components and scheduled to run on the most appropriate compute units.
It performs module-level dynamic offloading across accelerators on
unified-memory SoCs. By combining customized hardware design, system-level
scheduling, and optimized low-bit computation kernels, we demonstrate our
framework with a compact, battery-powered device capable of running LMMs
entirely on device. This prototype functions as a self-contained intelligent
assistant that requires no network connectivity, while achieving higher
throughput and superior power efficiency under strict resource constraints. The
design further bypasses CPU bottlenecks and reduces redundant memory usage
through token-aware buffer management and module-level coordination. Our system
outperforms existing implementations in resource efficiency, cutting energy
consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a
battery-powered device to run LLaVA-OneVision with a camera for nearly half a
day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Multimodal Models (LMMs) are inherently modular, consisting of vision and audio encoders, projectors, and large language models.</div>
</details>
</div>
<div class="card">
<div class="title">Thought Anchors: Which LLM Reasoning Steps Matter?</div>
<div class="meta-line">Authors: Paul C. Bogdan, Uzay Macar, Neel Nanda, Arthur Conmy</div>
<div class="meta-line">First: 2025-06-23T21:28:45+00:00 · Latest: 2025-10-27T12:36:23+00:00</div>
<div class="meta-line">Comments: Paul C. Bogdan and Uzay Macar contributed equally to this work, and
  their listed order was determined by coinflip. Neel Nanda and Arthur Conmy
  contributed equally to this work as senior authors, and their listed order
  was determined by coinflip</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.19143v4">Abs</a> · <a href="http://arxiv.org/pdf/2506.19143v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current frontier large-language models rely on reasoning to achieve
state-of-the-art performance. Many existing interpretability are limited in
this area, as standard methods have been designed to study single forward
passes of a model rather than the multi-token computational steps that unfold
during reasoning. We argue that analyzing reasoning traces at the sentence
level is a promising approach to understanding reasoning processes. We
introduce a black-box method that measures each sentence&#x27;s counterfactual
importance by repeatedly sampling replacement sentences from the model,
filtering for semantically different ones, and continuing the chain of thought
from that point onwards to quantify the sentence&#x27;s impact on the distribution
of final answers. We discover that certain sentences can have an outsized
impact on the trajectory of the reasoning trace and final answer. We term these
sentences \textit{thought anchors}. These are generally planning or uncertainty
management sentences, and specialized attention heads consistently attend from
subsequent sentences to thought anchors. We further show that examining
sentence-sentence causal links within a reasoning trace gives insight into a
model&#x27;s behavior. Such information can be used to predict a problem&#x27;s
difficulty and the extent different question domains involve sequential or
diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques
together provide a practical toolkit for analyzing reasoning models by
conducting a detailed case study of how the model solves a difficult math
problem, finding that our techniques yield a consistent picture of the
reasoning trace&#x27;s structure. We provide an open-source tool
(thought-anchors.com) for visualizing the outputs of our methods on further
problems. The convergence across our methods shows the potential of
sentence-level analysis for a deeper understanding of reasoning models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current frontier large-language models rely on reasoning to achieve state-of-the-art performance.</div>
</details>
</div>
<div class="card">
<div class="title">Autoregressive Styled Text Image Generation, but Make it Reliable</div>
<div class="meta-line">Authors: Carmine Zaccagnino, Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, Alessio Tonioni, Rita Cucchiara</div>
<div class="meta-line">First: 2025-10-27T11:54:23+00:00 · Latest: 2025-10-27T11:54:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23240v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23240v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating faithful and readable styled text images (especially for Styled
Handwritten Text generation - HTG) is an open problem with several possible
applications across graphic design, document understanding, and image editing.
A lot of research effort in this task is dedicated to developing strategies
that reproduce the stylistic characteristics of a given writer, with promising
results in terms of style fidelity and generalization achieved by the recently
proposed Autoregressive Transformer paradigm for HTG. However, this method
requires additional inputs, lacks a proper stop mechanism, and might end up in
repetition loops, generating visual artifacts. In this work, we rethink the
autoregressive formulation by framing HTG as a multimodal prompt-conditioned
generation task, and tackle the content controllability issues by introducing
special textual input tokens for better alignment with the visual ones.
Moreover, we devise a Classifier-Free-Guidance-based strategy for our
autoregressive model. Through extensive experimental validation, we demonstrate
that our approach, dubbed Eruku, compared to previous solutions requires fewer
inputs, generalizes better to unseen styles, and follows more faithfully the
textual prompt, improving content adherence.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Generating faithful and readable styled text images (especially for Styled Handwritten Text generation - HTG) is an open problem with several possible applications across graphic design, document understanding, and image editing.</div>
</details>
</div>
<div class="card">
<div class="title">PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation   Performance at Unseen Pre-Training Budgets</div>
<div class="meta-line">Authors: Etienne Goffinet, Shane Bergsma, Avraham Sheinin, Natalia Vassilieva, Shaheer Muhammad, Preslav Nakov, Gurpreet Gosal</div>
<div class="meta-line">First: 2025-10-27T10:36:15+00:00 · Latest: 2025-10-27T10:36:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23198v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23198v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual pre-training (CPT) for domain adaptation must balance target-domain
gains with stability on the base domain. Existing CPT scaling laws typically
assume a fixed pre-training budget, which limits their ability to forecast
adaptation outcomes for models trained at different tokens-per-parameter
(PTPP). We present \emph{PTPP-aware} adaptation scaling laws that make the
pre-training budget an explicit variable, enabling accurate \emph{prediction}
of adaptation loss at unseen \ptpp. On a multilingual setup (English/Arabic
$\rightarrow$ French), PTPP-aware formulations trained on early stages
(\ptpp{}=\{15,31\}) predict target loss at \ptpp{}=279 and outperform a
PTPP-agnostic \dcpt{} transfer baseline on metrics (Huber-on-log,
MAE$_\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in
the appendix. Beyond forecasting, we show a practical use case: planning replay
ratios and adaptation token budgets that satisfy target and forgetting
constraints under compute limits.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Continual pre-training (CPT) for domain adaptation must balance target-domain gains with stability on the base domain.</div>
</details>
</div>
<div class="card">
<div class="title">Attention! Your Vision Language Model Could Be Maliciously Manipulated</div>
<div class="meta-line">Authors: Xiaosen Wang, Shaokang Wang, Zhijin Ge, Yuyang Luo, Shudong Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-26T12:38:58+00:00 · Latest: 2025-10-27T09:54:32+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.19911v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.19911v2">PDF</a> · <a href="https://github.com/Trustworthy-AI-Group/VMA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) have achieved remarkable success in
understanding complex real-world scenarios and supporting data-driven
decision-making processes. However, VLMs exhibit significant vulnerability
against adversarial examples, either text or image, which can lead to various
adversarial outcomes, e.g., jailbreaking, hijacking, and hallucination, etc. In
this work, we empirically and theoretically demonstrate that VLMs are
particularly susceptible to image-based adversarial examples, where
imperceptible perturbations can precisely manipulate each output token. To this
end, we propose a novel attack called Vision-language model Manipulation Attack
(VMA), which integrates first-order and second-order momentum optimization
techniques with a differentiable transformation mechanism to effectively
optimize the adversarial perturbation. Notably, VMA can be a double-edged
sword: it can be leveraged to implement various attacks, such as jailbreaking,
hijacking, privacy breaches, Denial-of-Service, and the generation of sponge
examples, etc, while simultaneously enabling the injection of watermarks for
copyright protection. Extensive empirical evaluations substantiate the efficacy
and generalizability of VMA across diverse scenarios and datasets. Code is
available at https://github.com/Trustworthy-AI-Group/VMA.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Vision-Language Models (VLMs) have achieved remarkable success in understanding complex real-world scenarios and supporting data-driven decision-making processes.</div>
</details>
</div>
<div class="card">
<div class="title">Lost in Tokenization: Context as the Key to Unlocking Biomolecular   Understanding in Scientific LLMs</div>
<div class="meta-line">Authors: Kai Zhuang, Jiawei Zhang, Yumou Liu, Hanqun Cao, Chunbin Gu, Mengdi Liu, Zhangyang Gao, Zitong Jerry Wang, Xuanhe Zhou, Pheng-Ann Heng, Lijun Wu, Conghui He, Cheng Tan</div>
<div class="meta-line">First: 2025-10-27T09:03:21+00:00 · Latest: 2025-10-27T09:03:21+00:00</div>
<div class="meta-line">Comments: 36 pages, under review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23127v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23127v1">PDF</a> · <a href="http://github.com/opendatalab-raise-dev/CoKE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific Large Language Models (Sci-LLMs) have emerged as a promising
frontier for accelerating biological discovery. However, these models face a
fundamental challenge when processing raw biomolecular sequences: the
tokenization dilemma. Whether treating sequences as a specialized language,
risking the loss of functional motif information, or as a separate modality,
introducing formidable alignment challenges, current strategies fundamentally
limit their reasoning capacity. We challenge this sequence-centric paradigm by
positing that a more effective strategy is to provide Sci-LLMs with high-level
structured context derived from established bioinformatics tools, thereby
bypassing the need to interpret low-level noisy sequence data directly. Through
a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we
tested three input modes: sequence-only, context-only, and a combination of
both. Our findings are striking: the context-only approach consistently and
substantially outperforms all other modes. Even more revealing, the inclusion
of the raw sequence alongside its high-level context consistently degrades
performance, indicating that raw sequences act as informational noise, even for
models with specialized tokenization schemes. These results suggest that the
primary strength of existing Sci-LLMs lies not in their nascent ability to
interpret biomolecular syntax from scratch, but in their profound capacity for
reasoning over structured, human-readable knowledge. Therefore, we argue for
reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines
over expert knowledge. This work lays the foundation for a new class of hybrid
scientific AI agents, repositioning the developmental focus from direct
sequence interpretation towards high-level knowledge synthesis. The code is
available at github.com/opendatalab-raise-dev/CoKE.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Scientific Large Language Models (Sci-LLMs) have emerged as a promising frontier for accelerating biological discovery.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Higher Rank: Token-wise Input-Output Projections for Efficient   Low-Rank Adaptation</div>
<div class="meta-line">Authors: Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Ziqiang Cui, Dugang Liu, Yuhua Li, Xiuqiang He, Ruixuan Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-27T08:57:24+00:00 · Latest: 2025-10-27T08:57:24+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23123v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23123v1">PDF</a> · <a href="https://github.com/Leopold1423/toplora-neurips25">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA&#x27;s ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Analog Foundation Models</div>
<div class="meta-line">Authors: Julian Büchel, Iason Chalas, Giovanni Acampa, An Chen, Omobayode Fagbohungbe, Sidney Tsai, Kaoutar El Maghraoui, Manuel Le Gallo, Abbas Rahimi, Abu Sebastian</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2025-05-14T14:52:22+00:00 · Latest: 2025-10-27T08:55:30+00:00</div>
<div class="meta-line">Comments: Neural Information Processing Systems (NeurIPS) 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.09663v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.09663v3">PDF</a> · <a href="https://github.com/IBM/analog-foundation-models">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Analog in-memory computing (AIMC) is a promising compute paradigm to improve speed and power efficiency of neural network inference beyond the limits of conventional von Neumann-based architectures.</div>
</details>
</div>
<div class="card">
<div class="title">Task-Agnostic Fusion of Time Series and Imagery for Earth Observation</div>
<div class="meta-line">Authors: Gianfranco Basile, Johannes Jakubik, Benedikt Blumenstiel, Thomas Brunschwiler, Juan Bernabe Moreno</div>
<div class="meta-line">First: 2025-10-27T08:38:52+00:00 · Latest: 2025-10-27T08:38:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23118v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23118v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a task-agnostic framework for multimodal fusion of time series and
single timestamp images, enabling cross-modal generation and robust downstream
performance. Our approach explores deterministic and learned strategies for
time series quantization and then leverages a masked correlation learning
objective, aligning discrete image and time series tokens in a unified
representation space. Instantiated in the Earth observation domain, the
pretrained model generates consistent global temperature profiles from
satellite imagery and is validated through counterfactual experiments. Across
downstream tasks, our task-agnostic pretraining outperforms task-specific
fusion by 6\% in R$^2$ and 2\% in RMSE on average, and exceeds baseline methods
by 50\% in R$^2$ and 12\% in RMSE. Finally, we analyze gradient sensitivity
across modalities, providing insights into model robustness. Code, data, and
weights will be released under a permissive license.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose a task-agnostic framework for multimodal fusion of time series and single timestamp images, enabling cross-modal generation and robust downstream performance.</div>
</details>
</div>
<div class="card">
<div class="title">GroupSHAP-Guided Integration of Financial News Keywords and Technical   Indicators for Stock Price Prediction</div>
<div class="meta-line">Authors: Minjoo Kim, Jinwoong Kim, Sangjin Park</div>
<div class="meta-line">Venue: ICAIF 2025 Workshop</div>
<div class="meta-line">First: 2025-10-27T08:33:18+00:00 · Latest: 2025-10-27T08:33:18+00:00</div>
<div class="meta-line">Comments: 6 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23112v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23112v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in finance-specific language models such as FinBERT have
enabled the quantification of public sentiment into index-based measures, yet
compressing diverse linguistic signals into single metrics overlooks contextual
nuances and limits interpretability. To address this limitation, explainable AI
techniques, particularly SHAP (SHapley Additive Explanations), have been
employed to identify influential features. However, SHAP&#x27;s computational cost
grows exponentially with input features, making it impractical for large-scale
text-based financial data. This study introduces a GRU-based forecasting
framework enhanced with GroupSHAP, which quantifies contributions of
semantically related keyword groups rather than individual tokens,
substantially reducing computational burden while preserving interpretability.
We employed FinBERT to embed news articles from 2015 to 2024, clustered them
into coherent semantic groups, and applied GroupSHAP to measure each group&#x27;s
contribution to stock price movements. The resulting group-level SHAP variables
across multiple topics were used as input features for the prediction model.
Empirical results from one-day-ahead forecasting of the S&amp;P 500 index
throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE
and a 40.5% reduction in RMSE compared with benchmark models without the
GroupSHAP mechanism. This research presents the first application of GroupSHAP
in news-driven financial forecasting, showing that grouped sentiment
representations simultaneously enhance interpretability and predictive
performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability.</div>
</details>
</div>
<div class="card">
<div class="title">GraphInstruct: Empowering Large Language Models with Graph Understanding   and Reasoning Capability</div>
<div class="meta-line">Authors: Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, Xing Xie, Hai Jin</div>
<div class="meta-line">First: 2024-03-07T13:36:08+00:00 · Latest: 2025-10-27T08:07:17+00:00</div>
<div class="meta-line">Comments: Accepted by Frontiers of Computer Science</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2403.04483v3">Abs</a> · <a href="http://arxiv.org/pdf/2403.04483v3">PDF</a> · <a href="https://github.com/CGCL-codes/GraphInstruct">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improving the general capabilities of large language models (LLMs) is an
active research topic. As a common data structure in many real-world domains,
understanding graph data is a crucial part of advancing general intelligence.
To this end, we propose a dynamic benchmark named GraphInstruct in this paper,
which comprehensively includes 21 classical graph reasoning tasks, providing
diverse graph generation pipelines and detailed intermediate reasoning steps
for each sample. Based on GraphInstruct, we develop GraphSolver via efficient
instruction-tuning, which demonstrates prominent graph understanding capability
compared to other open-sourced LLMs. To further endow LLMs with multi-step
graph reasoning capability, we propose a label-mask training strategy and build
GraphSolver+, which leverages masked supervision on intermediate reasoning
tokens to emphasize crucial node-identification signals. As one of the
pioneering efforts to enhance the graph understanding and reasoning abilities
of LLMs, extensive experiments have demonstrated the superiority of GraphSolver
and GraphSolver+ over other LLMs. We sincerely hope GraphInstruct will
facilitate further research on applying LLMs to graph-structured data. Our code
and data are released publicly at: https://github.com/CGCL-codes/GraphInstruct.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Improving the general capabilities of large language models (LLMs) is an active research topic.</div>
</details>
</div>
<div class="card">
<div class="title">FaithLM: Towards Faithful Explanations for Large Language Models</div>
<div class="meta-line">Authors: Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Ruixiang Tang, Shaochen Zhong, Fan Yang, Mengnan Du, Xuanting Cai, Vladimir Braverman, Xia Hu</div>
<div class="meta-line">First: 2024-02-07T09:09:14+00:00 · Latest: 2025-10-27T06:19:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2402.04678v4">Abs</a> · <a href="http://arxiv.org/pdf/2402.04678v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) increasingly produce natural language
explanations, yet these explanations often lack faithfulness, and they do not
reliably reflect the evidence the model uses to decide. We introduce FaithLM, a
model-agnostic framework that evaluates and improves the faithfulness of LLM
explanations without token masking or task-specific heuristics. FaithLM
formalizes explanation faithfulness as an intervention property: a faithful
explanation should yield a prediction shift when its content is contradicted.
Theoretical analysis shows that the resulting contrary-hint score is a sound
and discriminative estimator of faithfulness. Building on this principle,
FaithLM iteratively refines both the elicitation prompt and the explanation to
maximize the measured score. Experiments on three multi-domain datasets and
multiple LLM backbones demonstrate that FaithLM consistently increases
faithfulness and produces explanations more aligned with human rationales than
strong self-explanation baselines. These findings highlight that
intervention-based evaluation, coupled with iterative optimization, provides a
principled route toward faithful and reliable LLM explanations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide.</div>
</details>
</div>
<div class="card">
<div class="title">HieraMamba: Video Temporal Grounding via Hierarchical Anchor-Mamba   Pooling</div>
<div class="meta-line">Authors: Joungbin An, Kristen Grauman</div>
<div class="meta-line">First: 2025-10-27T06:13:07+00:00 · Latest: 2025-10-27T06:13:07+00:00</div>
<div class="meta-line">Comments: Project Page: https://vision.cs.utexas.edu/projects/hieramamba/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23043v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vision.cs.utexas.edu/projects/hieramamba/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video temporal grounding, the task of localizing the start and end times of a
natural language query in untrimmed video, requires capturing both global
context and fine-grained temporal detail. This challenge is particularly
pronounced in long videos, where existing methods often compromise temporal
fidelity by over-downsampling or relying on fixed windows. We present
HieraMamba, a hierarchical architecture that preserves temporal structure and
semantic richness across scales. At its core are Anchor-MambaPooling (AMP)
blocks, which utilize Mamba&#x27;s selective scanning to produce compact anchor
tokens that summarize video content at multiple granularities. Two
complementary objectives, anchor-conditioned and segment-pooled contrastive
losses, encourage anchors to retain local detail while remaining globally
discriminative. HieraMamba sets a new state-of-the-art on Ego4D-NLQ, MAD, and
TACoS, demonstrating precise, temporally faithful localization in long,
untrimmed videos.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video temporal grounding, the task of localizing the start and end times of a natural language query in untrimmed video, requires capturing both global context and fine-grained temporal detail.</div>
</details>
</div>
<div class="card">
<div class="title">A high-capacity linguistic steganography based on entropy-driven   rank-token mapping</div>
<div class="meta-line">Authors: Jun Jiang, Weiming Zhang, Nenghai Yu, Kejiang Chen</div>
<div class="meta-line">First: 2025-10-27T06:02:47+00:00 · Latest: 2025-10-27T06:02:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23035v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linguistic steganography enables covert communication through embedding
secret messages into innocuous texts; however, current methods face critical
limitations in payload capacity and security. Traditional modification-based
methods introduce detectable anomalies, while retrieval-based strategies suffer
from low embedding capacity. Modern generative steganography leverages language
models to generate natural stego text but struggles with limited entropy in
token predictions, further constraining capacity. To address these issues, we
propose an entropy-driven framework called RTMStega that integrates rank-based
adaptive coding and context-aware decompression with normalized entropy. By
mapping secret messages to token probability ranks and dynamically adjusting
sampling via context-aware entropy-based adjustments, RTMStega achieves a
balance between payload capacity and imperceptibility. Experiments across
diverse datasets and models demonstrate that RTMStega triples the payload
capacity of mainstream generative steganography, reduces processing time by
over 50%, and maintains high text quality, offering a trustworthy solution for
secure and efficient covert communication.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Linguistic steganography enables covert communication through embedding secret messages into innocuous texts; however, current methods face critical limitations in payload capacity and security.</div>
</details>
</div>
<div class="card">
<div class="title">Nested AutoRegressive Models</div>
<div class="meta-line">Authors: Hongyu Wu, Xuhui Fan, Zhangkai Wu, Longbing Cao</div>
<div class="meta-line">First: 2025-10-27T05:49:02+00:00 · Latest: 2025-10-27T05:49:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23028v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.23028v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AutoRegressive (AR) models have demonstrated competitive performance in image
generation, achieving results comparable to those of diffusion models. However,
their token-by-token image generation mechanism remains computationally
intensive and existing solutions such as VAR often lead to limited sample
diversity. In this work, we propose a Nested AutoRegressive~(NestAR) model,
which proposes nested AutoRegressive architectures in generating images. NestAR
designs multi-scale modules in a hierarchical order. These different scaled
modules are constructed in an AR architecture, where one larger-scale module is
conditioned on outputs from its previous smaller-scale module. Within each
module, NestAR uses another AR structure to generate ``patches&#x27;&#x27; of tokens. The
proposed nested AR architecture reduces the overall complexity from
$\mathcal{O}(n)$ to $\mathcal{O}(\log n)$ in generating $n$ image tokens, as
well as increases image diversities. NestAR further incorporates flow matching
loss to use continuous tokens, and develops objectives to coordinate these
multi-scale modules in model training. NestAR achieves competitive image
generation performance while significantly lowering computational cost.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AutoRegressive (AR) models have demonstrated competitive performance in image generation, achieving results comparable to those of diffusion models.</div>
</details>
</div>
<div class="card">
<div class="title">ProfileXAI: User-Adaptive Explainable AI</div>
<div class="meta-line">Authors: Gilber A. Corrales, Carlos Andrés Ferro Sánchez, Reinel Tabares-Soto, Jesús Alfonso López Sotelo, Gonzalo A. Ruz, Johan Sebastian Piña Durán</div>
<div class="meta-line">First: 2025-10-27T04:34:02+00:00 · Latest: 2025-10-27T04:34:02+00:00</div>
<div class="meta-line">Comments: pages, 1 figure, 3 tables. Preprint. Evaluated on UCI Heart Disease
  (1989) and UCI Differentiated Thyroid Cancer Recurrence (2023). Uses IEEEtran</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22998v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.22998v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">ProfileXAI is a model- and domain-agnostic framework that couples post-hoc
explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce
explanations for different types of users. The system indexes a multimodal
knowledge base, selects an explainer per instance via quantitative criteria,
and generates grounded narratives with chat-enabled prompting. On Heart Disease
and Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token
use, and perceived quality. No explainer dominates: LIME achieves the best
fidelity-robustness trade-off (Infidelity $\le 0.30$, $L&lt;0.7$ on Heart
Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest
satisfaction ($\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\sigma
\le 13\%$) and maintains positive ratings across profiles ($\bar{x}\ge 3.7$,
with domain experts at $3.77$), enabling efficient and trustworthy
explanations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ProfileXAI is a model- and domain-agnostic framework that couples post-hoc explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce explanations for different types of users.</div>
</details>
</div>
<div class="card">
<div class="title">S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient   Data and Sparse Token Distillation</div>
<div class="meta-line">Authors: Weilun Feng, Haotong Qin, Chuanguang Yang, Xiangqi Li, Han Yang, Yuqi Li, Zhulin An, Libo Huang, Michele Magno, Yongjun Xu</div>
<div class="meta-line">First: 2025-08-06T02:12:29+00:00 · Latest: 2025-10-27T04:32:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.04016v3">Abs</a> · <a href="http://arxiv.org/pdf/2508.04016v3">PDF</a> · <a href="https://github.com/wlfeng0509/s2q-vdit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion transformers have emerged as the mainstream paradigm for video
generation models. However, the use of up to billions of parameters incurs
significant computational costs. Quantization offers a promising solution by
reducing memory usage and accelerating inference. Nonetheless, we observe that
the joint modeling of spatial and temporal information in video diffusion
models (V-DMs) leads to extremely long token sequences, which introduces high
calibration variance and learning challenges. To address these issues, we
propose S$^2$Q-VDiT, a post-training quantization framework for V-DMs that
leverages Salient data and Sparse token distillation. During the calibration
phase, we identify that quantization performance is highly sensitive to the
choice of calibration data. To mitigate this, we introduce
\textit{Hessian-aware Salient Data Selection}, which constructs high-quality
calibration datasets by considering both diffusion and quantization
characteristics unique to V-DMs. To tackle the learning challenges, we further
analyze the sparse attention patterns inherent in V-DMs. Based on this
observation, we propose \textit{Attention-guided Sparse Token Distillation},
which exploits token-wise attention distributions to emphasize tokens that are
more influential to the model&#x27;s output. Under W4A6 quantization, S$^2$Q-VDiT
achieves lossless performance while delivering $3.9\times$ model compression
and $1.3\times$ inference acceleration. Code will be available at
https://github.com/wlfeng0509/s2q-vdit.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion transformers have emerged as the mainstream paradigm for video generation models.</div>
</details>
</div>
<div class="card">
<div class="title">Measuring Teaching with LLMs</div>
<div class="meta-line">Authors: Michael Hardy</div>
<div class="meta-line">First: 2025-10-27T03:42:04+00:00 · Latest: 2025-10-27T03:42:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22968v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.22968v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Objective and scalable measurement of teaching quality is a persistent challenge in education.</div>
</details>
</div>
<div class="card">
<div class="title">FAME: Fairness-aware Attention-modulated Video Editing</div>
<div class="meta-line">Authors: Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Zhidong Li, Longbing Cao</div>
<div class="meta-line">First: 2025-10-27T03:34:15+00:00 · Latest: 2025-10-27T03:34:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22960v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.22960v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-free video editing (VE) models tend to fall back on gender
stereotypes when rendering profession-related prompts. We propose \textbf{FAME}
for \textit{Fairness-aware Attention-modulated Video Editing} that mitigates
profession-related gender biases while preserving prompt alignment and temporal
consistency for coherent VE. We derive fairness embeddings from existing
minority representations by softly injecting debiasing tokens into the text
encoder. Simultaneously, FAME integrates fairness modulation into both temporal
self attention and prompt-to-region cross attention to mitigate the motion
corruption and temporal inconsistency caused by directly introducing fairness
cues. For temporal self attention, FAME introduces a region constrained
attention mask combined with time decay weighting, which enhances intra-region
coherence while suppressing irrelevant inter-region interactions. For cross
attention, it reweights tokens to region matching scores by incorporating
fairness sensitive similarity masks derived from debiasing prompt embeddings.
Together, these modulations keep fairness-sensitive semantics tied to the right
visual regions and prevent temporal drift across frames. Extensive experiments
on new VE fairness-oriented benchmark \textit{FairVE} demonstrate that FAME
achieves stronger fairness alignment and semantic fidelity, surpassing existing
VE baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Training-free video editing (VE) models tend to fall back on gender stereotypes when rendering profession-related prompts.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
