<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-23 03:14</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251023_0314</div>
    <div class="row"><div class="card">
<div class="title">How Do LLMs Use Their Depth?</div>
<div class="meta-line">Authors: Akshat Gupta, Jay Yeung, Gopala Anumanchipalli, Anna Ivanova</div>
<div class="meta-line">First: 2025-10-21T17:59:05+00:00 · Latest: 2025-10-21T17:59:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18871v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18871v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Growing evidence suggests that large language models do not use their depth
uniformly, yet we still lack a fine-grained understanding of their layer-wise
prediction dynamics. In this paper, we trace the intermediate representations
of several open-weight models during inference and reveal a structured and
nuanced use of depth. Specifically, we propose a &quot;Guess-then-Refine&quot; framework
that explains how LLMs internally structure their computations to make
predictions. We first show that the top-ranked predictions in early LLM layers
are composed primarily of high-frequency tokens, which act as statistical
guesses proposed by the model early on due to the lack of appropriate
contextual information. As contextual information develops deeper into the
model, these initial guesses get refined into contextually appropriate tokens.
Even high-frequency token predictions from early layers get refined &gt;70% of the
time, indicating that correct token prediction is not &quot;one-and-done&quot;. We then
go beyond frequency-based prediction to examine the dynamic usage of layer
depth across three case studies. (i) Part-of-speech analysis shows that
function words are, on average, the earliest to be predicted correctly. (ii)
Fact recall task analysis shows that, in a multi-token answer, the first token
requires more computational depth than the rest. (iii) Multiple-choice task
analysis shows that the model identifies the format of the response within the
first half of the layers, but finalizes its response only toward the end.
Together, our results provide a detailed view of depth usage in LLMs, shedding
light on the layer-by-layer computations that underlie successful predictions
and providing insights for future works to improve computational efficiency in
transformer-based models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Growing evidence suggests that large language models do not use their depth uniformly, yet we still lack a fine-grained understanding of their layer-wise prediction dynamics.</div>
</details>
</div>
<div class="card">
<div class="title">LightMem: Lightweight and Efficient Memory-Augmented Generation</div>
<div class="meta-line">Authors: Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang</div>
<div class="meta-line">First: 2025-10-21T17:58:17+00:00 · Latest: 2025-10-21T17:58:17+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18866v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18866v1">PDF</a> · <a href="https://github.com/zjunlp/LightMem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their remarkable capabilities, Large Language Models (LLMs) struggle
to effectively leverage historical interaction information in dynamic and
complex environments. Memory systems enable LLMs to move beyond stateless
interactions by introducing persistent information storage, retrieval, and
utilization mechanisms. However, existing memory systems often introduce
substantial time and computational overhead. To this end, we introduce a new
memory system called LightMem, which strikes a balance between the performance
and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of
human memory, LightMem organizes memory into three complementary stages. First,
cognition-inspired sensory memory rapidly filters irrelevant information
through lightweight compression and groups information according to their
topics. Next, topic-aware short-term memory consolidates these topic-based
groups, organizing and summarizing content for more structured access. Finally,
long-term memory with sleep-time update employs an offline procedure that
decouples consolidation from online inference. Experiments on LongMemEval with
GPT and Qwen backbones show that LightMem outperforms strong baselines in
accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API
calls by up to 159x, and runtime by over 12x. The code is available at
https://github.com/zjunlp/LightMem.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments.</div>
</details>
</div>
<div class="card">
<div class="title">Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale   Thinking Model</div>
<div class="meta-line">Authors: Ling Team, Anqi Shen, Baihui Li, Bin Hu, Bin Jing, Cai Chen, Chao Huang, Chao Zhang, Chaokun Yang, Cheng Lin, Chengyao Wen, Congqi Li, Deng Zhao, Dingbo Yuan, Donghai You, Fagui Mao, Fanzhuang Meng, Feng Xu, Guojie Li, Guowei Wang, Hao Dai, Haonan Zheng, Hong Liu, Jia Guo, Jiaming Liu, Jian Liu, Jianhao Fu, Jiannan Shi, Jianwen Wang, Jianxin Lai, Jin Yang, Jun Mei, Jun Zhou, Junbo Zhao, Junping Zhao, Kuan Xu, Le Su, Lei Chen, Li Tang, Liang Jiang, Liangcheng Fu, Lianhao Xu, Linfeng Shi, Lisha Liao, Longfei Zheng, Meng Li, Mingchun Chen, Qi Zuo, Qiang Cheng, Qianggang Cao, Qitao Shi, Quanrui Guo, Senlin Zhu, Shaofei Wang, Shaomian Zheng, Shuaicheng Li, Shuwei Gu, Siba Chen, Tao Wu, Tao Zhang, Tianyu Zhang, Tianyu Zhou, Tiwei Bie, Tongkai Yang, Wang Hong, Wang Ren, Weihua Chen, Wenbo Yu, Wengang Zheng, Xiangchun Wang, Xiaodong Yan, Xiaopei Wan, Xin Zhao, Xinyu Kong, Xinyu Tang, Xudong Han, Xudong Wang, Xuemin Yang, Xueyu Hu, Yalin Zhang, Yan Sun, Yicheng Shan, Yilong Wang, Yingying Xu, Yongkang Liu, Yongzhen Guo, Yuanyuan Wang, Yuchen Yan, Yuefan Wang, Yuhong Guo, Zehuan Li, Zhankai Xu, Zhe Li, Zhenduo Zhang, Zhengke Gui, Zhenxuan Pan, Zhenyu Huang, Zhenzhong Lan, Zhiqiang Ding, Zhiqiang Zhang, Zhixun Li, Zhizhen Liu, Zihao Wang, Zujie Wen</div>
<div class="meta-line">First: 2025-10-21T17:46:14+00:00 · Latest: 2025-10-21T17:46:14+00:00</div>
<div class="meta-line">Comments: Technical Report</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18855v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18855v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Ring-1T, the first open-source, state-of-the-art thinking model
with a trillion-scale parameter. It features 1 trillion total parameters and
activates approximately 50 billion per token. Training such models at a
trillion-parameter scale introduces unprecedented challenges, including
train-inference misalignment, inefficiencies in rollout processing, and
bottlenecks in the RL system. To address these, we pioneer three interconnected
innovations: (1) IcePop stabilizes RL training via token-level discrepancy
masking and clipping, resolving instability from training-inference mismatches;
(2) C3PO++ improves resource utilization for long rollouts under a token budget
by dynamically partitioning them, thereby obtaining high time efficiency; and
(3) ASystem, a high-performance RL framework designed to overcome the systemic
bottlenecks that impede trillion-parameter model training. Ring-1T delivers
breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on
HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a
silver medal-level result on the IMO-2025, underscoring its exceptional
reasoning capabilities. By releasing the complete 1T parameter MoE model to the
community, we provide the research community with direct access to cutting-edge
reasoning capabilities. This contribution marks a significant milestone in
democratizing large-scale reasoning intelligence and establishes a new baseline
for open-source model performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter.</div>
</details>
</div>
<div class="card">
<div class="title">See the Text: From Tokenization to Visual Reading</div>
<div class="meta-line">Authors: Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang</div>
<div class="meta-line">First: 2025-10-21T17:34:48+00:00 · Latest: 2025-10-21T17:34:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18840v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18840v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">People see text. Humans read by recognizing words as visual objects,
including their shapes, layouts, and patterns, before connecting them to
meaning, which enables us to handle typos, distorted fonts, and various scripts
effectively. Modern large language models (LLMs), however, rely on subword
tokenization, fragmenting text into pieces from a fixed vocabulary. While
effective for high-resource languages, this approach over-segments low-resource
languages, yielding long, linguistically meaningless sequences and inflating
computation. In this work, we challenge this entrenched paradigm and move
toward a vision-centric alternative. Our method, SeeTok, renders text as images
(visual-text) and leverages pretrained multimodal LLMs to interpret them,
reusing strong OCR and text-vision alignment abilities learned from large-scale
multimodal training. Across three different language tasks, SeeTok matches or
surpasses subword tokenizers while requiring 4.43 times fewer tokens and
reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,
robustness to typographic noise, and linguistic hierarchy. SeeTok signals a
shift from symbolic tokenization to human-like visual reading, and takes a step
toward more natural and cognitively inspired language models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">People see text.</div>
</details>
</div>
<div class="card">
<div class="title">How Transformers Learn In-Context Recall Tasks? Optimality, Training   Dynamics and Generalization</div>
<div class="meta-line">Authors: Quan Nguyen, Thanh Nguyen-Tang</div>
<div class="meta-line">First: 2025-05-21T01:26:44+00:00 · Latest: 2025-10-21T17:34:30+00:00</div>
<div class="meta-line">Comments: V3: added new results for softmax attention, typos fixed, titled
  changed. 33 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.15009v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.15009v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the approximation capabilities, convergence speeds and
on-convergence behaviors of transformers trained on in-context recall tasks --
which requires to recognize the \emph{positional} association between a pair of
tokens from in-context examples. Existing theoretical results only focus on the
in-context reasoning behavior of transformers after being trained for the
\emph{one} gradient descent step. It remains unclear what is the on-convergence
behavior of transformers being trained by gradient descent and how fast the
convergence rate is. In addition, the generalization of transformers in
one-step in-context reasoning has not been formally investigated. This work
addresses these gaps. We first show that a class of transformers with either
linear, ReLU or softmax attentions, is provably Bayes-optimal for an in-context
recall task. When being trained with gradient descent, we show via a
finite-sample analysis that the expected loss converges at linear rate to the
Bayes risks. Moreover, we show that the trained transformers exhibit
out-of-distribution (OOD) generalization, i.e., generalizing to samples outside
of the population distribution. Our theoretical findings are further supported
by extensive empirical validations, showing that \emph{without} proper
parameterization, models with larger expressive power surprisingly \emph{fail}
to generalize OOD after being trained by gradient descent.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study the approximation capabilities, convergence speeds and on-convergence behaviors of transformers trained on in-context recall tasks -- which requires to recognize the \emph{positional} association between a pair of tokens from in-context examples.</div>
</details>
</div>
<div class="card">
<div class="title">MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long   Context Training</div>
<div class="meta-line">Authors: Wenxuan Li, Chengruidong Zhang, Huiqiang Jiang, Yucheng Li, Yuqing Yang, Lili Qiu</div>
<div class="meta-line">First: 2025-10-21T17:25:32+00:00 · Latest: 2025-10-21T17:25:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18830v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18830v1">PDF</a> · <a href="https://github.com/microsoft/MInference/tree/main/MTraining">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The adoption of long context windows has become a standard feature in Large
Language Models (LLMs), as extended contexts significantly enhance their
capacity for complex reasoning and broaden their applicability across diverse
scenarios. Dynamic sparse attention is a promising approach for reducing the
computational cost of long-context. However, efficiently training LLMs with
dynamic sparse attention on ultra-long contexts-especially in distributed
settings-remains a significant challenge, due in large part to worker- and
step-level imbalance. This paper introduces MTraining, a novel distributed
methodology leveraging dynamic sparse attention to enable efficient training
for LLMs with ultra-long contexts. Specifically, MTraining integrates three key
components: a dynamic sparse training pattern, balanced sparse ring attention,
and hierarchical sparse ring attention. These components are designed to
synergistically address the computational imbalance and communication overheads
inherent in dynamic sparse attention mechanisms during the training of models
with extensive context lengths. We demonstrate the efficacy of MTraining by
training Qwen2.5-3B, successfully expanding its context window from 32K to 512K
tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite
of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A
Haystack, reveal that MTraining achieves up to a 6x higher training throughput
while preserving model accuracy. Our code is available at
https://github.com/microsoft/MInference/tree/main/MTraining.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios.</div>
</details>
</div>
<div class="card">
<div class="title">TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode   Accelerator with Table-Lookup Matmul on Edge FPGAs</div>
<div class="meta-line">Authors: Ye Qiao, Zhiheng Chen, Yifan Zhang, Yian Wang, Sitao Huang</div>
<div class="meta-line">First: 2025-10-03T05:37:51+00:00 · Latest: 2025-10-21T17:20:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15926v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.15926v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the emergence of wearable devices and other embedded systems, deploying
large language models (LLMs) on edge platforms has become an urgent need.
However, this is challenging because of their high computational and memory
demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)
compress weights to as low as 1.58~bits with minimal accuracy loss, edge
deployment is still constrained by limited on-chip resources, power budgets,
and the often-neglected long latency of the prefill stage. We present
\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for
low-power edge FPGAs that fully supports both prefill and autoregressive
decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates
several novel techniques, including (1) a table-lookup-based ternary matrix
multiplication (TLMM) engine utilizing grouped activations and online
precomputation for low resource utilization and high throughput; (2) a
fine-grained analytic URAM-based weight buffer management scheme for efficient
loading and compute engine access; (3) a streaming dataflow architecture that
fuses floating-point element-wise operations with linear computations to hide
latency; (4) a reversed-reordered prefill stage attention with fused attention
operations for high memory efficiency; and (5) a resource-efficient specialized
decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to
25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for
64--128 token prompts, marking a significant energy-efficiency advancement in
LLM inference on edge FPGAs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the emergence of wearable devices and other embedded systems, deploying large language models (LLMs) on edge platforms has become an urgent need.</div>
</details>
</div>
<div class="card">
<div class="title">Glyph: Scaling Context Windows via Visual-Text Compression</div>
<div class="meta-line">Authors: Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, Yushi Bai, Jie Tang, Hongning Wang, Minlie Huang</div>
<div class="meta-line">First: 2025-10-20T17:58:56+00:00 · Latest: 2025-10-21T17:12:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17800v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.17800v2">PDF</a> · <a href="https://github.com/thu-coai/Glyph">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning.</div>
</details>
</div>
<div class="card">
<div class="title">ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</div>
<div class="meta-line">Authors: Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang</div>
<div class="meta-line">First: 2025-10-21T16:48:49+00:00 · Latest: 2025-10-21T16:48:49+00:00</div>
<div class="meta-line">Comments: 17 pages, 5 fiugres</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18795v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18795v1">PDF</a> · <a href="https://github.com/VisionXLab/ProCLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The original CLIP text encoder is limited by a maximum input length of 77
tokens, which hampers its ability to effectively process long texts and perform
fine-grained semantic understanding. In addition, the CLIP text encoder lacks
support for multilingual inputs. All these limitations significantly restrict
its applicability across a broader range of tasks. Recent studies have
attempted to replace the CLIP text encoder with an LLM-based embedder to
enhance its ability in processing long texts, multilingual understanding, and
fine-grained semantic comprehension. However, because the representation spaces
of LLMs and the vision-language space of CLIP are pretrained independently
without alignment priors, direct alignment using contrastive learning can
disrupt the intrinsic vision-language alignment in the CLIP image encoder,
leading to an underutilization of the knowledge acquired during pre-training.
To address this challenge, we propose ProCLIP, a curriculum learning-based
progressive vision-language alignment framework to effectively align the CLIP
image encoder with an LLM-based embedder. Specifically, ProCLIP first distills
knowledge from CLIP&#x27;s text encoder into the LLM-based embedder to leverage
CLIP&#x27;s rich pretrained knowledge while establishing initial alignment between
the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns
the CLIP image encoder with the LLM-based embedder through image-text
contrastive tuning, employing self-distillation regularization to avoid
overfitting. To achieve a more effective alignment, instance semantic alignment
loss and embedding structure alignment loss are employed during representation
inheritance and contrastive tuning. The Code is available at
https://github.com/VisionXLab/ProCLIP</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding.</div>
</details>
</div>
<div class="card">
<div class="title">A unified framework for establishing the universal approximation of   transformer-type architectures</div>
<div class="meta-line">Authors: Jingpu Cheng, Ting Lin, Zuowei Shen, Qianxiao Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-30T06:50:39+00:00 · Latest: 2025-10-21T15:34:53+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 camera-ready</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.23551v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.23551v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the universal approximation property (UAP) of transformer-type
architectures, providing a unified theoretical framework that extends prior
results on residual networks to models incorporating attention mechanisms. Our
work identifies token distinguishability as a fundamental requirement for UAP
and introduces a general sufficient condition that applies to a broad class of
architectures. Leveraging an analyticity assumption on the attention layer, we
can significantly simplify the verification of this condition, providing a
non-constructive approach in establishing UAP for such architectures. We
demonstrate the applicability of our framework by proving UAP for transformers
with various attention mechanisms, including kernel-based and sparse attention
mechanisms. The corollaries of our results either generalize prior works or
establish UAP for architectures not previously covered. Furthermore, our
framework offers a principled foundation for designing novel transformer
architectures with inherent UAP guarantees, including those with specific
functional symmetries. We propose examples to illustrate these insights.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We investigate the universal approximation property (UAP) of transformer-type architectures, providing a unified theoretical framework that extends prior results on residual networks to models incorporating attention mechanisms.</div>
</details>
</div>
<div class="card">
<div class="title">SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image   Generation</div>
<div class="meta-line">Authors: Siyong Jian, Huan Wang</div>
<div class="meta-line">First: 2025-10-21T15:17:37+00:00 · Latest: 2025-10-21T15:17:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18716v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18716v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive image generation models like Janus-Pro produce high-quality
images, but at the significant cost of high memory and ever-growing
computational demands due to the large number of visual tokens. While KV cache
compression has been extensively studied in language modeling, it still remains
largely unexplored for the image generation domain. In this work, we begin by
identifying a distinct and prominent attention phenomenon, which we term
spatial locality and emergent semantic sink. To leverage this key insight, we
introduce a novel KV cache compression framework. Specifically, we compress the
KV cache for all visual tokens by adaptively decoupling attention heads into
two separate types: for spatial-locality heads, our method maintains a short
recent token window; for semantic-sink heads, it strategically preserves a
compact set of highly-attended tokens. Our extensive experiments demonstrate
that the proposed method achieves a 5$\times$ reduction in memory usage and a
notable 6.6$\times$ speedup in overall throughput with only minimal visual
quality loss, thereby enabling highly efficient native autoregressive image
generation on resource-constrained hardware.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive image generation models like Janus-Pro produce high-quality images, but at the significant cost of high memory and ever-growing computational demands due to the large number of visual tokens.</div>
</details>
</div>
<div class="card">
<div class="title">A Renaissance of Explicit Motion Information Mining from Transformers   for Action Recognition</div>
<div class="meta-line">Authors: Peiqin Zhuang, Lei Bai, Yichao Wu, Ding Liang, Luping Zhou, Yali Wang, Wanli Ouyang</div>
<div class="meta-line">First: 2025-10-21T15:01:48+00:00 · Latest: 2025-10-21T15:01:48+00:00</div>
<div class="meta-line">Comments: accepted by Pattern Recognition. We have been always curious to see
  whether our designs could be beneficial in other scenarios, such as embedding
  it into the DiT model or 3D-VAE for video generation. If you are interested
  in it, why not give it a shot?</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18705v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, action recognition has been dominated by transformer-based methods,
thanks to their spatiotemporal contextual aggregation capacities. However,
despite the significant progress achieved on scene-related datasets, they do
not perform well on motion-sensitive datasets due to the lack of elaborate
motion modeling designs. Meanwhile, we observe that the widely-used cost volume
in traditional action recognition is highly similar to the affinity matrix
defined in self-attention, but equipped with powerful motion modeling
capacities. In light of this, we propose to integrate those effective motion
modeling properties into the existing transformer in a unified and neat way,
with the proposal of the Explicit Motion Information Mining module (EMIM). In
EMIM, we propose to construct the desirable affinity matrix in a cost volume
style, where the set of key candidate tokens is sampled from the query-based
neighboring area in the next frame in a sliding-window manner. Then, the
constructed affinity matrix is used to aggregate contextual information for
appearance modeling and is converted into motion features for motion modeling
as well. We validate the motion modeling capacities of our method on four
widely-used datasets, and our method performs better than existing
state-of-the-art approaches, especially on motion-sensitive datasets, i.e.,
Something-Something V1 &amp; V2.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently, action recognition has been dominated by transformer-based methods, thanks to their spatiotemporal contextual aggregation capacities.</div>
</details>
</div>
<div class="card">
<div class="title">Exploring a Unified Vision-Centric Contrastive Alternatives on   Multi-Modal Web Documents</div>
<div class="meta-line">Authors: Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou</div>
<div class="meta-line">First: 2025-10-21T14:59:29+00:00 · Latest: 2025-10-21T14:59:29+00:00</div>
<div class="meta-line">Comments: Project page: this https://linyq17.github.io/VC2L/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18703v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18703v1">PDF</a> · <a href="https://github.com/showlab/VC2L">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://linyq17.github.io/VC2L/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contrastive vision-language models such as CLIP have demonstrated strong
performance across a wide range of multimodal tasks by learning from aligned
image-text pairs. However, their ability to handle complex, real-world web
documents remains limited, particularly in scenarios where text and images are
interleaved, loosely aligned, or embedded in visual form. To address these
challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified
framework that models text, images, and their combinations using a single
vision transformer. VC2L operates entirely in pixel space by rendering all
inputs, whether textual, visual, or combined, as images, thus eliminating the
need for OCR, text tokenization, or modality fusion strategy. To capture
complex cross-modal relationships in multimodal web documents, VC2L employs a
snippet-level contrastive learning objective that aligns consecutive multimodal
segments, leveraging the inherent coherence of documents without requiring
explicitly paired image-text data. To assess the effectiveness of this
approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,
designed to evaluate cross-modal retrieval, fine-grained sequential
understanding, and generalization to unseen data, respectively. Empirical
results show that VC2L achieves competitive or superior performance compared to
CLIP-style models on both the proposed benchmarks and established datasets such
as M-BEIR and MTEB. These findings underscore the potential of multimodal web
data as a valuable training resource for contrastive learning and illustrate
the scalability of a unified, vision-centric approach for multimodal
representation learning. Code and models are available at:
https://github.com/showlab/VC2L.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Contrastive vision-language models such as CLIP have demonstrated strong performance across a wide range of multimodal tasks by learning from aligned image-text pairs.</div>
</details>
</div>
<div class="card">
<div class="title">MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation</div>
<div class="meta-line">Authors: Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, Zhendong Mao</div>
<div class="meta-line">First: 2025-10-21T14:50:42+00:00 · Latest: 2025-10-21T14:50:42+00:00</div>
<div class="meta-line">Comments: 15 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18692v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18692v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long video generation with Diffusion Transformers (DiTs) is bottlenecked by
the quadratic scaling of full attention with sequence length. Since attention
is highly redundant, outputs are dominated by a small subset of query-key
pairs. Existing sparse methods rely on blockwise coarse estimation, whose
accuracy-efficiency trade-offs are constrained by block size. This paper
introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention
that uses a lightweight, learnable token router to precisely match tokens
without blockwise estimation. Through semantic-aware routing, MoGA enables
effective long-range interactions. As a kernel-free method, MoGA integrates
seamlessly with modern attention stacks, including FlashAttention and sequence
parallelism. Building on MoGA, we develop an efficient long video generation
model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps,
with a context length of approximately 580k. Comprehensive experiments on
various video generation tasks validate the effectiveness of our approach.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length.</div>
</details>
</div>
<div class="card">
<div class="title">Lightweight Baselines for Medical Abstract Classification: DistilBERT   with Cross-Entropy as a Strong Default</div>
<div class="meta-line">Authors: Jiaqi Liu, Tong Wang, Su Liu, Xin Hu, Ran Tong, Lanruo Wang, Jiexi Xu</div>
<div class="meta-line">First: 2025-10-11T05:05:21+00:00 · Latest: 2025-10-21T14:44:14+00:00</div>
<div class="meta-line">Comments: Healthcare AI, Medical Text Classification,LLM, DistilBERT</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.10025v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.10025v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The research evaluates lightweight medical abstract classification methods to
establish their maximum performance capabilities under financial budget
restrictions. On the public medical abstracts corpus, we finetune BERT base and
Distil BERT with three objectives cross entropy (CE), class weighted CE, and
focal loss under identical tokenization, sequence length, optimizer, and
schedule. DistilBERT with plain CE gives the strongest raw argmax trade off,
while a post hoc operating point selection (validation calibrated, classwise
thresholds) sub stantially improves deployed performance; under this tuned
regime, focal benefits most. We report Accuracy, Macro F1, and WeightedF1,
release evaluation artifacts, and include confusion analyses to clarify error
structure. The practical takeaway is to start with a compact encoder and CE,
then add lightweight calibration or thresholding when deployment requires
higher macro balance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research evaluates lightweight medical abstract classification methods to establish their maximum performance capabilities under financial budget restrictions.</div>
</details>
</div>
<div class="card">
<div class="title">VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient   Large Speech-Language Model</div>
<div class="meta-line">Authors: Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, Peixian Chen, Mengdan Zhang, Hang Shao, Jian Li, Jinlong Peng, Haoyu Cao, Ke Li, Rongrong Ji, Xing Sun</div>
<div class="meta-line">First: 2025-05-06T17:59:53+00:00 · Latest: 2025-10-21T13:54:36+00:00</div>
<div class="meta-line">Comments: Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.03739v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.03739v2">PDF</a> · <a href="https://github.com/VITA-MLLM/VITA-Audio">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the growing requirement for natural human-computer interaction,
speech-based systems receive increasing attention as speech is one of the most
common forms of daily communication. However, the existing speech models still
experience high latency when generating the first audio token during streaming,
which poses a significant bottleneck for deployment. To address this issue, we
propose VITA-Audio, an end-to-end large speech model with fast audio-text token
generation. Specifically, we introduce a lightweight Multiple Cross-modal Token
Prediction (MCTP) module that efficiently generates multiple audio tokens
within a single model forward pass, which not only accelerates the inference
but also significantly reduces the latency for generating the first audio in
streaming scenarios. In addition, a four-stage progressive training strategy is
explored to achieve model acceleration with minimal loss of speech quality. To
our knowledge, VITA-Audio is the first multi-modal large language model capable
of generating audio output during the first forward pass, enabling real-time
conversational capabilities with minimal latency. VITA-Audio is fully
reproducible and is trained on open-source data only. Experimental results
demonstrate that our model achieves an inference speedup of 3~5x at the 7B
parameter scale, but also significantly outperforms open-source models of
similar model size on multiple benchmarks for automatic speech recognition
(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication.</div>
</details>
</div>
<div class="card">
<div class="title">Patent Language Model Pretraining with ModernBERT</div>
<div class="meta-line">Authors: Amirhossein Yousefiramandi, Ciaran Cooney</div>
<div class="meta-line">First: 2025-09-18T13:04:30+00:00 · Latest: 2025-10-21T13:13:11+00:00</div>
<div class="meta-line">Comments: 7 pages, 5 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.14926v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.14926v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based language models such as BERT have become foundational in
NLP, yet their performance degrades in specialized domains like patents, which
contain long, technical, and legally structured text. Prior approaches to
patent NLP have primarily relied on fine-tuning general-purpose models or
domain-adapted variants pretrained with limited data. In this work, we pretrain
3 domain-specific masked language models for patents, using the ModernBERT
architecture and a curated corpus of over 60 million patent records. Our
approach incorporates architectural optimizations, including FlashAttention,
rotary embeddings, and GLU feed-forward layers. We evaluate our models on four
downstream patent classification tasks. Our model, ModernBERT-base-PT,
consistently outperforms the general-purpose ModernBERT baseline on three out
of four datasets and achieves competitive performance with a baseline
PatentBERT. Additional experiments with ModernBERT-base-VX and
Mosaic-BERT-large demonstrate that scaling the model size and customizing the
tokenizer further enhance performance on selected tasks. Notably, all
ModernBERT variants retain substantially faster inference over - 3x that of
PatentBERT - underscoring their suitability for time-sensitive applications.
These results underscore the benefits of domain-specific pretraining and
architectural improvements for patent-focused NLP tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text.</div>
</details>
</div>
<div class="card">
<div class="title">SimKO: Simple Pass@K Policy Optimization</div>
<div class="meta-line">Authors: Ruotian Peng, Yi Ren, Zhouliang Yu, Weiyang Liu, Yandong Wen</div>
<div class="meta-line">First: 2025-10-16T15:40:49+00:00 · Latest: 2025-10-21T12:46:48+00:00</div>
<div class="meta-line">Comments: Technical report (20 pages, 10 figures, project page:
  https://spherelab.ai/simko/)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14807v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.14807v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spherelab.ai/simko/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K&gt;1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR&#x27;s exploration.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">RAISE: A Unified Framework for Responsible AI Scoring and Evaluation</div>
<div class="meta-line">Authors: Loc Phuc Truong Nguyen, Hung Thanh Do</div>
<div class="meta-line">First: 2025-10-21T12:15:13+00:00 · Latest: 2025-10-21T12:15:13+00:00</div>
<div class="meta-line">Comments: Accepted at the 26th International Conference on Principles and
  Practice of Multi-Agent Systems</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18559v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18559v1">PDF</a> · <a href="https://github.com/raise-framework/raise">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As AI systems enter high-stakes domains, evaluation must extend beyond
predictive accuracy to include explainability, fairness, robustness, and
sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a
unified framework that quantifies model performance across these four
dimensions and aggregates them into a single, holistic Responsibility Score. We
evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular
ResNet, and a Feature Tokenizer Transformer, on structured datasets from
finance, healthcare, and socioeconomics. Our findings reveal critical
trade-offs: the MLP demonstrated strong sustainability and robustness, the
Transformer excelled in explainability and fairness at a very high
environmental cost, and the Tabular ResNet offered a balanced profile. These
results underscore that no single model dominates across all responsibility
criteria, highlighting the necessity of multi-dimensional evaluation for
responsible model selection. Our implementation is available at:
https://github.com/raise-framework/raise.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As AI systems enter high-stakes domains, evaluation must extend beyond predictive accuracy to include explainability, fairness, robustness, and sustainability.</div>
</details>
</div>
<div class="card">
<div class="title">Pay Attention to the Triggers: Constructing Backdoors That Survive   Distillation</div>
<div class="meta-line">Authors: Giovanni De Muri, Mark Vero, Robin Staab, Martin Vechev</div>
<div class="meta-line">First: 2025-10-21T11:39:45+00:00 · Latest: 2025-10-21T11:39:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18541v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18541v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs are often used by downstream users as teacher models for knowledge
distillation, compressing their capabilities into memory-efficient models.
However, as these teacher models may stem from untrusted parties, distillation
can raise unexpected security risks. In this paper, we investigate the security
implications of knowledge distillation from backdoored teacher models. First,
we show that prior backdoors mostly do not transfer onto student models. Our
key insight is that this is because existing LLM backdooring methods choose
trigger tokens that rarely occur in usual contexts. We argue that this
underestimates the security risks of knowledge distillation and introduce a new
backdooring technique, T-MTB, that enables the construction and study of
transferable backdoors. T-MTB carefully constructs a composite backdoor
trigger, made up of several specific tokens that often occur individually in
anticipated distillation datasets. As such, the poisoned teacher remains
stealthy, while during distillation the individual presence of these tokens
provides enough signal for the backdoor to transfer onto the student. Using
T-MTB, we demonstrate and extensively study the security risks of transferable
backdoors across two attack scenarios, jailbreaking and content modulation, and
across four model families of LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLMs are often used by downstream users as teacher models for knowledge distillation, compressing their capabilities into memory-efficient models.</div>
</details>
</div>
<div class="card">
<div class="title">REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion   Transformers</div>
<div class="meta-line">Authors: Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, Liang Zheng</div>
<div class="meta-line">First: 2025-04-14T17:59:53+00:00 · Latest: 2025-10-21T10:39:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.10483v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.10483v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://end2end-diffusion.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper we tackle a fundamental question: &quot;Can we train latent
diffusion models together with the variational auto-encoder (VAE) tokenizer in
an end-to-end manner?&quot; Traditional deep-learning wisdom dictates that
end-to-end training is often preferable when possible. However, for latent
diffusion transformers, it is observed that end-to-end training both VAE and
diffusion-model using standard diffusion-loss is ineffective, even causing a
degradation in final performance. We show that while diffusion loss is
ineffective, end-to-end training can be unlocked through the
representation-alignment (REPA) loss -- allowing both VAE and diffusion model
to be jointly tuned during the training process. Despite its simplicity, the
proposed training recipe (REPA-E) shows remarkable performance; speeding up
diffusion model training by over 17x and 45x over REPA and vanilla training
recipes, respectively. Interestingly, we observe that end-to-end tuning with
REPA-E also improves the VAE itself; leading to improved latent space structure
and downstream generation performance. In terms of final performance, our
approach sets a new state-of-the-art; achieving FID of 1.12 and 1.69 with and
without classifier-free guidance on ImageNet 256 x 256. Code is available at
https://end2end-diffusion.github.io.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper we tackle a fundamental question: &quot;Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?&quot; Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible.</div>
</details>
</div>
<div class="card">
<div class="title">VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive   Token Caching</div>
<div class="meta-line">Authors: Siyu Xu, Yunke Wang, Chenghao Xia, Dihao Zhu, Tao Huang, Chang Xu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-02-04T09:48:14+00:00 · Latest: 2025-10-21T10:33:29+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.02175v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.02175v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vla-cache.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have demonstrated strong multi-modal
reasoning capabilities, enabling direct action generation from visual
perception and language instructions in an end-to-end manner. However, their
substantial computational cost poses a challenge for real-time robotic control,
where rapid decision-making is essential. This paper introduces VLA-Cache, a
training-free inference acceleration method that reduces computational overhead
by adaptively caching and reusing static visual tokens across frames.
Exploiting the temporal continuity in robotic manipulation, VLA-Cache
identifies minimally changed tokens between adjacent frames and reuses their
cached key-value representations, thereby circumventing redundant computations.
Additionally, to maintain action precision, VLA-Cache selectively re-computes
task-relevant tokens that are environmentally sensitive, ensuring the fidelity
of critical visual information. To further optimize efficiency, we introduce a
layer adaptive token reusing strategy that dynamically adjusts the reuse ratio
based on attention concentration across decoder layers, prioritizing critical
tokens for recomputation. Extensive experiments on two simulation platforms
(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache
achieves up to 1.7x speedup in CUDA latency and a 15% increase in control
frequency, with negligible loss on task success rate. The code and videos can
be found at our project page: https://vla-cache.github.io.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action (VLA) models have demonstrated strong multi-modal reasoning capabilities, enabling direct action generation from visual perception and language instructions in an end-to-end manner.</div>
</details>
</div>
<div class="card">
<div class="title">LIMOPro: Reasoning Refinement for Efficient and Effective Test-time   Scaling</div>
<div class="meta-line">Authors: Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, Pengfei Liu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-25T15:17:57+00:00 · Latest: 2025-10-21T10:15:36+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.19187v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.19187v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have demonstrated remarkable reasoning
capabilities through test-time scaling approaches, particularly when fine-tuned
with chain-of-thought (CoT) data distilled from more powerful large reasoning
models (LRMs). However, these reasoning chains often contain verbose elements
that mirror human problem-solving, categorized as progressive reasoning (the
essential solution development path) and functional elements (verification
processes, alternative solution approaches, and error corrections). While
progressive reasoning is crucial, the functional elements significantly
increase computational demands during test-time inference. We introduce PIR
(Perplexity-based Importance Refinement), a principled framework that
quantitatively evaluates the importance of each reasoning step based on its
impact on answer prediction confidence. PIR systematically identifies and
selectively prunes only low-importance functional steps while preserving
progressive reasoning components, creating optimized training data that
maintains the integrity of the core solution path while reducing verbosity.
Models fine-tuned on PIR-optimized data exhibit superior test-time scaling
properties, generating more concise reasoning chains while achieving improved
accuracy (+0.9\% to +6.6\%) with significantly reduced token usage (-3\% to
-41\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).
Our approach demonstrates strong generalizability across different model sizes,
data sources, and token budgets, offering a practical solution for deploying
reasoning-capable LLMs in scenarios where efficient test-time scaling, response
time, and computational efficiency are valuable constraints.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs).</div>
</details>
</div>
<div class="card">
<div class="title">Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion   Models</div>
<div class="meta-line">Authors: Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng</div>
<div class="meta-line">First: 2025-10-21T09:30:45+00:00 · Latest: 2025-10-21T09:30:45+00:00</div>
<div class="meta-line">Comments: Code and models available at: https://github.com/tianciB/VFM-VAE</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18457v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18457v1">PDF</a> · <a href="https://github.com/tianciB/VFM-VAE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of Latent Diffusion Models (LDMs) is critically dependent on
the quality of their visual tokenizer. While recent works have explored
incorporating Vision Foundation Models (VFMs) via distillation, we identify a
fundamental flaw in this approach: it inevitably weakens the robustness of
alignment with the original VFM, causing the aligned latents to deviate
semantically under distribution shifts. In this paper, we bypass distillation
by proposing a more direct approach: Vision Foundation Model Variational
Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM&#x27;s
semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE
decoder with Multi-Scale Latent Fusion and Progressive Resolution
Reconstruction blocks, enabling high-quality reconstruction from spatially
coarse VFM features. Furthermore, we provide a comprehensive analysis of
representation dynamics during diffusion training, introducing the proposed
SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows
us to develop a joint tokenizer-diffusion alignment strategy that dramatically
accelerates convergence. Our innovations in tokenizer design and training
strategy lead to superior performance and efficiency: our system reaches a gFID
(w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).
With continued training to 640 epochs, it further attains a gFID (w/o CFG) of
1.62, establishing direct VFM integration as a superior paradigm for LDMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The performance of Latent Diffusion Models (LDMs) is critically dependent on the quality of their visual tokenizer.</div>
</details>
</div>
<div class="card">
<div class="title">Explaining Large Language Models with gSMILE</div>
<div class="meta-line">Authors: Zeinab Dehghani, Mohammed Naveed Akram, Koorosh Aslansefat, Adil Khan, Yiannis Papadopoulos</div>
<div class="meta-line">First: 2025-05-27T18:32:38+00:00 · Latest: 2025-10-21T08:27:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.21657v5">Abs</a> · <a href="http://arxiv.org/pdf/2505.21657v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) such as GPT, LLaMA, and Claude achieve
remarkable performance in text generation but remain opaque in their
decision-making processes, limiting trust and accountability in high-stakes
applications. We present gSMILE (generative SMILE), a model-agnostic,
perturbation-based framework for token-level interpretability in LLMs.
Extending the SMILE methodology, gSMILE uses controlled prompt perturbations,
Wasserstein distance metrics, and weighted linear surrogates to identify input
tokens with the most significant impact on the output. This process enables the
generation of intuitive heatmaps that visually highlight influential tokens and
reasoning paths. We evaluate gSMILE across leading LLMs (OpenAI&#x27;s
gpt-3.5-turbo-instruct, Meta&#x27;s LLaMA 3.1 Instruct Turbo, and Anthropic&#x27;s Claude
2.1) using attribution fidelity, attribution consistency, attribution
stability, attribution faithfulness, and attribution accuracy as metrics.
Results show that gSMILE delivers reliable human-aligned attributions, with
Claude 2.1 excelling in attention fidelity and GPT-3.5 achieving the highest
output consistency. These findings demonstrate gSMILE&#x27;s ability to balance
model performance and interpretability, enabling more transparent and
trustworthy AI systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) such as GPT, LLaMA, and Claude achieve remarkable performance in text generation but remain opaque in their decision-making processes, limiting trust and accountability in high-stakes applications.</div>
</details>
</div>
<div class="card">
<div class="title">SpecExit: Accelerating Large Reasoning Model via Speculative Exit</div>
<div class="meta-line">Authors: Rubing Yang, Huajun Bai, Song Liu, Guanghua Yu, Runzhi Fan, Yanbin Dang, Jiejing Zhang, Kai Liu, Jianchen Zhu, Peng Chen</div>
<div class="meta-line">First: 2025-09-29T03:39:32+00:00 · Latest: 2025-10-21T07:44:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.24248v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.24248v2">PDF</a> · <a href="https://github.com/Tencent/AngelSlim">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their strong performance on reasoning tasks, large reasoning models
(LRMs) often suffer from overthinking, producing unnecessarily long outputs and
incurring high end-to-end latency, a significant limitation to their real-world
deployment. To address overthinking, early-exit mechanisms have been proposed
to terminate reasoning before typical completion, showing that this approach
can effectively shorten generation length with minimal impact on accuracy.
However, their reliance on probing mechanisms introduces a detection overhead
that limits their end-to-end latency gains and compromises their
generalizability across diverse problems. Inspired by the use of hidden states
in speculative decoding, we propose SpecExit, a novel framework that predicts
both future tokens and an early-exit signal directly from a lightweight draft
model without probing overhead. Our method offers significant improvements,
reducing average generation length by 66\% and achieving a 2.5x speedup in
end-to-end latency compared to the speculative decoding baseline, without
compromising accuracy. Our method leverages the inherent signals from hidden
states to provide effective early-exit signals, suggesting broader use of
hidden states for efficient reasoning. Our code is available at
https://github.com/Tencent/AngelSlim.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment.</div>
</details>
</div>
<div class="card">
<div class="title">Polyline Path Masked Attention for Vision Transformer</div>
<div class="meta-line">Authors: Zhongchen Zhao, Chaodong Xiao, Hui Lin, Qi Xie, Lei Zhang, Deyu Meng</div>
<div class="meta-line">First: 2025-06-19T00:52:30+00:00 · Latest: 2025-10-21T07:16:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.15940v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.15940v2">PDF</a> · <a href="https://github.com/zhongchenzhao/PPMA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Global dependency modeling and spatial position modeling are two core issues
of the foundational architecture design in current deep learning frameworks.
Recently, Vision Transformers (ViTs) have achieved remarkable success in
computer vision, leveraging the powerful global dependency modeling capability
of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its
significant potential in natural language processing tasks by explicitly
modeling the spatial adjacency prior through the structured mask. In this
paper, we propose Polyline Path Masked Attention (PPMA) that integrates the
self-attention mechanism of ViTs with an enhanced structured mask of Mamba2,
harnessing the complementary strengths of both architectures. Specifically, we
first ameliorate the traditional structured mask of Mamba2 by introducing a 2D
polyline path scanning strategy and derive its corresponding structured mask,
polyline path mask, which better preserves the adjacency relationships among
image tokens. Notably, we conduct a thorough theoretical analysis on the
structural characteristics of the proposed polyline path mask and design an
efficient algorithm for the computation of the polyline path mask. Next, we
embed the polyline path mask into the self-attention mechanism of ViTs,
enabling explicit modeling of spatial adjacency prior. Extensive experiments on
standard benchmarks, including image classification, object detection, and
segmentation, demonstrate that our model outperforms previous state-of-the-art
approaches based on both state-space models and Transformers. For example, our
proposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K
semantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%,
respectively. Code is available at https://github.com/zhongchenzhao/PPMA.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Global dependency modeling and spatial position modeling are two core issues of the foundational architecture design in current deep learning frameworks.</div>
</details>
</div>
<div class="card">
<div class="title">Segment Policy Optimization: Effective Segment-Level Credit Assignment   in RL for Large Language Models</div>
<div class="meta-line">Authors: Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, Shuang Qiu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-29T15:38:19+00:00 · Latest: 2025-10-21T07:05:48+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.23564v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.23564v2">PDF</a> · <a href="https://github.com/AIFrameResearch/SPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enhancing the reasoning capabilities of large language models effectively
using reinforcement learning (RL) remains a crucial challenge. Existing
approaches primarily adopt two contrasting advantage estimation granularities:
token-level methods (e.g., PPO) aim to provide fine-grained advantage signals
but suffer from inaccurate estimation due to difficulties in training an
accurate critic model. On the other extreme, trajectory-level methods (e.g.,
GRPO) solely rely on a coarse-grained advantage signal from the final reward,
leading to imprecise credit assignment. To address these limitations, we
propose Segment Policy Optimization (SPO), a novel RL framework that leverages
segment-level advantage estimation at an intermediate granularity, achieving a
better balance by offering more precise credit assignment than trajectory-level
methods and requiring fewer estimation points than token-level methods,
enabling accurate advantage estimation based on Monte Carlo (MC) without a
critic model. SPO features three components with novel strategies: (1) flexible
segment partition; (2) accurate segment advantage estimation; and (3) policy
optimization using segment advantages, including a novel probability-mask
strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain
for short chain-of-thought (CoT), featuring novel cutpoint-based partition and
chain-based advantage estimation, achieving $6$-$12$ percentage point
improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,
featuring novel tree-based advantage estimation, which significantly reduces
the cost of MC estimation, achieving $7$-$11$ percentage point improvements
over GRPO on MATH500 under 2K and 4K context evaluation. We make our code
publicly available at https://github.com/AIFrameResearch/SPO.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge.</div>
</details>
</div>
<div class="card">
<div class="title">FlexQuant: A Flexible and Efficient Dynamic Precision Switching   Framework for LLM Quantization</div>
<div class="meta-line">Authors: Fangxin Liu, Zongwu Wang, JinHong Xia, Junping Zhao, Shouren Zhao, Jinjin Li, Jian Liu, Li Jiang, Haibing Guan</div>
<div class="meta-line">First: 2025-05-21T07:42:53+00:00 · Latest: 2025-10-21T07:03:45+00:00</div>
<div class="meta-line">Comments: 10 pages, 7 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.12024v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.12024v3">PDF</a> · <a href="https://github.com/ZongwuWang/FlexQuant.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of large language models (LLMs) has exacerbated the
memory bottleneck due to the widening gap between model parameter scaling and
hardware capabilities. While post-training quantization techniques effectively
reduce memory overhead, existing methods predominantly rely on static
quantization strategies, which struggle to adapt to dynamic workloads. To
address this, we propose FlexQuant, a dynamic precision-switching framework
that optimizes the trade-off between inference speed and accuracy. Leveraging
model perplexity entropy and Kullback-Leibler divergence, FlexQuant enables
fine-grained, layer-wise mixed-precision quantization and dynamically adjusts
bit-widths during each token generation. FlexQuant provides a comprehensive
analysis of quantization strategies, introduces a precision requirement model
for optimal switching, and implements efficient fine-grained precision
management. Evaluations demonstrate that FlexQuant achieves a 1.3x end-to-end
speedup across diverse language tasks with negligible accuracy loss introduced.
This framework offers a flexible and adaptive solution for efficient LLM
deployment. Code is released at https://github.com/ZongwuWang/FlexQuant.git.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid advancement of large language models (LLMs) has exacerbated the memory bottleneck due to the widening gap between model parameter scaling and hardware capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive   Token Ensemble Decoding</div>
<div class="meta-line">Authors: Jinlin Li, Yuran Wang, Yifei Yuan, Xiao Zhou, Yingying Zhang, Xixian Yong, Yefeng Zheng, Xian Wu</div>
<div class="meta-line">First: 2025-10-21T06:11:24+00:00 · Latest: 2025-10-21T06:11:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18321v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.18321v1">PDF</a> · <a href="https://github.com/jinlin2021/ATED">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have recently achieved impressive
results in multimodal tasks such as image captioning and visual question
answering. However, they remain prone to object hallucination -- generating
descriptions of nonexistent or misidentified objects. Prior work has partially
mitigated this via auxiliary training objectives or external modules, but
challenges remain in terms of scalability, adaptability, and model
independence. To address these limitations, we propose Adaptive Token Ensemble
Decoding (ATED), a training-free, token-level ensemble framework that mitigates
hallucination by aggregating predictions from multiple LVLMs during inference.
ATED dynamically computes uncertainty-based weights for each model, reflecting
their reliability at each decoding step. It also integrates diverse decoding
paths to improve contextual grounding and semantic consistency. Experiments on
standard hallucination detection benchmarks demonstrate that ATED significantly
outperforms state-of-the-art methods, reducing hallucination without
compromising fluency or relevance. Our findings highlight the benefits of
adaptive ensembling and point to a promising direction for improving LVLM
robustness in high-stakes applications. The code is available at
https://github.com/jinlin2021/ATED.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have recently achieved impressive results in multimodal tasks such as image captioning and visual question answering.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
