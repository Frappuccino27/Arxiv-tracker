<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-04 03:14</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251104_0314</div>
    <div class="row"><div class="card">
<div class="title">Continuous Autoregressive Language Models</div>
<div class="meta-line">Authors: Chenze Shao, Darren Li, Fandong Meng, Jie Zhou</div>
<div class="meta-line">First: 2025-10-31T17:58:11+00:00 · Latest: 2025-10-31T17:58:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27688v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27688v1">PDF</a> · <a href="https://github.com/shaochenze/calm">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://shaochenze.github.io/blog/2025/CALM">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The efficiency of large language models (LLMs) is fundamentally limited by
their sequential, token-by-token generation process. We argue that overcoming
this bottleneck requires a new design axis for LLM scaling: increasing the
semantic bandwidth of each generative step. To this end, we introduce
Continuous Autoregressive Language Models (CALM), a paradigm shift from
discrete next-token prediction to continuous next-vector prediction. CALM uses
a high-fidelity autoencoder to compress a chunk of K tokens into a single
continuous vector, from which the original tokens can be reconstructed with
over 99.9\% accuracy. This allows us to model language as a sequence of
continuous vectors instead of discrete tokens, which reduces the number of
generative steps by a factor of K. The paradigm shift necessitates a new
modeling toolkit; therefore, we develop a comprehensive likelihood-free
framework that enables robust training, evaluation, and controllable sampling
in the continuous domain. Experiments show that CALM significantly improves the
performance-compute trade-off, achieving the performance of strong discrete
baselines at a significantly lower computational cost. More importantly, these
findings establish next-vector prediction as a powerful and scalable pathway
towards ultra-efficient language models. Code:
https://github.com/shaochenze/calm. Project:
https://shaochenze.github.io/blog/2025/CALM.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process.</div>
</details>
</div>
<div class="card">
<div class="title">The End of Manual Decoding: Towards Truly End-to-End Language Models</div>
<div class="meta-line">Authors: Zhichao Wang, Dongyang Ma, Xinting Huang, Deng Cai, Tian Lan, Jiahao Xu, Haitao Mi, Xiaoying Tang, Yan Wang</div>
<div class="meta-line">First: 2025-10-30T17:01:43+00:00 · Latest: 2025-10-31T17:36:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.26697v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.26697v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The &quot;end-to-end&quot; label for LLMs is a misnomer. In practice, they depend on a
non-differentiable decoding process that requires laborious, hand-tuning of
hyperparameters like temperature and top-p. This paper introduces AutoDeco, a
novel architecture that enables truly &quot;end-to-end&quot; generation by learning to
control its own decoding strategy. We augment the standard transformer with
lightweight heads that, at each step, dynamically predict context-specific
temperature and top-p values alongside the next-token logits. This approach
transforms decoding into a parametric, token-level process, allowing the model
to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that
AutoDeco not only significantly outperforms default decoding strategies but
also achieves performance comparable to an oracle-tuned baseline derived from
&quot;hacking the test set&quot;-a practical upper bound for any static method.
Crucially, we uncover an emergent capability for instruction-based decoding
control: the model learns to interpret natural language commands (e.g.,
&quot;generate with low randomness&quot;) and adjusts its predicted temperature and top-p
on a token-by-token basis, opening a new paradigm for steerable and interactive
LLM decoding.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The &quot;end-to-end&quot; label for LLMs is a misnomer.</div>
</details>
</div>
<div class="card">
<div class="title">SpecAttn: Speculating Sparse Attention</div>
<div class="meta-line">Authors: Harsh Shah</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-31T17:12:34+00:00 · Latest: 2025-10-31T17:12:34+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025 Workshop on Structured Probabilistic
  Inference &amp; Generative Modeling</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27641v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27641v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) face significant computational bottlenecks
during inference due to the quadratic complexity of self-attention mechanisms,
particularly as context lengths increase. We introduce SpecAttn, a novel
training-free approach that seamlessly integrates with existing speculative
decoding techniques to enable efficient sparse attention in pre-trained
transformers. Our key insight is to exploit the attention weights already
computed by the draft model during speculative decoding to identify important
tokens for the target model, eliminating redundant computation while
maintaining output quality. SpecAttn employs three core techniques: KL
divergence-based layer alignment between draft and target models, a
GPU-optimized sorting-free algorithm for top-p token selection from draft
attention patterns, and dynamic key-value cache pruning guided by these
predictions. By leveraging the computational work already performed in standard
speculative decoding pipelines, SpecAttn achieves over 75% reduction in
key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19
dataset, significantly outperforming existing sparse attention methods. Our
approach demonstrates that speculative execution can be enhanced to provide
approximate verification without significant performance degradation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) face significant computational bottlenecks during inference due to the quadratic complexity of self-attention mechanisms, particularly as context lengths increase.</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action   Model</div>
<div class="meta-line">Authors: John Won, Kyungmin Lee, Huiwon Jang, Dongyoung Kim, Jinwoo Shin</div>
<div class="meta-line">First: 2025-10-31T16:32:12+00:00 · Latest: 2025-10-31T16:32:12+00:00</div>
<div class="meta-line">Comments: 20 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27607v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, augmenting Vision-Language-Action models (VLAs) with world modeling
has shown promise in improving robotic policy learning. However, it remains
challenging to jointly predict next-state observations and action sequences
because of the inherent difference between the two modalities. To address this,
we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework
that handles the modality conflict and enhances the performance of VLAs across
diverse tasks. Specifically, we propose a multimodal diffusion transformer
architecture that explicitly maintains separate modality streams while still
enabling cross-modal knowledge sharing. In addition, we introduce independent
noise perturbations for each modality and a decoupled flow-matching loss. This
design enables the model to learn the joint distribution in a bidirectional
manner while avoiding the need for a unified latent space. Based on the
decoupling of modalities during training, we also introduce a joint sampling
method that supports test-time scaling, where action and vision tokens evolve
asynchronously at different rates. Through experiments on simulated benchmarks
such as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,
while our test-time scaling approach provides an additional 2-5% boost. On
real-world tasks with the Franka Research 3, DUST improves success rates by
13%, confirming its effectiveness beyond simulation. Furthermore, pre-training
on action-free videos from BridgeV2 yields significant transfer gains on
RoboCasa, underscoring DUST&#x27;s potential for large-scale VLA pretraining.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently, augmenting Vision-Language-Action models (VLAs) with world modeling has shown promise in improving robotic policy learning.</div>
</details>
</div>
<div class="card">
<div class="title">SparsePO: Controlling Preference Alignment of LLMs via Sparse Token   Masks</div>
<div class="meta-line">Authors: Fenia Christopoulou, Ronald Cardenas, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2024-10-07T15:01:29+00:00 · Latest: 2025-10-31T16:15:22+00:00</div>
<div class="meta-line">Comments: 27 pages, 9 figures, 5 tables. Accepted to EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.05102v3">Abs</a> · <a href="http://arxiv.org/pdf/2410.05102v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct alignment algorithms have proven an effective step for aligning
language models to human-desired behaviors. Current variants of the Direct
Preference Optimization objective have focused on a strict setting where all
tokens are contributing signals of KL divergence and rewards to the loss
function. However, human preference is not affected equally by each word in a
sequence but is often dependent on specific words or phrases, e.g. existence of
toxic terms leads to non-preferred responses. Based on this observation, we
argue that not all tokens should be weighted equally during PO and propose a
flexible objective termed SparsePO, that aims to automatically learn to weight
the KL divergence and reward corresponding to each token during PO training. We
propose two different variants of weight-masks that can either be derived from
the reference model itself or learned on the fly. Notably, our method induces
sparsity in the learned masks, allowing the model to learn how to best balance
reward and KL divergence contributions at the token level, learning an optimal
level of mask sparsity. Extensive experiments illustrate the effectiveness of
our approach at aligning to preference proxies, including sentiment control,
helpfulness and harmlessness, and summary quality. Our method obtains +10% and
+3% win rate points in summarization and dialogue scenarios, respectively,
without compromising model reasoning or the relevancy and faithfulness of the
summary response.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Direct alignment algorithms have proven an effective step for aligning language models to human-desired behaviors.</div>
</details>
</div>
<div class="card">
<div class="title">HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM   Inference Serving</div>
<div class="meta-line">Authors: Avinash Kumar, Shashank Nag, Jason Clemons, Lizy John, Poulami Das</div>
<div class="meta-line">First: 2025-04-14T21:30:43+00:00 · Latest: 2025-10-31T16:06:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.10724v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.10724v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early-Exit Large Language Models (EE-LLMs) enable high throughput inference
by allowing tokens to exit early at intermediate layers. However, their
throughput is limited by the computational and memory savings. Existing EE-LLM
frameworks rely on a single model and therefore, their token generation
latencies are bottlenecked by tokens that do not exit early and traverse
additional layers. Moreover, early exits are only known at runtime and depend
on the request. Therefore, these frameworks load the weights of all model
layers even though large portions remain unused when tokens exit early. The
lack of memory savings limit us from scaling the batch sizes.
  We propose $\textit{HELIOS}$, a framework that improves both token generation
latency and batch sizes to enable high-throughput in EE-LLMs. HELIOS exploits
two insights. $\textit{First}$, early exits are often complimentary across
models, tokens that do not exit early on one model often take an early-exit on
another. HELIOS employs multiple models and dynamically switches between them
to collectively maximize the number of tokens that exit early, and minimize
token generation latencies. $\textit{Second}$, even when a predicted token does
not exit early due to poor confidence, it often remains unchanged even after
additional layer traversal. HELIOS greedily allows such tokens to exit early
and only loads the weights of the most likely to be used layers, yielding
memory savings which is then re-purposed to increase batch sizes. HELIOS
employs real-time profiling to accurately identify the early-exit
distributions, and adaptively switches between models by tracking tokens in
real-time to minimize the performance degradation caused by greedy model
loading and exiting. Our evaluations show that HELIOS achieves $1.48\times$
higher throughput and $15.14\times$ larger batch size compared to existing
EE-LLM frameworks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Early-Exit Large Language Models (EE-LLMs) enable high throughput inference by allowing tokens to exit early at intermediate layers.</div>
</details>
</div>
<div class="card">
<div class="title">VRoPE: Rotary Position Embedding for Video Large Language Models</div>
<div class="meta-line">Authors: Zikang Liu, Longteng Guo, Yepeng Tang, Tongtian Yue, Junxian Cai, Kai Ma, Qingbin Liu, Xi Chen, Jing Liu</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-02-17T10:53:57+00:00 · Latest: 2025-10-31T15:26:11+00:00</div>
<div class="meta-line">Comments: EMNLP 2025 Main Camera Ready</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.11664v4">Abs</a> · <a href="http://arxiv.org/pdf/2502.11664v4">PDF</a> · <a href="https://github.com/johncaged/VRoPE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rotary Position Embedding (RoPE) has shown strong performance in text-based
Large Language Models (LLMs), but extending it to video remains a challenge due
to the intricate spatiotemporal structure of video frames. Existing
adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions
separately but suffer from two major limitations: positional bias in attention
distribution and disruptions in video-text transitions. To overcome these
issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional
encoding method tailored for Video-LLMs. Specifically, we introduce a more
balanced encoding strategy that mitigates attention biases, ensuring a more
uniform distribution of spatial focus. Additionally, our approach restructures
positional indices to ensure a smooth transition between video and text tokens.
Extensive experiments on different models demonstrate that VRoPE consistently
outperforms previous RoPE variants, achieving significant improvements in video
understanding, temporal reasoning, and retrieval tasks. Code is available at
https://github.com/johncaged/VRoPE.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames.</div>
</details>
</div>
<div class="card">
<div class="title">TetraJet-v2: Accurate NVFP4 Training for Large Language Models with   Oscillation Suppression and Outlier Control</div>
<div class="meta-line">Authors: Yuxiang Chen, Xiaoming Xu, Pengle Zhang, Michael Beyer, Martin Rapp, Jun Zhu, Jianfei Chen</div>
<div class="meta-line">First: 2025-10-31T14:57:16+00:00 · Latest: 2025-10-31T14:57:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27527v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27527v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) training is prohibitively expensive, driving
interest in low-precision fully-quantized training (FQT). While novel 4-bit
formats like NVFP4 offer substantial efficiency gains, achieving near-lossless
training at such low precision remains challenging. We introduce TetraJet-v2,
an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,
and gradients in all linear layers. We identify two critical issues hindering
low-precision LLM training: weight oscillation and outliers. To address these,
we propose: 1) an unbiased double-block quantization method for NVFP4 linear
layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)
OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently
outperforms prior FP4 training methods on pre-training LLMs across varying
model sizes up to 370M and data sizes up to 200B tokens, reducing the
performance gap to full-precision training by an average of 51.3%.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) training is prohibitively expensive, driving interest in low-precision fully-quantized training (FQT).</div>
</details>
</div>
<div class="card">
<div class="title">BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for   Scalable and Efficient Text Summarization</div>
<div class="meta-line">Authors: Desta Haileselassie Hagos, Legand L. Burge, Anietie Andy, Anis Yazidi, Vladimir Vlassov</div>
<div class="meta-line">First: 2025-10-31T14:42:19+00:00 · Latest: 2025-10-31T14:42:19+00:00</div>
<div class="meta-line">Comments: Accepted at the IEEE International Conference on Data Mining (ICDM)
  2025, Washington, DC, USA</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27516v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27516v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based architectures have advanced text summarization, yet their
quadratic complexity limits scalability on long documents. This paper
introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a
novel framework that combines sparse attention, adaptive spans, and bilinear
attention to address these limitations. Sparse attention reduces computational
costs by focusing on the most relevant parts of the input, while adaptive spans
dynamically adjust the attention ranges. Bilinear attention complements both by
modeling complex token interactions within this refined context. BiSparse-AAS
consistently outperforms state-of-the-art baselines in both extractive and
abstractive summarization tasks, achieving average ROUGE improvements of about
68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance
on OpenWebText and Gigaword datasets. By addressing efficiency, scalability,
and long-sequence modeling, BiSparse-AAS provides a unified, practical solution
for real-world text summarization applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformer-based architectures have advanced text summarization, yet their quadratic complexity limits scalability on long documents.</div>
</details>
</div>
<div class="card">
<div class="title">InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames</div>
<div class="meta-line">Authors: Haorui Li, Weitao Du, Yuqiang Li, Hongyu Guo, Shengchao Liu</div>
<div class="meta-line">First: 2025-10-31T14:19:50+00:00 · Latest: 2025-10-31T14:19:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27497v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27497v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based autoregressive models have emerged as a unifying paradigm
across modalities such as text and images, but their extension to 3D molecule
generation remains underexplored. The gap stems from two fundamental
challenges: (1) tokenizing molecules into a canonical 1D sequence of tokens
that is invariant to both SE(3) transformations and atom index permutations,
and (2) designing an architecture capable of modeling hybrid atom-based tokens
that couple discrete atom types with continuous 3D coordinates. To address
these challenges, we introduce InertialAR. InertialAR devises a canonical
tokenization that aligns molecules to their inertial frames and reorders atoms
to ensure SE(3) and permutation invariance. Moreover, InertialAR equips the
attention mechanism with geometric awareness via geometric rotary positional
encoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive
paradigm to predict the next atom-based token, predicting the atom type first
and then its 3D coordinates via Diffusion loss. Experimentally, InertialAR
achieves state-of-the-art performance on 7 of the 10 evaluation metrics for
unconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover,
it significantly outperforms strong baselines in controllable generation for
targeted chemical functionality, attaining state-of-the-art results across all
5 metrics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformer-based autoregressive models have emerged as a unifying paradigm across modalities such as text and images, but their extension to 3D molecule generation remains underexplored.</div>
</details>
</div>
<div class="card">
<div class="title">ESTformer: Transformer utilising spatiotemporal dependencies for   electroencephalogram super-resolution</div>
<div class="meta-line">Authors: Dongdong Li, Zhongliang Zeng, Zhe Wang, Hai Yang</div>
<div class="meta-line">Venue: Knowledge-Based Systems, 317, 113345 (2025)</div>
<div class="meta-line">First: 2023-12-03T12:26:32+00:00 · Latest: 2025-10-31T14:14:31+00:00</div>
<div class="meta-line">Comments: Accepted by Knowledge-Based Systems</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2312.10052v3">Abs</a> · <a href="http://arxiv.org/pdf/2312.10052v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Towards practical applications of Electroencephalography (EEG), lightweight
acquisition devices garner significant attention. However, EEG channel
selection methods are commonly data-sensitive and cannot establish a unified
sound paradigm for EEG acquisition devices. Through reverse conceptualisation,
we formulated EEG applications in an EEG super-resolution (SR) manner, but
suffered from high computation costs, extra interpolation bias, and few
insights into spatiotemporal dependency modelling. To this end, we propose
ESTformer, an EEG SR framework that utilises spatiotemporal dependencies based
on the transformer. ESTformer applies positional encoding methods and a
multihead self-attention mechanism to the space and time dimensions, which can
learn spatial structural correlations and temporal functional variations.
ESTformer, with the fixed mask strategy, adopts a mask token to upsample
low-resolution (LR) EEG data in the case of disturbance from mathematical
interpolation methods. On this basis, we designed various transformer blocks to
construct a spatial interpolation module (SIM) and a temporal reconstruction
module (TRM). Finally, ESTformer cascades the SIM and TRM to capture and model
the spatiotemporal dependencies for EEG SR with fidelity. Extensive
experimental results on two EEG datasets show the effectiveness of ESTformer
against previous state-of-the-art methods, demonstrating the versatility of the
Transformer for EEG SR tasks. The superiority of the SR data was verified in an
EEG-based person identification and emotion recognition task, achieving a 2% to
38% improvement compared with the LR data at different sampling scales.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Towards practical applications of Electroencephalography (EEG), lightweight acquisition devices garner significant attention.</div>
</details>
</div>
<div class="card">
<div class="title">Token Distillation: Attention-aware Input Embeddings For New Tokens</div>
<div class="meta-line">Authors: Konstantin Dobler, Desmond Elliott, Gerard de Melo</div>
<div class="meta-line">First: 2025-05-26T15:35:29+00:00 · Latest: 2025-10-31T13:44:06+00:00</div>
<div class="meta-line">Comments: Additional experiments + clearer method name compared to the May 2025
  version</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.20133v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.20133v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current language models rely on static vocabularies determined at pretraining
time, which can lead to decreased performance and increased computational cost
for domains underrepresented in the original vocabulary. New tokens can be
added to solve this problem, when coupled with a good initialization for their
new embeddings. However, existing embedding initialization methods require
expensive further training or pretraining of additional modules. In this paper,
we propose Token Distillation and show that by distilling representations
obtained using the original tokenization, we can quickly learn high-quality
input embeddings for new tokens. Experimental results with a wide range of
open-weight models show that Token Distillation outperforms even strong
baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current language models rely on static vocabularies determined at pretraining time, which can lead to decreased performance and increased computational cost for domains underrepresented in the original vocabulary.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents</div>
<div class="meta-line">Authors: Thassilo M. Schiepanski, Nicholas Piël</div>
<div class="meta-line">First: 2025-08-06T12:56:54+00:00 · Latest: 2025-10-31T13:21:04+00:00</div>
<div class="meta-line">Comments: 20 pages, LaTeX; repository URL updated, typos corrected</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.04412v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.04412v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation - referred to as
snapshot. State-of-the-art web agents are premised on grounded GUI snapshots,
i.e., screenshots enhanced with visual cues. Not least to resemble human
perception, but for images representing relatively cheap means of model input.
LLM vision still lag behind code interpretation capabilities. DOM snapshots,
which structurally resemble HTML, impose a desired alternative. Vast model
input token size, however, disables reliable implementation with web agents to
date. We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based
on a GPT-4o backend, we evaluate D2Snap on tasks sampled from the
Online-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots
(67%) matches a grounded GUI snapshot baseline (65%) - within the same input
token order of magnitude (1e3). Our best evaluated configurations - one token
order above, but within the model&#x27;s context window - outperform this baseline
by 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a
strong UI feature for LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Frontier LLMs only recently enabled serviceable, autonomous web agents.</div>
</details>
</div>
<div class="card">
<div class="title">VCORE: Variance-Controlled Optimization-based Reweighting for   Chain-of-Thought Supervision</div>
<div class="meta-line">Authors: Xuan Gong, Senmiao Wang, Hanbo Huang, Ruoyu Sun, Shiyu Liang</div>
<div class="meta-line">First: 2025-10-31T13:19:24+00:00 · Latest: 2025-10-31T13:19:24+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27462v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27462v1">PDF</a> · <a href="https://github.com/coder-gx/VCORE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has
emerged as a crucial technique for enhancing the reasoning abilities of large
language models (LLMs). However, the standard cross-entropy loss treats all
tokens equally, ignoring their heterogeneous contributions across a reasoning
trajectory. This uniform treatment leads to misallocated supervision and weak
generalization, especially in complex, long-form reasoning tasks. To address
this, we introduce \textbf{V}ariance-\textbf{C}ontrolled
\textbf{O}ptimization-based \textbf{RE}weighting (VCORE), a principled
framework that reformulates CoT supervision as a constrained optimization
problem. By adopting an optimization-theoretic perspective, VCORE enables a
principled and adaptive allocation of supervision across tokens, thereby
aligning the training objective more closely with the goal of robust reasoning
generalization. Empirical evaluations demonstrate that VCORE consistently
outperforms existing token reweighting methods. Across both in-domain and
out-of-domain settings, VCORE achieves substantial performance gains on
mathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,
32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more
effective initialization for subsequent reinforcement learning, establishing a
stronger foundation for advancing the reasoning capabilities of LLMs. The Code
will be released at https://github.com/coder-gx/VCORE.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has emerged as a crucial technique for enhancing the reasoning abilities of large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">CoMViT: An Efficient Vision Backbone for Supervised Classification in   Medical Imaging</div>
<div class="meta-line">Authors: Aon Safdar, Mohamed Saadeldin</div>
<div class="meta-line">Venue: MICCAI 2025</div>
<div class="meta-line">First: 2025-10-31T12:49:13+00:00 · Latest: 2025-10-31T12:49:13+00:00</div>
<div class="meta-line">Comments: Preprint (submitted manuscript). Accepted at the MICCAI 2025 MIRASOL
  Workshop; to appear in the Springer proceedings volume. This is the
  pre-review version (not the Version of Record). DOI will be added after
  publication. [Optional: 8 pages, 4 figures, 4 tables.]</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27442v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers (ViTs) have demonstrated strong potential in medical
imaging; however, their high computational demands and tendency to overfit on
small datasets limit their applicability in real-world clinical scenarios. In
this paper, we present CoMViT, a compact and generalizable Vision Transformer
architecture optimized for resource-constrained medical image analysis. CoMViT
integrates a convolutional tokenizer, diagonal masking, dynamic temperature
scaling, and pooling-based sequence aggregation to improve performance and
generalization. Through systematic architectural optimization, CoMViT achieves
robust performance across twelve MedMNIST datasets while maintaining a
lightweight design with only ~4.5M parameters. It matches or outperforms deeper
CNN and ViT variants, offering up to 5-20x parameter reduction without
sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT
consistently attends to clinically relevant regions despite its compact size.
These results highlight the potential of principled ViT redesign for developing
efficient and interpretable models in low-resource medical imaging settings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision Transformers (ViTs) have demonstrated strong potential in medical imaging; however, their high computational demands and tendency to overfit on small datasets limit their applicability in real-world clinical scenarios.</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Semantic Collapse in Partially Relevant Video Retrieval</div>
<div class="meta-line">Authors: WonJun Moon, MinSeok Jung, Gilhan Park, Tae-Young Kim, Cheol-Ho Cho, Woojin Jun, Jae-Pil Heo</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-31T12:39:20+00:00 · Latest: 2025-10-31T12:39:20+00:00</div>
<div class="meta-line">Comments: Accpeted to NeurIPS 2025. Code is available at
  https://github.com/admins97/MSC_PRVR</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27432v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27432v1">PDF</a> · <a href="https://github.com/admins97/MSC_PRVR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the
content matches a text query. Existing methods treat every annotated text-video
pair as a positive and all others as negatives, ignoring the rich semantic
variation both within a single video and across different videos. Consequently,
embeddings of both queries and their corresponding video-clip segments for
distinct events within the same video collapse together, while embeddings of
semantically similar queries and segments from different videos are driven
apart. This limits retrieval performance when videos contain multiple, diverse
events. This paper addresses the aforementioned problems, termed as semantic
collapse, in both the text and video embedding spaces. We first introduce Text
Correlation Preservation Learning, which preserves the semantic relationships
encoded by the foundation model across text queries. To address collapse in
video embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastive
alignment method that disentangles hierarchical video representations across
temporal scales. Subsequently, we introduce order-preserving token merging and
adaptive CBVA to enhance alignment by producing video segments that are
internally coherent yet mutually distinctive. Extensive experiments on PRVR
benchmarks demonstrate that our framework effectively prevents semantic
collapse and substantially improves retrieval accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Partially Relevant Video Retrieval (PRVR) seeks videos where only part of the content matches a text query.</div>
</details>
</div>
<div class="card">
<div class="title">DeepCompress: A Dual Reward Strategy for Dynamically Exploring and   Compressing Reasoning Chains</div>
<div class="meta-line">Authors: Tian Liang, Wenxiang Jiao, Zhiwei He, Jiahao Xu, Haitao Mi, Dong Yu</div>
<div class="meta-line">First: 2025-10-31T12:13:11+00:00 · Latest: 2025-10-31T12:13:11+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27419v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27419v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have demonstrated impressive capabilities but
suffer from cognitive inefficiencies like ``overthinking&#x27;&#x27; simple problems and
``underthinking&#x27;&#x27; complex ones. While existing methods that use supervised
fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can
improve efficiency, they often do so at the cost of accuracy. This paper
introduces \textbf{DeepCompress}, a novel framework that simultaneously
enhances both the accuracy and efficiency of LRMs. We challenge the prevailing
approach of consistently favoring shorter reasoning paths, showing that longer
responses can contain a broader range of correct solutions for difficult
problems. DeepCompress employs an adaptive length reward mechanism that
dynamically classifies problems as ``Simple&#x27;&#x27; or ``Hard&#x27;&#x27; in real-time based on
the model&#x27;s evolving capability. It encourages shorter, more efficient
reasoning for ``Simple&#x27;&#x27; problems while promoting longer, more exploratory
thought chains for ``Hard&#x27;&#x27; problems. This dual-reward strategy enables the
model to autonomously adjust its Chain-of-Thought (CoT) length, compressing
reasoning for well-mastered problems and extending it for those it finds
challenging. Experimental results on challenging mathematical benchmarks show
that DeepCompress consistently outperforms baseline methods, achieving superior
accuracy while significantly improving token efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like ``overthinking&#x27;&#x27; simple problems and ``underthinking&#x27;&#x27; complex ones.</div>
</details>
</div>
<div class="card">
<div class="title">Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds</div>
<div class="meta-line">Authors: Wu Wei, Xiaomeng Fan, Yuwei Wu, Zhi Gao, Pengxiang Li, Yunde Jia, Mehrtash Harandi</div>
<div class="meta-line">First: 2025-10-31T11:32:15+00:00 · Latest: 2025-10-31T11:32:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27391v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27391v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modality alignment is critical for vision-language models (VLMs) to
effectively integrate information across modalities. However, existing methods
extract hierarchical features from text while representing each image with a
single feature, leading to asymmetric and suboptimal alignment. To address
this, we propose Alignment across Trees, a method that constructs and aligns
tree-like hierarchical features for both image and text modalities.
Specifically, we introduce a semantic-aware visual feature extraction framework
that applies a cross-attention mechanism to visual class tokens from
intermediate Transformer layers, guided by textual cues to extract visual
features with coarse-to-fine semantics. We then embed the feature trees of the
two modalities into hyperbolic manifolds with distinct curvatures to
effectively model their hierarchical structures. To align across the
heterogeneous hyperbolic manifolds with different curvatures, we formulate a KL
distance measure between distributions on heterogeneous manifolds, and learn an
intermediate manifold for manifold alignment by minimizing the distance. We
prove the existence and uniqueness of the optimal intermediate manifold.
Experiments on taxonomic open-set classification tasks across multiple image
datasets demonstrate that our method consistently outperforms strong baselines
under few-shot and cross-domain settings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modality alignment is critical for vision-language models (VLMs) to effectively integrate information across modalities.</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Models Sometimes Output Illegible Chains of Thought</div>
<div class="meta-line">Authors: Arun Jose</div>
<div class="meta-line">First: 2025-10-31T10:16:35+00:00 · Latest: 2025-10-31T10:16:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27338v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27338v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models trained via outcome-based reinforcement learning (RL) to
reason using chain-of-thought (CoT) have shown remarkable performance.
Monitoring such a model&#x27;s CoT may allow us to understand its intentions and
detect potential malicious behavior. However, to be effective, this requires
that CoTs are legible and faithful. We study CoT legibility across 14 reasoning
models, finding that RL often causes reasoning to become illegible to both
humans and AI monitors, with reasoning models (except Claude) generating
illegible CoTs while returning to perfectly readable final answers. We show
that models use illegible reasoning to reach correct answers (accuracy dropping
by 53\% when forced to use only legible portions), yet find no correlation
between legibility and performance when resampling - suggesting the
relationship is more nuanced. We also find that legibility degrades on harder
questions. We discuss potential hypotheses for these results, including
steganography, training artifacts, and vestigial tokens. These results suggest
that without explicit optimization for legibility, outcome-based RL naturally
produces models with increasingly opaque reasoning processes, potentially
undermining monitoring approaches.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance.</div>
</details>
</div>
<div class="card">
<div class="title">FOCUS: Efficient Keyframe Selection for Long Video Understanding</div>
<div class="meta-line">Authors: Zirui Zhu, Hailun Xu, Yang Luo, Yong Liu, Kanchan Sarkar, Zhenheng Yang, Yang You</div>
<div class="meta-line">First: 2025-10-31T08:41:13+00:00 · Latest: 2025-10-31T08:41:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27280v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27280v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) represent images and video frames as
visual tokens. Scaling from single images to hour-long videos, however,
inflates the token budget far beyond practical limits. Popular pipelines
therefore either uniformly subsample or apply keyframe selection with
retrieval-style scoring using smaller vision-language models. However, these
keyframe selection methods still rely on pre-filtering before selection to
reduce the inference cost and can miss the most informative moments.
  We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a
training-free, model-agnostic keyframe selection module that selects
query-relevant frames under a strict token budget. FOCUS formulates keyframe
selection as a combinatorial pure-exploration (CPE) problem in multi-armed
bandits: it treats short temporal clips as arms, and uses empirical means and
Bernstein confidence radius to identify informative regions while preserving
exploration of uncertain areas. The resulting two-stage
exploration-exploitation procedure reduces from a sequential policy with
theoretical guarantees, first identifying high-value temporal regions, then
selecting top-scoring frames within each region On two long-video
question-answering benchmarks, FOCUS delivers substantial accuracy improvements
while processing less than 2% of video frames. For videos longer than 20
minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating
its effectiveness as a keyframe selection method and providing a simple and
general solution for scalable long-video understanding with MLLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) represent images and video frames as visual tokens.</div>
</details>
</div>
<div class="card">
<div class="title">LLM Based Long Code Translation using Identifier Replacement</div>
<div class="meta-line">Authors: Manojit Chakraborty, Madhusudan Ghosh, Rishabh Gupta</div>
<div class="meta-line">First: 2025-10-10T06:28:15+00:00 · Latest: 2025-10-31T08:20:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09045v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.09045v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don&#x27;t fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In the domain of software development, LLMs have been utilized to automate tasks such as code translation, where source code from one programming language is translated to another while preserving its functionality.</div>
</details>
</div>
<div class="card">
<div class="title">RegionRAG: Region-level Retrieval-Augumented Generation for   Visually-Rich Documents</div>
<div class="meta-line">Authors: Yinglu Li, Zhiying Lu, Zhihang Liu, Chuanbin Liu, Hongtao Xie</div>
<div class="meta-line">First: 2025-10-31T08:00:32+00:00 · Latest: 2025-10-31T08:00:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27261v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27261v1">PDF</a> · <a href="https://github.com/Aeryn666/RegionRAG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method
for empowering LLMs by leveraging candidate visual documents. However, current
methods consider the entire document as the basic retrieval unit, introducing
substantial irrelevant visual content in two ways: 1) Relevant documents often
contain large regions unrelated to the query, diluting the focus on salient
information; 2) Retrieving multiple documents to increase recall further
introduces redundant and irrelevant documents. These redundant contexts
distract the model&#x27;s attention and further degrade the performance. To address
this challenge, we propose \modelname, a novel framework that shifts the
retrieval paradigm from the document level to the region level. During
training, we design a hybrid supervision strategy from both labeled data and
unlabeled data to pinpoint relevant patches. During inference, we propose a
dynamic pipeline that intelligently groups salient patches into complete
semantic regions. By delegating the task of identifying relevant regions to the
retriever, \modelname enables the generator to focus solely on concise visual
content relevant to queries, improving both efficiency and accuracy.
Experiments on six benchmarks demonstrate that RegionRAG achieves
state-of-the-art performance. Improves retrieval accuracy by 10.02\% in R@1 on
average and increases question answering accuracy by 3.56\% while using only
71.42\% visual tokens compared to prior methods. The code will be available at
https://github.com/Aeryn666/RegionRAG.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method for empowering LLMs by leveraging candidate visual documents.</div>
</details>
</div>
<div class="card">
<div class="title">Higher-order Linear Attention</div>
<div class="meta-line">Authors: Yifan Zhang, Zhen Qin, Quanquan Gu</div>
<div class="meta-line">First: 2025-10-31T07:54:37+00:00 · Latest: 2025-10-31T07:54:37+00:00</div>
<div class="meta-line">Comments: Project Page: https://github.com/yifanzhang-pro/HLA</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27258v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27258v1">PDF</a> · <a href="https://github.com/yifanzhang-pro/HLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The quadratic cost of scaled dot-product attention is a central obstacle to
scaling autoregressive language models to long contexts. Linear-time attention
and State Space Models (SSMs) provide scalable alternatives but are typically
restricted to first-order or kernel-based approximations, which can limit
expressivity. We introduce Higher-order Linear Attention (HLA), a causal,
streaming mechanism that realizes higher interactions via compact prefix
sufficient statistics. In the second-order case, HLA maintains a constant-size
state and computes per-token outputs in linear time without materializing any
$n \times n$ matrices. We give closed-form streaming identities, a strictly
causal masked variant using two additional summaries, and a chunk-parallel
training scheme based on associative scans that reproduces the activations of a
serial recurrence exactly. We further outline extensions to third and higher
orders. Collectively, these results position HLA as a principled, scalable
building block that combines attention-like, data-dependent mixing with the
efficiency of modern recurrent architectures. Project Page:
https://github.com/yifanzhang-pro/HLA.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts.</div>
</details>
</div>
<div class="card">
<div class="title">Languages are Modalities: Cross-Lingual Alignment via Encoder Injection</div>
<div class="meta-line">Authors: Rajan Agarwal, Aarush Gupta</div>
<div class="meta-line">First: 2025-10-31T07:43:21+00:00 · Latest: 2025-10-31T07:43:21+00:00</div>
<div class="meta-line">Comments: 14 pages, 3 Figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27254v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27254v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instruction-tuned Large Language Models (LLMs) underperform on low resource,
non-Latin scripts due to tokenizer fragmentation and weak cross-lingual
coupling. We present LLINK (Latent Language Injection for Non-English
Knowledge), a compute efficient language-as-modality method that conditions an
instruction-tuned decoder without changing the tokenizer or retraining the
decoder. First, we align sentence embeddings from a frozen multilingual encoder
to the decoder&#x27;s latent embedding space at a reserved position via a
lightweight contrastive projector. Second, the vector is expanded into K soft
slots and trained with minimal adapters so the frozen decoder consumes the
signal. LLINK substantially improves bilingual retrieval and achieves 81.3%
preference over the base model and 63.6% over direct fine-tuning in LLM-judged
Q&amp;A evaluations. We further find that improvements can be attributed to reduced
tokenization inflation and a stronger cross lingual alignment, despite the
model having residual weaknesses in numeric fidelity. Treating low resource
languages as a modality offers a practical path to stronger cross-lingual
alignment in lightweight LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Instruction-tuned Large Language Models (LLMs) underperform on low resource, non-Latin scripts due to tokenizer fragmentation and weak cross-lingual coupling.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in   LLMs</div>
<div class="meta-line">Authors: Mohammad Tavakoli, Alireza Salemi, Carrie Ye, Mohamed Abdalla, Hamed Zamani, J Ross Mitchell</div>
<div class="meta-line">First: 2025-10-31T07:29:52+00:00 · Latest: 2025-10-31T07:29:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27246v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27246v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the abilities of large language models (LLMs) for tasks that
require long-term memory and thus long-context reasoning, for example in
conversational settings, is hampered by the existing benchmarks, which often
lack narrative coherence, cover narrow domains, and only test simple
recall-oriented tasks. This paper introduces a comprehensive solution to these
challenges. First, we present a novel framework for automatically generating
long (up to 10M tokens), coherent, and topically diverse conversations,
accompanied by probing questions targeting a wide range of memory abilities.
From this, we construct BEAM, a new benchmark comprising 100 conversations and
2,000 validated questions. Second, to enhance model performance, we propose
LIGHT-a framework inspired by human cognition that equips LLMs with three
complementary memory systems: a long-term episodic memory, a short-term working
memory, and a scratchpad for accumulating salient facts. Our experiments on
BEAM reveal that even LLMs with 1M token context windows (with and without
retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT
consistently improves performance across various models, achieving an average
improvement of 3.5%-12.69% over the strongest baselines, depending on the
backbone LLM. An ablation study further confirms the contribution of each
memory component.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Evaluating the abilities of large language models (LLMs) for tasks that require long-term memory and thus long-context reasoning, for example in conversational settings, is hampered by the existing benchmarks, which often lack narrative coherence, cover narrow domains, and only test simple recall-oriented tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in   Transformers</div>
<div class="meta-line">Authors: Zhexiang Li, Haoyu Wang, Yutong Bao, David Woodruff</div>
<div class="meta-line">First: 2025-05-16T09:35:11+00:00 · Latest: 2025-10-31T06:09:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.11040v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.11040v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in transformer architectures deeply enhanced long-context
language modeling. Among them, HyperAttention achieves competitive efficiency
by combining a single-level LSH-based clustering with uniform residual
sampling. However, HyperAttention fails to find all significant keys, which in
turn raises the overall perplexity. We propose a pre-scoring mechanism that
prioritizes significant keys before applying HyperAttention. We introduce three
scoring methods: $k$-means and kernel $k$-means clustering, $k$-median
clustering, and leverage score-based ranking (inspired by LevAttention) to
filter keys effectively. We further replace HyperAttention&#x27;s original uniform
residual sampling, relying exclusively on our pre-scoring mechanism.
Experiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3,
which outperforms standard HyperAttention. Moreover, when running on the
Vision-Transformer (ViT), our method shows that it can guarantee similar
accuracy compared with LevAttention, and will surpass LevAttention given
specific parameters. Although this method introduces some computational
overhead, its combination with HyperAttention achieves up to 20 times faster
than FlashAttention, providing a balanced trade-off between speed and modeling
accuracy. Our results highlight the effectiveness of integrating pre-scoring
into hierarchical attention mechanisms, significantly improving transformer
efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in transformer architectures deeply enhanced long-context language modeling.</div>
</details>
</div>
<div class="card">
<div class="title">Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in   Masked Diffusion</div>
<div class="meta-line">Authors: Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji</div>
<div class="meta-line">First: 2025-10-06T06:30:22+00:00 · Latest: 2025-10-31T05:31:58+00:00</div>
<div class="meta-line">Comments: 23 pages, fixed cleveref-related issue</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.04525v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.04525v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the &quot;moment sampler,&quot; an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a &quot;choose-then-sample&quot; approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Masked diffusion models have shown promising performance in generating high-quality samples in a wide range of domains, but accelerating their sampling process remains relatively underexplored.</div>
</details>
</div>
<div class="card">
<div class="title">E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast   Image Synthesis under Limited Resources</div>
<div class="meta-line">Authors: Tong Shen, Jingai Yu, Dong Zhou, Dong Li, Emad Barsoum</div>
<div class="meta-line">First: 2025-10-31T03:13:08+00:00 · Latest: 2025-10-31T03:13:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27135v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27135v1">PDF</a> · <a href="https://github.com/AMD-AGI/Nitro-E">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have shown strong capabilities in generating high-quality
images from text prompts. However, these models often require large-scale
training data and significant computational resources to train, or suffer from
heavy structure with high latency. To this end, we propose Efficient Multimodal
Diffusion Transformer (E-MMDiT), an efficient and lightweight multimodal
diffusion model with only 304M parameters for fast image synthesis requiring
low training resources. We provide an easily reproducible baseline with
competitive results. Our model for 512px generation, trained with only 25M
public data in 1.5 days on a single node of 8 AMD MI300X GPUs, achieves 0.66 on
GenEval and easily reaches to 0.72 with some post-training techniques such as
GRPO. Our design philosophy centers on token reduction as the computational
cost scales significantly with the token count. We adopt a highly compressive
visual tokenizer to produce a more compact representation and propose a novel
multi-path compression module for further compression of tokens. To enhance our
design, we introduce Position Reinforcement, which strengthens positional
information to maintain spatial coherence, and Alternating Subregion Attention
(ASA), which performs attention within subregions to further reduce
computational cost. In addition, we propose AdaLN-affine, an efficient
lightweight module for computing modulation parameters in transformer blocks.
Our code is available at https://github.com/AMD-AGI/Nitro-E and we hope E-MMDiT
serves as a strong and practical baseline for future research and contributes
to democratization of generative AI models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion models have shown strong capabilities in generating high-quality images from text prompts.</div>
</details>
</div>
<div class="card">
<div class="title">Functional embeddings enable Aggregation of multi-area SEEG recordings   over subjects and sessions</div>
<div class="meta-line">Authors: Sina Javadzadeh, Rahil Soroushmojdehi, S. Alireza Seyyed Mousavi, Mehrnaz Asadi, Sumiko Abe, Terence D. Sanger</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-31T01:23:05+00:00 · Latest: 2025-10-31T01:23:05+00:00</div>
<div class="meta-line">Comments: Submitted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27090v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aggregating intracranial recordings across subjects is challenging since
electrode count, placement, and covered regions vary widely. Spatial
normalization methods like MNI coordinates offer a shared anatomical reference,
but often fail to capture true functional similarity, particularly when
localization is imprecise; even at matched anatomical coordinates, the targeted
brain region and underlying neural dynamics can differ substantially between
individuals. We propose a scalable representation-learning framework that (i)
learns a subject-agnostic functional identity for each electrode from
multi-region local field potentials using a Siamese encoder with contrastive
objectives, inducing an embedding geometry that is locality-sensitive to
region-specific neural signatures, and (ii) tokenizes these embeddings for a
transformer that models inter-regional relationships with a variable number of
channels. We evaluate this framework on a 20-subject dataset spanning basal
ganglia-thalamic regions collected during flexible rest/movement recording
sessions with heterogeneous electrode layouts. The learned functional space
supports accurate within-subject discrimination and forms clear,
region-consistent clusters; it transfers zero-shot to unseen channels. The
transformer, operating on functional tokens without subject-specific heads or
supervision, captures cross-region dependencies and enables reconstruction of
masked channels, providing a subject-agnostic backbone for downstream decoding.
Together, these results indicate a path toward large-scale, cross-subject
aggregation and pretraining for intracranial neural data where strict task
structure and uniform sensor placement are unavailable.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Aggregating intracranial recordings across subjects is challenging since electrode count, placement, and covered regions vary widely.</div>
</details>
</div>
<div class="card">
<div class="title">Towards Understanding Self-play for LLM Reasoning</div>
<div class="meta-line">Authors: Justin Yang Chae, Md Tanvirul Alam, Nidhi Rastogi</div>
<div class="meta-line">First: 2025-10-31T00:41:37+00:00 · Latest: 2025-10-31T00:41:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27072v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.27072v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language model (LLM) reasoning, led by reinforcement
learning with verifiable rewards (RLVR), have inspired self-play post-training,
where models improve by generating and solving their own problems. While
self-play has shown strong in-domain and out-of-domain gains, the mechanisms
behind these improvements remain poorly understood. In this work, we analyze
the training dynamics of self-play through the lens of the Absolute Zero
Reasoner, comparing it against RLVR and supervised fine-tuning (SFT). Our study
examines parameter update sparsity, entropy dynamics of token distributions,
and alternative proposer reward functions. We further connect these dynamics to
reasoning performance using pass@k evaluations. Together, our findings clarify
how self-play differs from other post-training strategies, highlight its
inherent limitations, and point toward future directions for improving LLM math
reasoning through self-play.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in large language model (LLM) reasoning, led by reinforcement learning with verifiable rewards (RLVR), have inspired self-play post-training, where models improve by generating and solving their own problems.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251103_0313.html">20251103_0313</a>
<a href="archive/20251102_0313.html">20251102_0313</a>
<a href="archive/20251101_0314.html">20251101_0314</a>
<a href="archive/20251031_0314.html">20251031_0314</a>
<a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
