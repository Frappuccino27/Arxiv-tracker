<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-07 03:19</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251107_0319</div>
    <div class="row"><div class="card">
<div class="title">Whisper Leak: a side-channel attack on Large Language Models</div>
<div class="meta-line">Authors: Geoff McDonald, Jonathan Bar Or</div>
<div class="meta-line">First: 2025-11-05T17:47:46+00:00 · Latest: 2025-11-05T17:47:46+00:00</div>
<div class="meta-line">Comments: 14 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03675v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly deployed in sensitive domains
including healthcare, legal services, and confidential communications, where
privacy is paramount. This paper introduces Whisper Leak, a side-channel attack
that infers user prompt topics from encrypted LLM traffic by analyzing packet
size and timing patterns in streaming responses. Despite TLS encryption
protecting content, these metadata patterns leak sufficient information to
enable topic classification. We demonstrate the attack across 28 popular LLMs
from major providers, achieving near-perfect classification (often &gt;98% AUPRC)
and high precision even at extreme class imbalance (10,000:1 noise-to-target
ratio). For many models, we achieve 100% precision in identifying sensitive
topics like &quot;money laundering&quot; while recovering 5-20% of target conversations.
This industry-wide vulnerability poses significant risks for users under
network surveillance by ISPs, governments, or local adversaries. We evaluate
three mitigation strategies - random padding, token batching, and packet
injection - finding that while each reduces attack effectiveness, none provides
complete protection. Through responsible disclosure, we have collaborated with
providers to implement initial countermeasures. Our findings underscore the
need for LLM providers to address metadata leakage as AI systems handle
increasingly sensitive information.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) are increasingly deployed in sensitive domains including healthcare, legal services, and confidential communications, where privacy is paramount.</div>
</details>
</div>
<div class="card">
<div class="title">HAFixAgent: History-Aware Automated Program Repair Agent</div>
<div class="meta-line">Authors: Yu Shi, Hao Li, Bram Adams, Ahmed E. Hassan</div>
<div class="meta-line">First: 2025-11-02T18:45:34+00:00 · Latest: 2025-11-05T17:04:35+00:00</div>
<div class="meta-line">Comments: 31 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.01047v2">Abs</a> · <a href="http://arxiv.org/pdf/2511.01047v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated program repair (APR) has recently shifted toward large language
models and agent-based systems, yet most systems rely on local snapshot
context, overlooking repository history. Prior work shows that repository
history helps repair single-line bugs, since the last commit touching the buggy
line is often the bug-introducing one. In this paper, we investigate whether
repository history can also improve agentic APR systems at scale, especially
for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing
Agent that injects blame-derived repository heuristics into its repair loop. A
preliminary study of all 854 real-world bugs from Defects4J motivates our
design, showing that bug-relevant history is both widely available and highly
concentrated. Empirical comparison of HAFixAgent with two state-of-the-art
baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the
agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)
Efficiency: history does not significantly increase agent steps and keeps token
costs comparable, with notably lower median costs for complex
multi-file-multi-hunk bugs. (3) Practicality: combining different historical
heuristics repairs more bugs, offering a clear cost-benefit trade-off.
HAFixAgent offers a practical recipe for history-aware agentic APR: ground the
agent in version control history, prioritize diff-based historical context, and
integrate complementary heuristics when needed.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Automated program repair (APR) has recently shifted toward large language models and agent-based systems, yet most systems rely on local snapshot context, overlooking repository history.</div>
</details>
</div>
<div class="card">
<div class="title">Watermarking Large Language Models in Europe: Interpreting the AI Act in   Light of Technology</div>
<div class="meta-line">Authors: Thomas Souverain</div>
<div class="meta-line">First: 2025-11-05T17:00:39+00:00 · Latest: 2025-11-05T17:00:39+00:00</div>
<div class="meta-line">Comments: 17 pages, 2 Tables and 2 Pictures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03641v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03641v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To foster trustworthy Artificial Intelligence (AI) within the European Union,
the AI Act requires providers to mark and detect the outputs of their
general-purpose models. The Article 50 and Recital 133 call for marking methods
that are &#x27;&#x27;sufficiently reliable, interoperable, effective and robust&#x27;&#x27;. Yet,
the rapidly evolving and heterogeneous landscape of watermarks for Large
Language Models (LLMs) makes it difficult to determine how these four standards
can be translated into concrete and measurable evaluations. Our paper addresses
this challenge, anchoring the normativity of European requirements in the
multiplicity of watermarking techniques. Introducing clear and distinct
concepts on LLM watermarking, our contribution is threefold. (1) Watermarking
Categorisation: We propose an accessible taxonomy of watermarking methods
according to the stage of the LLM lifecycle at which they are applied - before,
during, or after training, and during next-token distribution or sampling. (2)
Watermarking Evaluation: We interpret the EU AI Act&#x27;s requirements by mapping
each criterion with state-of-the-art evaluations on robustness and
detectability of the watermark, and of quality of the LLM. Since
interoperability remains largely untheorised in LLM watermarking research, we
propose three normative dimensions to frame its assessment. (3) Watermarking
Comparison: We compare current watermarking methods for LLMs against the
operationalised European criteria and show that no approach yet satisfies all
four standards. Encouraged by emerging empirical tests, we recommend further
research into watermarking directly embedded within the low-level architecture
of LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models.</div>
</details>
</div>
<div class="card">
<div class="title">R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large   Model Token Routing</div>
<div class="meta-line">Authors: Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang</div>
<div class="meta-line">First: 2025-05-27T16:57:20+00:00 · Latest: 2025-11-05T16:39:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.21600v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.21600v2">PDF</a> · <a href="https://github.com/thu-nics/R2R">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) achieve impressive reasoning capabilities at the
cost of substantial inference overhead, posing substantial deployment
challenges. Although distilled Small Language Models (SLMs) significantly
enhance efficiency, their performance suffers as they fail to follow LLMs&#x27;
reasoning paths. Luckily, we reveal that only a small fraction of tokens
genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens
are either identical or exhibit neutral differences, such as minor variations
in abbreviations or expressions. Leveraging this insight, we introduce **Roads
to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs
only for these critical, path-divergent tokens, while leaving the majority of
token generation to the SLM. We also develop an automatic data generation
pipeline that identifies divergent tokens and generates token-level routing
labels to train the lightweight router. We apply R2R to combine R1-1.5B and
R1-32B models from the DeepSeek family, and evaluate on challenging math,
coding, and QA benchmarks. With an average activated parameter size of 5.6B,
R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the
R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with
comparable performance, advancing the Pareto frontier of test-time scaling
efficiency. Our code is available at https://github.com/thu-nics/R2R.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges.</div>
</details>
</div>
<div class="card">
<div class="title">TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and   Retrieval</div>
<div class="meta-line">Authors: Günther Schindler, Maximilian Schambach, Michael Medek, Sam Thelin</div>
<div class="meta-line">First: 2025-11-05T15:51:03+00:00 · Latest: 2025-11-05T15:51:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03570v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03570v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study LLMs for tabular prediction with mixed text, numeric, and categorical fields.</div>
</details>
</div>
<div class="card">
<div class="title">Hulu-Med: A Transparent Generalist Model towards Holistic Medical   Vision-Language Understanding</div>
<div class="meta-line">Authors: Songtao Jiang, Yuan Wang, Sibo Song, Tianxiang Hu, Chenyi Zhou, Bin Pu, Yan Zhang, Zhibo Yang, Yang Feng, Joey Tianyi Zhou, Jin Hao, Zijian Chen, Ruijia Wu, Tao Tang, Junhui Lv, Hongxia Xu, Hongwei Wang, Jun Xiao, Bin Feng, Fudong Zhu, Kenli Li, Weidi Xie, Jimeng Sun, Jian Wu, Zuozhu Liu</div>
<div class="meta-line">First: 2025-10-09T17:06:42+00:00 · Latest: 2025-11-05T15:19:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.08668v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.08668v2">PDF</a> · <a href="https://github.com/ZJUI-AI4H/Hulu-Med">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world clinical decision-making requires integrating heterogeneous data,
including medical text, 2D images, 3D volumes, and videos, while existing AI
systems fail to unify all these signals, limiting their utility. In this paper,
we introduce Hulu-Med, a transparent, generalist medical Vision-Language Model
(VLM) designed to unify language-only, 2D/3D vision-language, and video
understanding within a single architecture. Hulu-Med is trained on a curated
corpus of 16.7 million samples, comprising exclusively public or synthetic
data, spanning 12 major anatomical systems and 14 medical imaging modalities.
Hulu-Med employs a medical-aware token-reduction strategy that prunes redundant
visual tokens, achieving up to a 55% reduction for 3D and video inputs,
improving cross-modal efficiency, and enabling training at 7B-32B parameter
scales in approximately 4,000-40,000 GPU hours. Across 30 public in-domain and
out-of-domain medical benchmarks-covering text reasoning, visual question
answering, report generation, multilingual dialogue, video understanding, and
rare disease diagnosis-Hulu-Med surpasses existing open-source models on 27 of
30 benchmarks and outperforms proprietary systems such as GPT-4o on 16
benchmarks. Despite being a VLM, Hulu-Med outperforms GPT-4o and matches GPT-o1
on the text-only HealthBench. For the first time in the community, we provide a
fully transparent, reproducible and cost-effective pipeline for holistic
medical vision-language understanding by releasing our end-to-end data
curation, training procedures, and model parameters. Code and models are
available at https://github.com/ZJUI-AI4H/Hulu-Med.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Real-world clinical decision-making requires integrating heterogeneous data, including medical text, 2D images, 3D volumes, and videos, while existing AI systems fail to unify all these signals, limiting their utility.</div>
</details>
</div>
<div class="card">
<div class="title">Traversal Verification for Speculative Tree Decoding</div>
<div class="meta-line">Authors: Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, Zhongchao Shi</div>
<div class="meta-line">Venue: NeurIPS 2025 poster</div>
<div class="meta-line">First: 2025-05-18T12:51:55+00:00 · Latest: 2025-11-05T13:59:39+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 poster</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.12398v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.12398v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speculative decoding is a promising approach for accelerating large language
models. The primary idea is to use a lightweight draft model to speculate the
output of the target model for multiple subsequent timesteps, and then verify
them in parallel to determine whether the drafted tokens should be accepted or
rejected. To enhance acceptance rates, existing frameworks typically construct
token trees containing multiple candidates in each timestep. However, their
reliance on token-level verification mechanisms introduces two critical
limitations: First, the probability distribution of a sequence differs from
that of individual tokens, leading to suboptimal acceptance length. Second,
current verification schemes begin from the root node and proceed layer by
layer in a top-down manner. Once a parent node is rejected, all its child nodes
should be discarded, resulting in inefficient utilization of speculative
candidates. This paper introduces Traversal Verification, a novel speculative
decoding algorithm that fundamentally rethinks the verification paradigm
through leaf-to-root traversal. Our approach considers the acceptance of the
entire token sequence from the current node to the root, and preserves
potentially valid subsequences that would be prematurely discarded by existing
methods. We theoretically prove that the probability distribution obtained
through Traversal Verification is identical to that of the target model,
guaranteeing lossless inference while achieving substantial acceleration gains.
Experimental results across different large language models and multiple tasks
show that our method consistently improves acceptance length and throughput
over existing methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Speculative decoding is a promising approach for accelerating large language models.</div>
</details>
</div>
<div class="card">
<div class="title">Towards 1000-fold Electron Microscopy Image Compression for Connectomics   via VQ-VAE with Transformer Prior</div>
<div class="meta-line">Authors: Fuming Yang, Yicong Li, Hanspeter Pfister, Jeff W. Lichtman, Yaron Meirovitch</div>
<div class="meta-line">First: 2025-10-31T20:05:21+00:00 · Latest: 2025-11-05T13:15:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.00231v2">Abs</a> · <a href="http://arxiv.org/pdf/2511.00231v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Petascale electron microscopy (EM) datasets push storage, transfer, and downstream analysis toward their current limits.</div>
</details>
</div>
<div class="card">
<div class="title">CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the   Biomedical Field</div>
<div class="meta-line">Authors: Doria Bonzi, Alexandre Guiggi, Frédéric Béchet, Carlos Ramisch, Benoit Favre</div>
<div class="meta-line">First: 2025-11-05T13:02:06+00:00 · Latest: 2025-11-05T13:02:06+00:00</div>
<div class="meta-line">Comments: Preprint submitted to LREC 2026 (under review) To access the dataset,
  see https://github.com/bonzid/CareMedEval</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03441v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03441v1">PDF</a> · <a href="https://github.com/bonzid/CareMedEval">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Critical appraisal of scientific literature is an essential skill in the biomedical field.</div>
</details>
</div>
<div class="card">
<div class="title">Seeing What You Say: Expressive Image Generation from Speech</div>
<div class="meta-line">Authors: Jiyoung Lee, Song Park, Sanghyuk Chun, Soo-Whan Chung</div>
<div class="meta-line">First: 2025-11-05T12:40:28+00:00 · Latest: 2025-11-05T12:40:28+00:00</div>
<div class="meta-line">Comments: In progress</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03423v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03423v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes VoxStudio, the first unified and end-to-end
speech-to-image model that generates expressive images directly from spoken
descriptions by jointly aligning linguistic and paralinguistic information. At
its core is a speech information bottleneck (SIB) module, which compresses raw
speech into compact semantic tokens, preserving prosody and emotional nuance.
By operating directly on these tokens, VoxStudio eliminates the need for an
additional speech-to-text system, which often ignores the hidden details beyond
text, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired
emotional speech-image dataset built via an advanced TTS engine to affordably
generate richly expressive utterances. Comprehensive experiments on the
SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility
of our method and highlight key challenges, including emotional consistency and
linguistic ambiguity, paving the way for future research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes VoxStudio, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information.</div>
</details>
</div>
<div class="card">
<div class="title">Sundial: A Family of Highly Capable Time Series Foundation Models</div>
<div class="meta-line">Authors: Yong Liu, Guo Qin, Zhiyuan Shi, Zhi Chen, Caiyin Yang, Xiangdong Huang, Jianmin Wang, Mingsheng Long</div>
<div class="meta-line">First: 2025-02-02T14:52:50+00:00 · Latest: 2025-11-05T11:26:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.00816v3">Abs</a> · <a href="http://arxiv.org/pdf/2502.00816v3">PDF</a> · <a href="https://github.com/thuml/Sundial">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Sundial, a family of native, flexible, and scalable time series
foundation models. To predict the next-patch&#x27;s distribution, we propose a
TimeFlow Loss based on flow-matching, which facilitates native pre-training of
Transformers on continuous-valued time series without discrete tokenization.
Conditioned on arbitrary-length time series, our models are pre-trained without
specifying any prior distribution and can generate multiple probable
predictions, achieving more flexibility in representation learning than using
parametric densities. Towards time series foundation models, we leverage
minimal but crucial adaptations of Transformers and curate TimeBench with one
trillion time points, comprising mostly real-world datasets and synthetic data.
By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial
models on TimeBench, which achieve unprecedented model capacity and
generalization performance. In addition to excellent scalability, Sundial
achieves state-of-the-art results on both point and probabilistic forecasting
benchmarks with a just-in-time inference speed, i.e., making zero-shot
predictions within a few milliseconds. We believe that Sundial&#x27;s pioneering
generative forecasting capability can improve model reliability in real-world
decision-making. Code is available at: https://github.com/thuml/Sundial.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce Sundial, a family of native, flexible, and scalable time series foundation models.</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Augmentation Bias in Prompt Learning for Vision-Language   Models</div>
<div class="meta-line">Authors: Gahyeon Kim, Sohee Kim, Seokju Lee</div>
<div class="meta-line">First: 2025-11-05T11:15:16+00:00 · Latest: 2025-11-05T11:15:16+00:00</div>
<div class="meta-line">Comments: Accepted in Pattern Recognition</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03367v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03367v1">PDF</a> · <a href="https://github.com/Gahyeonkim09/AAPL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large-scale vision and language models have led to
significant progress in zero-shot learning tasks. Methods such as CoOp and
CoCoOp have shown that replacing handcrafted prompts with learnable vectors,
known as prompt learning, can result in improved performance. However, these
models often struggle to generalize to entirely unseen categories. While
traditional zero-shot learning techniques benefit from various data
augmentation strategies, prompt learning has primarily focused on text-based
modifications, leaving the potential of image-based augmentation largely
unexplored. In this work, we explore how image-level augmentations,
particularly those that introduce attribute-specific variations, can support
and enhance prompt learning. Our analysis examines the interaction between
these augmentations and soft prompt frameworks, revealing their potential to
improve generalization. We also identify a limitation in existing methods, such
as CoCoOp, which do not provide explicit guidance for learning prompts that
focus on semantically meaningful visual features. To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations. This
decoupling enables the learned prompts to concentrate on visually
discriminative features that align with the target categories. We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings. Our source code is publicly available at:
https://github.com/Gahyeonkim09/AAPL</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Open Source State-Of-the-Art Solution for Romanian Speech Recognition</div>
<div class="meta-line">Authors: Gabriel Pirlogeanu, Alexandru-Lucian Georgescu, Horia Cucu</div>
<div class="meta-line">First: 2025-11-05T11:02:16+00:00 · Latest: 2025-11-05T11:02:16+00:00</div>
<div class="meta-line">Comments: 13th Conference on Speech Technology and Human-Computer Dialogue
  (SpeD 2025), Cluj-Napoca, Romania</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03361v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we present a new state-of-the-art Romanian Automatic Speech
Recognition (ASR) system based on NVIDIA&#x27;s FastConformer architecture--explored
here for the first time in the context of Romanian. We train our model on a
large corpus of, mostly, weakly supervised transcriptions, totaling over 2,600
hours of speech. Leveraging a hybrid decoder with both Connectionist Temporal
Classification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate
a range of decoding strategies including greedy, ALSD, and CTC beam search with
a 6-gram token-level language model. Our system achieves state-of-the-art
performance across all Romanian evaluation benchmarks, including read,
spontaneous, and domain-specific speech, with up to 27% relative WER reduction
compared to previous best-performing systems. In addition to improved
transcription accuracy, our approach demonstrates practical decoding
efficiency, making it suitable for both research and deployment in low-latency
ASR applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this work, we present a new state-of-the-art Romanian Automatic Speech Recognition (ASR) system based on NVIDIA&#x27;s FastConformer architecture--explored here for the first time in the context of Romanian.</div>
</details>
</div>
<div class="card">
<div class="title">Towards Interpretable and Efficient Attention: Compressing All by   Contracting a Few</div>
<div class="meta-line">Authors: Qishuai Wen, Zhiyuan Huang, Chun-Guang Li</div>
<div class="meta-line">First: 2025-09-21T01:57:13+00:00 · Latest: 2025-11-05T10:49:52+00:00</div>
<div class="meta-line">Comments: NeurIPS2025 Spotlight; Code is available at
  https://github.com/QishuaiWen/CBSA</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.16875v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.16875v3">PDF</a> · <a href="https://github.com/QishuaiWen/CBSA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Attention mechanisms have achieved significant empirical success in multiple
fields, but their underlying optimization objectives remain unclear yet.
Moreover, the quadratic complexity of self-attention has become increasingly
prohibitive. Although interpretability and efficiency are two mutually
reinforcing pursuits, prior work typically investigates them separately. In
this paper, we propose a unified optimization objective that derives inherently
interpretable and efficient attention mechanisms through algorithm unrolling.
Precisely, we construct a gradient step of the proposed objective with a set of
forward-pass operations of our \emph{Contract-and-Broadcast Self-Attention}
(CBSA), which compresses input tokens towards low-dimensional structures by
contracting a few representatives of them. This novel mechanism can not only
scale linearly by fixing the number of representatives, but also covers the
instantiations of varied attention mechanisms when using different sets of
representatives. We conduct extensive experiments to demonstrate comparable
performance and superior advantages over black-box attention mechanisms on
visual tasks. Our work sheds light on the integration of interpretability and
efficiency, as well as the unified formula of attention mechanisms.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Attention mechanisms have achieved significant empirical success in multiple fields, but their underlying optimization objectives remain unclear yet.</div>
</details>
</div>
<div class="card">
<div class="title">REFA: Reference Free Alignment for multi-preference optimization</div>
<div class="meta-line">Authors: Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, Saravan Rajmohan</div>
<div class="meta-line">First: 2024-12-20T22:25:23+00:00 · Latest: 2025-11-05T09:46:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.16378v4">Abs</a> · <a href="http://arxiv.org/pdf/2412.16378v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To mitigate reward hacking from response verbosity, modern preference
optimization methods are increasingly adopting length normalization (e.g.,
SimPO, ORPO, LN-DPO). While effective against this bias, we demonstrate that
length normalization itself introduces a failure mode: the URSLA shortcut. Here
models learn to satisfy the alignment objective by prematurely truncating
low-quality responses rather than learning from their semantic content. To
address this, we introduce REFA, a new alignment framework that proposes
probabilistic control on a structural token that controls termination. Our core
innovation is a new class of regularizers that operate directly on the
probability of the End-of-Sequence (EOS) token, a previously unexploited
control lever. This token-level intervention provides a principled solution to
the URSLA shortcut, ensuring genuine quality improvements. Furthermore, it
unlocks a versatile mechanism for managing the alignment-efficiency tradeoff,
enabling practitioners to fine-tune models that adhere to specific token
budgets. Empirically, REFA achieves a 60.29% win rate and a 52.17%
length-controlled win rate on AlpacaEval2 with Llama-3-8B-Instruct,
demonstrating the power of our token-level control paradigm.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To mitigate reward hacking from response verbosity, modern preference optimization methods are increasingly adopting length normalization (e.g., SimPO, ORPO, LN-DPO).</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Objective Adaptive Rate Limiting in Microservices Using Deep   Reinforcement Learning</div>
<div class="meta-line">Authors: Ning Lyu, Yuxi Wang, Ziyu Cheng, Qingyuan Zhang, Feng Chen</div>
<div class="meta-line">First: 2025-11-05T08:22:42+00:00 · Latest: 2025-11-05T08:22:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03279v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03279v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As cloud computing and microservice architectures become increasingly
prevalent, API rate limiting has emerged as a critical mechanism for ensuring
system stability and service quality. Traditional rate limiting algorithms,
such as token bucket and sliding window, while widely adopted, struggle to
adapt to dynamic traffic patterns and varying system loads. This paper proposes
an adaptive rate limiting strategy based on deep reinforcement learning that
dynamically balances system throughput and service latency. We design a hybrid
architecture combining Deep Q-Network (DQN) and Asynchronous Advantage
Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a
Markov Decision Process. The system continuously monitors microservice states
and learns optimal rate limiting policies through environmental interaction.
Extensive experiments conducted in a Kubernetes cluster environment demonstrate
that our approach achieves 23.7% throughput improvement and 31.4% P99 latency
reduction compared to traditional fixed-threshold strategies under high-load
scenarios. Results from a 90-day production deployment handling 500 million
daily requests validate the practical effectiveness of the proposed method,
with 82% reduction in service degradation incidents and 68% decrease in manual
interventions.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As cloud computing and microservice architectures become increasingly prevalent, API rate limiting has emerged as a critical mechanism for ensuring system stability and service quality.</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Language Models are Super Data Learners</div>
<div class="meta-line">Authors: Jinjie Ni, Qian Liu, Longxu Dou, Chao Du, Zili Wang, Hang Yan, Tianyu Pang, Michael Qizhe Shieh</div>
<div class="meta-line">First: 2025-11-05T08:17:42+00:00 · Latest: 2025-11-05T08:17:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03276v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03276v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves &gt; 56% accuracy on HellaSwag
and &gt; 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs.</div>
</details>
</div>
<div class="card">
<div class="title">Layer Importance for Mathematical Reasoning is Forged in Pre-Training   and Invariant after Post-Training</div>
<div class="meta-line">Authors: Aadim Nepal, Safal Shrestha, Anubhav Shrestha, Minwu Kim, Jalal Naghiyev, Ravid Shwartz-Ziv, Keith Ross</div>
<div class="meta-line">First: 2025-06-27T21:04:55+00:00 · Latest: 2025-11-05T05:15:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.22638v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.22638v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models improve at math after instruction tuning, reinforcement
learning, or knowledge distillation. We ask whether these gains come from major
changes in the transformer layers or from smaller adjustments that keep the
original structure. Using layer-wise ablation on base and trained variants, we
find that math reasoning depends on a few critical layers, which stay important
across all post-training methods. Removing these layers reduces math accuracy
by as much as 80%, whereas factual recall tasks only show relatively smaller
drops. This suggests that specialized layers for mathematical tasks form during
pre-training and remain stable afterward. As measured by Normalized Mutual
Information (NMI), we find that near these critical layers, tokens drift from
their original syntactic clusters toward representations aligned with tokens
less syntactically related but potentially more useful for downstream task.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models improve at math after instruction tuning, reinforcement learning, or knowledge distillation.</div>
</details>
</div>
<div class="card">
<div class="title">SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven   Temporal Cross-Attention</div>
<div class="meta-line">Authors: Shreyas C. Dhake, Jiayuan Huang, Runlong He, Danyal Z. Khan, Evangelos B. Mazomenos, Sophia Bano, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarak I. Hoque</div>
<div class="meta-line">First: 2025-11-05T04:55:11+00:00 · Latest: 2025-11-05T04:55:11+00:00</div>
<div class="meta-line">Comments: 12 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03178v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03178v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Anticipating forthcoming surgical events is vital for real-time assistance in
endonasal transsphenoidal pituitary surgery, where visibility is limited and
workflow changes rapidly. Most visual question answering (VQA) systems reason
on isolated frames with static vision language alignment, providing little
support for forecasting next steps or instrument needs. Existing surgical VQA
datasets likewise center on the current scene rather than the near future. We
introduce PitVQA-Anticipation, the first VQA dataset designed for forward
looking surgical reasoning. It comprises 33.5 hours of operative video and
734,769 question answer pairs built from temporally grouped clips and expert
annotations across four tasks: predicting the future phase, next step, upcoming
instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video
language model that adapts a large language model using a GRU Gated Temporal
Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics,
while an adaptive gate injects visual context into the language stream at the
token level. Parameter efficient fine tuning customizes the language backbone
to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and
EndoVis datasets, surpassing strong image and video based baselines. Ablations
show that temporal recurrence and gated fusion drive most of the gains. A frame
budget study indicates a trade-off: 8 frames maximize fluency, whereas 32
frames slightly reduce BLEU but improve numeric time estimation. By pairing a
temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA
advances surgical VQA from retrospective description to proactive anticipation.
PitVQA-Anticipation offers a comprehensive benchmark for this setting and
highlights the importance of targeted temporal modeling for reliable, future
aware surgical assistance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Anticipating forthcoming surgical events is vital for real-time assistance in endonasal transsphenoidal pituitary surgery, where visibility is limited and workflow changes rapidly.</div>
</details>
</div>
<div class="card">
<div class="title">ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation</div>
<div class="meta-line">Authors: Lingfeng Wang, Hualing Lin, Senda Chen, Tao Wang, Changxu Cheng, Yangyang Zhong, Dong Zheng, Wuyue Zhao</div>
<div class="meta-line">First: 2025-05-22T10:26:51+00:00 · Latest: 2025-11-05T04:24:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.16495v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.16495v2">PDF</a> · <a href="https://github.com/yayafengzi/ALToLLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While humans effortlessly draw visual objects and shapes by adaptively
allocating attention based on their complexity, existing multimodal large
language models (MLLMs) remain constrained by rigid token representations.
Bridging this gap, we propose ALTo, an adaptive length tokenizer for
autoregressive mask generation. To achieve this, a novel token length predictor
is designed, along with a length regularization term and a differentiable token
chunking strategy. We further build ALToLLM that seamlessly integrates ALTo
into MLLM. Preferences on the trade-offs between mask quality and efficiency is
implemented by group relative policy optimization (GRPO). Experiments
demonstrate that ALToLLM achieves state-of-the-art performance with adaptive
token cost on popular segmentation benchmarks. Code and models are released at
https://github.com/yayafengzi/ALToLLM.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While humans effortlessly draw visual objects and shapes by adaptively allocating attention based on their complexity, existing multimodal large language models (MLLMs) remain constrained by rigid token representations.</div>
</details>
</div>
<div class="card">
<div class="title">FaStfact: Faster, Stronger Long-Form Factuality Evaluations in LLMs</div>
<div class="meta-line">Authors: Yingjia Wan, Haochen Tan, Xiao Zhu, Xinyu Zhou, Zhiwei Li, Qingsong Lv, Changxuan Sun, Jiaqi Zeng, Yi Xu, Jianqiao Lu, Yinhong Liu, Zhijiang Guo</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-10-13T19:00:15+00:00 · Latest: 2025-11-05T03:36:23+00:00</div>
<div class="meta-line">Comments: EMNLP 2025 (Findings)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12839v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.12839v2">PDF</a> · <a href="https://github.com/Yingjia-Wan/FaStfact">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the factuality of long-form generations from Large Language Models
(LLMs) remains challenging due to efficiency bottlenecks and reliability
concerns. Prior efforts attempt this by decomposing text into claims, searching
for evidence, and verifying claims, but suffer from critical drawbacks: (1)
inefficiency due to overcomplicated pipeline components, and (2)
ineffectiveness stemming from inaccurate claim sets and insufficient evidence.
To address these limitations, we propose \textbf{FaStfact}, an evaluation
framework that achieves the highest alignment with human evaluation and
time/token efficiency among existing baselines. FaStfact first employs
chunk-level claim extraction integrated with confidence-based pre-verification,
significantly reducing the time and token cost while ensuring reliability. For
searching and verification, it collects document-level evidence from crawled
web-pages and selectively retrieves it during verification. Extensive
experiments based on an annotated benchmark \textbf{FaStfact-Bench} demonstrate
the reliability of FaStfact in both efficiently and effectively evaluating
long-form factuality. Code, benchmark data, and annotation interface tool are
available at https://github.com/Yingjia-Wan/FaStfact.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to efficiency bottlenecks and reliability concerns.</div>
</details>
</div>
<div class="card">
<div class="title">Control Barrier Function for Aligning Large Language Models</div>
<div class="meta-line">Authors: Yuya Miyaoka, Masaki Inoue</div>
<div class="meta-line">First: 2025-11-05T02:12:59+00:00 · Latest: 2025-11-05T02:12:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03121v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03121v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation.</div>
</details>
</div>
<div class="card">
<div class="title">ForTIFAI: Fending Off Recursive Training Induced Failure for AI Model   Collapse</div>
<div class="meta-line">Authors: Soheil Zibakhsh Shabgahi, Pedram Aghazadeh, Azalia Mirhoseini, Farinaz Koushanfar</div>
<div class="meta-line">First: 2025-09-10T20:06:51+00:00 · Latest: 2025-11-05T00:55:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.08972v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.08972v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing reliance on generative AI models is rapidly increasing the
volume of synthetic data, with some projections suggesting that most available
new data for training could be machine-generated by 2030. This shift to a
mainly synthetic content presents a critical challenge: repeated training in
synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. While the causes of model collapse are increasingly
understood, effective mitigation strategies remain scarce. We address this
challenge by leveraging a key insight: auto-regressive models tend to generate
text sequences to which they assign high confidence (i.e., high
log-likelihood). Based on this observation, we introduce the
Truncated-Cross-Entropy (TCE) loss function. TCE mitigates collapse by
selectively ignoring high-confidence tokens during training, effectively
filtering out likely machine-generated artifacts from the learning process. Our
experiments demonstrate that models trained with TCE not only learn effectively
but also exhibit significantly increased resilience, tolerating over 2.3x more
synthetic data before the onset of collapse. In addition, we provide an
open-source benchmark for collapse dynamics in mixed-data settings. Our results
demonstrate that confidence-aware training objectives can substantially delay
collapse onset, offering a practical and generalizable tool for model
robustness under synthetic-data exposure.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing reliance on generative AI models is rapidly increasing the volume of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030.</div>
</details>
</div>
<div class="card">
<div class="title">SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</div>
<div class="meta-line">Authors: Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar</div>
<div class="meta-line">First: 2025-11-05T00:38:31+00:00 · Latest: 2025-11-05T00:38:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03092v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03092v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches.</div>
</details>
</div>
<div class="card">
<div class="title">The Curved Spacetime of Transformer Architectures</div>
<div class="meta-line">Authors: Riccardo Di Sipio, Jairo Diaz-Rodriguez, Luis Serrano</div>
<div class="meta-line">First: 2025-11-04T22:58:40+00:00 · Latest: 2025-11-04T22:58:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.03060v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.03060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein&#x27;s eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present a geometric framework for understanding Transformer-based language models, drawing an explicit analogy to General Relativity.</div>
</details>
</div>
<div class="card">
<div class="title">SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving</div>
<div class="meta-line">Authors: Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Hans Vandierendonck, Deepu John, Bo Ji, Dimitrios Nikolopoulos</div>
<div class="meta-line">First: 2025-06-11T04:55:54+00:00 · Latest: 2025-11-04T21:34:59+00:00</div>
<div class="meta-line">Comments: 8 pages, 8 figures, 2 tables, accepted by SEC 2025: Tenth ACM/IEEE
  Symposium on Edge Computing</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.09397v5">Abs</a> · <a href="http://arxiv.org/pdf/2506.09397v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing gap between the increasing complexity of large language models
(LLMs) and the limited computational budgets of edge devices poses a key
challenge for efficient on-device inference, despite gradual improvements in
hardware capabilities. Existing strategies, such as aggressive quantization,
pruning, or remote inference, trade accuracy for efficiency or lead to
substantial cost burdens. This position paper introduces a new framework that
leverages speculative decoding, previously viewed primarily as a decoding
acceleration technique for autoregressive generation of LLMs, as a promising
approach specifically adapted for edge computing by orchestrating computation
across heterogeneous devices. We propose \acronym, a framework that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server verifies the tokens
utilizing a more precise target model. To further increase the efficiency of
verification, the edge server batch the diverse verification requests from
devices. This approach supports device heterogeneity and reduces server-side
memory footprint by sharing the same upstream target model across multiple
devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and
an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits:
2.2 more system throughput, 2.8 more system capacity, and better cost
efficiency, all without sacrificing model accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data   Compression (Technical Report)</div>
<div class="meta-line">Authors: Guozhong Li, Muhannad Alhumaidi, Spiros Skiadopoulos, Panos Kalnis</div>
<div class="meta-line">First: 2025-10-24T05:41:04+00:00 · Latest: 2025-11-04T20:59:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23632v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.23632v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of high-resolution scientific simulations and observation
systems is generating massive spatiotemporal datasets, making efficient,
error-bounded compression increasingly important. Meanwhile, decoder-only large
language models (LLMs) have demonstrated remarkable capabilities in modeling
complex sequential data. In this paper, we propose LLMCOMP, a novel lossy
compression paradigm that leverages decoder-only large LLMs to model scientific
data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via
Z-order curves to preserve locality, and applies coverage-guided sampling to
enhance training efficiency. An autoregressive transformer is then trained with
spatial-temporal embeddings to model token transitions. During compression, the
model performs top-k prediction, storing only rank indices and fallback
corrections to ensure strict error bounds. Experiments on multiple reanalysis
datasets show that LLMCOMP consistently outperforms state-of-the-art
compressors, achieving up to 30% higher compression ratios under strict error
bounds. These results highlight the potential of LLMs as general-purpose
compressors for high-fidelity scientific data.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid growth of high-resolution scientific simulations and observation systems is generating massive spatiotemporal datasets, making efficient, error-bounded compression increasingly important.</div>
</details>
</div>
<div class="card">
<div class="title">Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities</div>
<div class="meta-line">Authors: Amanda Bertsch, Adithya Pratapa, Teruko Mitamura, Graham Neubig, Matthew R. Gormley</div>
<div class="meta-line">First: 2025-11-04T18:42:12+00:00 · Latest: 2025-11-04T18:42:12+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02817v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02817v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted.</div>
</details>
</div>
<div class="card">
<div class="title">LAWCAT: Efficient Distillation from Quadratic to Linear Attention with   Convolution across Tokens for Long Context Modeling</div>
<div class="meta-line">Authors: Zeyu Liu, Souvik Kundu, Lianghao Jiang, Anni Li, Srikanth Ronanki, Sravan Bodapati, Gourav Datta, Peter A. Beerel</div>
<div class="meta-line">First: 2025-09-22T22:43:44+00:00 · Latest: 2025-11-04T18:01:01+00:00</div>
<div class="meta-line">Comments: 17 pages, 8 figures. EMNLP2025 Findings</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.18467v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.18467v2">PDF</a> · <a href="https://github.com/zeyuliu1037/LAWCAT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&amp;2\&amp;3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&amp;QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources. Code is released at:
https://github.com/zeyuliu1037/LAWCAT</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Although transformer architectures have achieved state-of-the-art performance across diverse domains, their quadratic computational complexity with respect to sequence length remains a significant bottleneck, particularly for latency-sensitive long-context applications.</div>
</details>
</div>
<div class="card">
<div class="title">A Practical Investigation of Spatially-Controlled Image Generation with   Transformers</div>
<div class="meta-line">Authors: Guoxuan Xia, Harleen Hanspal, Petru-Daniel Tudosiu, Shifeng Zhang, Sarah Parisot</div>
<div class="meta-line">First: 2025-07-21T15:33:49+00:00 · Latest: 2025-11-04T17:54:35+00:00</div>
<div class="meta-line">Comments: TMLR https://openreview.net/forum?id=loT6xhgLYK</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.15724v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.15724v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
&quot;forgetting&quot; and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Enabling image generation models to be spatially controlled is an important area of research, empowering users to better generate images according to their own fine-grained specifications via e.g.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251106_0316.html">20251106_0316</a>
<a href="archive/20251105_0315.html">20251105_0315</a>
<a href="archive/20251104_0314.html">20251104_0314</a>
<a href="archive/20251103_0313.html">20251103_0313</a>
<a href="archive/20251102_0313.html">20251102_0313</a>
<a href="archive/20251101_0314.html">20251101_0314</a>
<a href="archive/20251031_0314.html">20251031_0314</a>
<a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
