<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-15 03:13</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251015_0313</div>
    <div class="row"><div class="card">
<div class="title">DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training</div>
<div class="meta-line">Authors: Haoran Feng, Dizhe Zhang, Xiangtai Li, Bo Du, Lu Qi</div>
<div class="meta-line">First: 2025-10-13T17:59:15+00:00 · Latest: 2025-10-13T17:59:15+00:00</div>
<div class="meta-line">Comments: https://fenghora.github.io/DiT360-Page/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11712v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11712v1">PDF</a> · <a href="https://github.com/Insta360-Research-Team/DiT360">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://fenghora.github.io/DiT360-Page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we propose DiT360, a DiT-based framework that performs hybrid
training on perspective and panoramic data for panoramic image generation. For
the issues of maintaining geometric fidelity and photorealism in generation
quality, we attribute the main reason to the lack of large-scale, high-quality,
real-world panoramic data, where such a data-centric view differs from prior
methods that focus on model design. Basically, DiT360 has several key modules
for inter-domain transformation and intra-domain augmentation, applied at both
the pre-VAE image level and the post-VAE token level. At the image level, we
incorporate cross-domain knowledge through perspective image guidance and
panoramic refinement, which enhance perceptual quality while regularizing
diversity and photorealism. At the token level, hybrid supervision is applied
across multiple modules, which include circular padding for boundary
continuity, yaw loss for rotational robustness, and cube loss for distortion
awareness. Extensive experiments on text-to-panorama, inpainting, and
outpainting tasks demonstrate that our method achieves better boundary
consistency and image fidelity across eleven quantitative metrics. Our code is
available at https://github.com/Insta360-Research-Team/DiT360.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation.</div>
</details>
</div>
<div class="card">
<div class="title">StreamAgent: Towards Anticipatory Agents for Streaming Video   Understanding</div>
<div class="meta-line">Authors: Haolin Yang, Feilong Tang, Lingxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, Junjun He, Zongyuan Ge, Imran Razzak</div>
<div class="meta-line">First: 2025-08-03T18:15:42+00:00 · Latest: 2025-10-13T17:15:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.01875v3">Abs</a> · <a href="http://arxiv.org/pdf/2508.01875v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-time streaming video understanding in domains such as autonomous driving
and intelligent surveillance poses challenges beyond conventional offline video
processing, requiring continuous perception, proactive decision making, and
responsive interaction based on dynamically evolving visual content. However,
existing methods rely on alternating perception-reaction or asynchronous
triggers, lacking task-driven planning and future anticipation, which limits
their real-time responsiveness and proactive decision making in evolving video
streams. To this end, we propose a StreamAgent that anticipates the temporal
intervals and spatial regions expected to contain future task-relevant
information to enable proactive and goal-driven responses. Specifically, we
integrate question semantics and historical observations through prompting the
anticipatory agent to anticipate the temporal progression of key events, align
current observations with the expected future evidence, and subsequently adjust
the perception action (e.g., attending to task-relevant regions or continuously
tracking in subsequent frames). To enable efficient inference, we design a
streaming KV-cache memory mechanism that constructs a hierarchical memory
structure for selective recall of relevant tokens, enabling efficient semantic
retrieval while reducing the overhead of storing all tokens in the traditional
KV-cache. Extensive experiments on streaming and long video understanding tasks
demonstrate that our method outperforms existing methods in response accuracy
and real-time efficiency, highlighting its practical value for real-world
streaming scenarios.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content.</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Oriented Token-Adaptive Knowledge Distillation</div>
<div class="meta-line">Authors: Xurong Xie, Zhucun Xue, Jiafu Wu, Jian Li, Yabiao Wang, Xiaobin Hu, Yong Liu, Jiangning Zhang</div>
<div class="meta-line">First: 2025-10-13T16:55:07+00:00 · Latest: 2025-10-13T16:55:07+00:00</div>
<div class="meta-line">Comments: 15 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11615v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11615v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Knowledge distillation (KD) is a key technique for compressing large-scale
language models (LLMs), yet prevailing logit-based methods typically employ
static strategies that are misaligned with the dynamic learning process of
student models. These methods typically treat all tokens indiscriminately and
apply a single, fixed temperature, resulting in suboptimal knowledge transfer.
To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge
Distillation (AdaKD), a novel framework that adapts the distillation process to
the real-time learning state of each token. AdaKD consists of two synergistic
modules driven by a unified token difficulty metric. First, our Loss-Driven
Adaptive Token Focusing (LATF) module dynamically adjusts the distillation
focus by monitoring the student&#x27;s learning stability, concentrating
computational resources on the most valuable tokens at each training phase.
Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a
counterintuitive yet effective token-level temperature strategy. It employs low
temperatures for difficult tokens for targeted error correction, and high
temperatures for easy tokens to encourage students to learn from the teacher&#x27;s
complete and smooth output distribution, thereby enhancing generalization. As a
plug-and-play framework, AdaKD can consistently improve the performance of
various distillation methods on multiple model architectures and benchmarks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Knowledge distillation (KD) is a key technique for compressing large-scale language models (LLMs), yet prevailing logit-based methods typically employ static strategies that are misaligned with the dynamic learning process of student models.</div>
</details>
</div>
<div class="card">
<div class="title">Part-of-speech tagging for Nagamese Language using CRF</div>
<div class="meta-line">Authors: Alovi N Shohe, Chonglio Khiamungam, Teisovi Angami</div>
<div class="meta-line">First: 2025-09-16T12:59:55+00:00 · Latest: 2025-10-13T16:54:53+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.19343v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.19343v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates part-of-speech tagging, an important task in Natural
Language Processing (NLP) for the Nagamese language. The Nagamese language,
a.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily
as a means of communication in trade between the Nagas and people from Assam in
northeast India. A substantial amount of work in part-of-speech-tagging has
been done for resource-rich languages like English, Hindi, etc. However, no
work has been done in the Nagamese language. To the best of our knowledge, this
is the first attempt at part-of-speech tagging for the Nagamese Language. The
aim of this work is to identify the part-of-speech for a given sentence in the
Nagamese language. An annotated corpus of 16,112 tokens is created and applied
machine learning technique known as Conditional Random Fields (CRF). Using CRF,
an overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score
of 85% is achieved.
  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates part-of-speech tagging, an important task in Natural Language Processing (NLP) for the Nagamese language.</div>
</details>
</div>
<div class="card">
<div class="title">Deconstructing Attention: Investigating Design Principles for Effective   Language Modeling</div>
<div class="meta-line">Authors: Huiyin Xue, Nafise Sadat Moosavi, Nikolaos Aletras</div>
<div class="meta-line">First: 2025-10-13T16:42:14+00:00 · Latest: 2025-10-13T16:42:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11602v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of Transformer language models is widely credited to their
dot-product attention mechanism, which interweaves a set of key design
principles: mixing information across positions (enabling multi-token
interactions), sequence-dependent activations (where attention weights adapt to
each input), a specific mathematical form (dot-product similarities plus
softmax weighting), and coupling of queries and keys to evolving hidden states
(grounding attention in the current layer). However, the necessity of each of
these principles remains largely untested. In this work, we systematically
deconstruct attention by designing controlled variants that selectively relax
these principles, applied both uniformly across all layers and in hybrid
architectures where only some layers retain standard attention. Our empirical
analysis reveals that mechanisms for mixing tokens are indispensable, as their
absence collapses models to near-random behavior, while the exact mathematical
form and sequence dependency can be substantially relaxed, especially when
preserved in just a subset of layers. Surprisingly, even variants that fail in
isolation can achieve robust performance when interleaved with standard
attention, highlighting a cooperative effect. These findings deepen our
understanding of what truly underpins attention&#x27;s effectiveness and open new
avenues for simplifying language models without sacrificing performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The success of Transformer language models is widely credited to their dot-product attention mechanism, which interweaves a set of key design principles: mixing information across positions (enabling multi-token interactions), sequence-dependent activations (where attention weights adapt to each input), a specific mathematical form (dot-product similarities plus softmax weighting), and coupling of queries and keys to evolving hidden states (grounding attention in the current layer).</div>
</details>
</div>
<div class="card">
<div class="title">InstructSAM: A Training-Free Framework for Instruction-Oriented Remote   Sensing Object Recognition</div>
<div class="meta-line">Authors: Yijie Zheng, Weijie Wu, Qingyun Li, Xuehui Wang, Xu Zhou, Aiai Ren, Jun Shen, Long Zhao, Guoqing Li, Xue Yang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-21T17:59:56+00:00 · Latest: 2025-10-13T16:36:15+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.15818v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.15818v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language-Guided object recognition in remote sensing imagery is crucial for
large-scale mapping and automated data annotation. However, existing
open-vocabulary and visual grounding methods rely on explicit category cues,
limiting their ability to handle complex or implicit queries that require
advanced reasoning. To address this issue, we introduce a new suite of tasks,
including Instruction-Oriented Object Counting, Detection, and Segmentation
(InstructCDS), covering open-vocabulary, open-ended, and open-subclass
scenarios. We further present EarthInstruct, the first InstructCDS benchmark
for earth observation. It is constructed from two diverse remote sensing
datasets with varying spatial resolutions and annotation rules across 20
categories, necessitating models to interpret dataset-specific instructions.
Given the scarcity of semantically rich labeled data in remote sensing, we
propose InstructSAM, a training-free framework for instruction-driven object
recognition. InstructSAM leverages large vision-language models to interpret
user instructions and estimate object counts, employs SAM2 for mask proposal,
and formulates mask-label assignment as a binary integer programming problem.
By integrating semantic similarity with counting constraints, InstructSAM
efficiently assigns categories to predicted masks without relying on confidence
thresholds. Experiments demonstrate that InstructSAM matches or surpasses
specialized baselines across multiple tasks while maintaining near-constant
inference time regardless of object count, reducing output tokens by 89% and
overall runtime by over 32% compared to direct generation approaches. We
believe the contributions of the proposed tasks, benchmark, and effective
approach will advance future research in developing versatile object
recognition systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Language-Guided object recognition in remote sensing imagery is crucial for large-scale mapping and automated data annotation.</div>
</details>
</div>
<div class="card">
<div class="title">Holistic Evaluation of Multimodal LLMs on Spatial Intelligence</div>
<div class="meta-line">Authors: Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Jiaqi Li, Xiangyu Fan, Hanming Deng, Lewei Lu, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang</div>
<div class="meta-line">First: 2025-08-18T17:55:17+00:00 · Latest: 2025-10-13T16:08:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.13142v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.13142v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, the very capability that anchors artificial
general intelligence in the physical world. With the recent release of GPT-5,
allegedly the most powerful AI model to date, it is timely to examine where the
leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path
toward spatial intelligence. We first propose a holistic taxonomy of spatial
tasks that unifies existing benchmarks and a standardized protocol for the fair
evaluation of state-of-the-art proprietary and open-source models across eight
key benchmarks, at a cost exceeding ten billion total tokens. Our empirical
study then reveals that (1) GPT-5 demonstrates unprecedented strength in
spatial intelligence (SI), yet (2) still falls short of human performance
significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that
SI-tasks expose greater model capability deficiency than non-SI tasks, to the
extent that (4) proprietary models do not exhibit a decisive advantage when
facing the most difficult ones. In addition, we conduct a qualitative
evaluation across a diverse set of scenarios that are intuitive for humans, yet
fail even the most advanced multimodal models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal models have achieved remarkable progress in recent years.</div>
</details>
</div>
<div class="card">
<div class="title">Massive Activations are the Key to Local Detail Synthesis in Diffusion   Transformers</div>
<div class="meta-line">Authors: Chaofan Gan, Zicheng Zhao, Yuanpeng Tu, Xi Chen, Ziran Qin, Tieyuan Chen, Mehrtash Harandi, Weiyao Lin</div>
<div class="meta-line">First: 2025-10-13T15:39:13+00:00 · Latest: 2025-10-13T15:39:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11538v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11538v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) have recently emerged as a powerful backbone
for visual generation. Recent observations reveal \emph{Massive Activations}
(MAs) in their internal feature maps, yet their function remains poorly
understood. In this work, we systematically investigate these activations to
elucidate their role in visual generation. We found that these massive
activations occur across all spatial tokens, and their distribution is
modulated by the input timestep embeddings. Importantly, our investigations
further demonstrate that these massive activations play a key role in local
detail synthesis, while having minimal impact on the overall semantic content
of output. Building on these insights, we propose \textbf{D}etail
\textbf{G}uidance (\textbf{DG}), a MAs-driven, training-free self-guidance
strategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG
constructs a degraded ``detail-deficient&#x27;&#x27; model by disrupting MAs and
leverages it to guide the original network toward higher-quality detail
synthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG),
enabling further refinements of fine-grained details. Extensive experiments
demonstrate that our DG consistently improves fine-grained detail quality
across various pre-trained DiTs (\eg, SD3, SD3.5, and Flux).</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion Transformers (DiTs) have recently emerged as a powerful backbone for visual generation.</div>
</details>
</div>
<div class="card">
<div class="title">Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal   Large Language Model</div>
<div class="meta-line">Authors: Ruiping Liu, Junwei Zheng, Yufan Chen, Zirui Wang, Kunyu Peng, Kailun Yang, Jiaming Zhang, Marc Pollefeys, Rainer Stiefelhagen</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-13T15:17:18+00:00 · Latest: 2025-10-13T15:17:18+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025 Datasets and Benchmarks Track. Dataset and
  Code: https://github.com/RuipingL/Situat3DChange</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11509v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11509v1">PDF</a> · <a href="https://github.com/RuipingL/Situat3DChange">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Physical environments and circumstances are fundamentally dynamic, yet
current 3D datasets and evaluation benchmarks tend to concentrate on either
dynamic scenarios or dynamic situations in isolation, resulting in incomplete
comprehension. To overcome these constraints, we introduce Situat3DChange, an
extensive dataset supporting three situation-aware change understanding tasks
following the perception-action model: 121K question-answer pairs, 36K change
descriptions for perception tasks, and 17K rearrangement instructions for the
action task. To construct this large-scale dataset, Situat3DChange leverages
11K human observations of environmental changes to establish shared mental
models and shared situational awareness for human-AI collaboration. These
observations, enriched with egocentric and allocentric perspectives as well as
categorical and coordinate spatial relations, are integrated using an LLM to
support understanding of situated changes. To address the challenge of
comparing pairs of point clouds from the same scene with minor changes, we
propose SCReasoner, an efficient 3D MLLM approach that enables effective point
cloud comparison with minimal parameter overhead and no additional tokens
required for the language decoder. Comprehensive evaluation on Situat3DChange
tasks highlights both the progress and limitations of MLLMs in dynamic scene
and situation understanding. Additional experiments on data scaling and
cross-domain transfer demonstrate the task-agnostic effectiveness of using
Situat3DChange as a training dataset for MLLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Physical environments and circumstances are fundamentally dynamic, yet current 3D datasets and evaluation benchmarks tend to concentrate on either dynamic scenarios or dynamic situations in isolation, resulting in incomplete comprehension.</div>
</details>
</div>
<div class="card">
<div class="title">How Reinforcement Learning After Next-Token Prediction Facilitates   Learning</div>
<div class="meta-line">Authors: Nikolaos Tsilivis, Eran Malach, Karen Ullrich, Julia Kempe</div>
<div class="meta-line">First: 2025-10-13T15:04:00+00:00 · Latest: 2025-10-13T15:04:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11495v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11495v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reasoning domains with neural networks have primarily been
enabled by a training recipe that optimizes Large Language Models, previously
trained to predict the next-token in a sequence, with reinforcement learning
algorithms. We introduce a framework to study the success of this paradigm, and
we theoretically expose the optimization mechanisms by which reinforcement
learning improves over next-token prediction in this setting. We study learning
from mixture distributions of short and long ``chain-of-thought&#x27;&#x27; sequences
encoding a single task. In particular, when the task consists of predicting the
parity of $d$ bits and long sequences are rare, we show how reinforcement
learning after next-token prediction enables autoregressive transformers to
generalize, whereas mere next-token prediction requires extreme statistical or
computational resources to do so. We further explain how reinforcement learning
leverages increased test-time computation, manifested in longer responses, to
facilitate this learning process. In a simplified setting, we theoretically
prove that autoregressive linear models following this training recipe can
efficiently learn to predict the parity of $d$ bits as long as the proportion
of long demonstrations in the data mix is not exponentially small in the input
dimension $d$. Finally, we demonstrate these same phenomena in other settings,
including the post-training of Llama-series models on mixture variations of
common mathematical reasoning benchmarks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in reasoning domains with neural networks have primarily been enabled by a training recipe that optimizes Large Language Models, previously trained to predict the next-token in a sequence, with reinforcement learning algorithms.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond [cls]: Exploring the true potential of Masked Image Modeling   representations</div>
<div class="meta-line">Authors: Marcin Przewięźlikowski, Randall Balestriero, Wojciech Jasiński, Marek Śmieja, Bartosz Zieliński</div>
<div class="meta-line">First: 2024-12-04T11:08:32+00:00 · Latest: 2025-10-13T14:50:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.03215v3">Abs</a> · <a href="http://arxiv.org/pdf/2412.03215v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked Image Modeling (MIM) has emerged as a promising approach for
Self-Supervised Learning (SSL) of visual representations. However, the
out-of-the-box performance of MIMs is typically inferior to competing
approaches. Most users cannot afford fine-tuning due to the need for large
amounts of data, high GPU consumption, and specialized user knowledge.
Therefore, the practical use of MIM representations is limited. In this paper
we ask what is the reason for the poor out-of-the-box performance of MIMs. Is
it due to weaker features produced by MIM models, or is it due to suboptimal
usage? Through detailed analysis, we show that attention in MIMs is spread
almost uniformly over many patches, leading to ineffective aggregation by the
[cls] token. Based on this insight, we propose Selective Aggregation to better
capture the rich semantic information retained in patch tokens, which
significantly improves the out-of-the-box performance of MIM.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Masked Image Modeling (MIM) has emerged as a promising approach for Self-Supervised Learning (SSL) of visual representations.</div>
</details>
</div>
<div class="card">
<div class="title">From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by   Composing Old Ones</div>
<div class="meta-line">Authors: Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng</div>
<div class="meta-line">First: 2025-09-29T17:44:27+00:00 · Latest: 2025-10-13T13:03:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.25123v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.25123v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Does RL teach LLMs genuinely new skills, or does it merely activate existing
ones? This question lies at the core of ongoing debates about the role of RL in
LLM post-training. On one side, strong empirical results can be achieved with
RL even without preceding supervised finetuning; on the other, critics argue
that RL contributes little beyond reweighting existing reasoning strategies.
This work provides concrete evidence that LLMs can acquire genuinely new skills
during RL by composing existing ones, mirroring one of the central mechanisms
by which humans acquire new cognitive skills. To mitigate data contamination
and other confounding factors, and to allow precise control over task
complexity, we develop a synthetic framework for our investigation.
Specifically, we define a skill as the ability to infer the output of a string
transformation function f(x) given x. When an LLM has already learned f and g
prior to RL, our experiments reveal that RL enables it to learn unseen
compositions of them h(x)=g(f(x)). Further, this compositional ability
generalizes to more difficult problems such as compositions of &gt;2 functions
unseen during RL training. Surprisingly, our experiments show that
compositional skill acquired on a source task transfers to a different target
task. This transfer happens even without compositional training on the target,
requiring only prior knowledge of the target&#x27;s atomic skills. Our qualitative
analysis shows that RL fundamentally changes the reasoning behaviors of the
models. In contrast, next-token training with the same data yields none of
these findings. Our systematic experiments provide fresh insights into LLM
learning, suggesting the value of first building base models with basic skills,
then using RL to incentivize advanced, generalizable skills for complex
problems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Does RL teach LLMs genuinely new skills, or does it merely activate existing ones?</div>
</details>
</div>
<div class="card">
<div class="title">InternSVG: Towards Unified SVG Tasks with Multimodal Large Language   Models</div>
<div class="meta-line">Authors: Haomin Wang, Jinhui Yin, Qi Wei, Wenguang Zeng, Lixin Gu, Shenglong Ye, Zhangwei Gao, Yaohui Wang, Yanting Zhang, Yuanqi Li, Yanwen Guo, Wenhai Wang, Kai Chen, Yu Qiao, Hongjie Zhang</div>
<div class="meta-line">First: 2025-10-13T12:38:04+00:00 · Latest: 2025-10-13T12:38:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11341v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11341v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">General SVG modeling remains challenging due to fragmented datasets, limited
transferability of methods across tasks, and the difficulty of handling
structural complexity. In response, we leverage the strong transfer and
generalization capabilities of multimodal large language models (MLLMs) to
achieve unified modeling for SVG understanding, editing, and generation. We
present the InternSVG family, an integrated data-benchmark-model suite. At its
core is SAgoge, the largest and most comprehensive multimodal dataset for SVG
tasks, encompassing both static graphics and dynamic animations. It covers
icons, long-sequence illustrations, scientific diagrams, and dynamic
animations, supporting tasks of varied difficulty levels and providing deeper
hierarchies with richer attributes compared to previous datasets. Based on this
resource, we introduce SArena, a companion benchmark with comprehensive task
definitions and standardized evaluation that aligns with the domains and
difficulty spectrum covered by SAgoge. Building on these foundations, we
propose InternSVG, a unified MLLM for SVG understanding, editing, and
generation with SVG-specific special tokens, subword-based embedding
initialization, and a two-stage training strategy that progresses from short
static SVGs to long-sequence illustrations and complex animations. This unified
formulation induces positive transfer and improves overall performance.
Experiments on SArena and prior benchmark confirm that InternSVG achieves
substantial gains and consistently outperforms leading open and proprietary
counterparts.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity.</div>
</details>
</div>
<div class="card">
<div class="title">LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences</div>
<div class="meta-line">Authors: Wenbo Wu, Qingyi Si, Xiurui Pan, Ye Wang, Jie Zhang</div>
<div class="meta-line">First: 2025-10-13T11:28:30+00:00 · Latest: 2025-10-13T11:28:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11292v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11292v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Key-Value (KV) cache succeeds in reducing redundant computations in
auto-regressive models, it introduces significant memory overhead, limiting its
practical deployment in long-sequence scenarios. Existing KV retrieval methods
mitigate this by dynamically retaining only a subset of KV entries on the GPU.
However, they still suffer from notable efficiency and accuracy bottlenecks due
to per-token retrieval and coarse-grained page-level KV management, especially
in long-output reasoning scenarios. With the emergence of large reasoning
models, efficiently handling such scenarios has become increasingly important.
To address this issue, we present two key observations: (1) critical KVs
exhibit strong temporal locality during decoding, and (2) these KVs exhibit
distinct distribution patterns across the input prompt and generated output.
Building on these observations, we propose LouisKV, an efficient KV cache
retrieval framework designed for various long-sequence scenarios. Specifically,
LouisKV introduces a semantic-aware retrieval strategy leveraging temporal
locality to trigger retrieval only at semantic boundaries, drastically reducing
computation and data transfer overhead. LouisKV also designs a decoupled,
fine-grained management scheme that tailors differentiated strategies for input
and output sequences to create retrieval units that better match the model&#x27;s
attention patterns, enabling precise identification of critical KVs.
Furthermore, to boost efficiency, LouisKV incorporates several kernel-level
optimizations, including custom Triton and CUDA kernels to accelerate the KV
clustering and retrieval. Evaluations show that LouisKV achieves up to
4.7$\times$ speedup over state-of-the-art KV retrieval methods while
maintaining near-lossless accuracy across diverse long-sequence tasks,
including long-input short-output, short-input long-output, and long-input
long-output scenarios.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While Key-Value (KV) cache succeeds in reducing redundant computations in auto-regressive models, it introduces significant memory overhead, limiting its practical deployment in long-sequence scenarios.</div>
</details>
</div>
<div class="card">
<div class="title">Vision-LLMs for Spatiotemporal Traffic Forecasting</div>
<div class="meta-line">Authors: Ning Yang, Hengyu Zhong, Haijun Zhang, Randall Berry</div>
<div class="meta-line">First: 2025-10-13T11:15:56+00:00 · Latest: 2025-10-13T11:15:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11282v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11282v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate spatiotemporal traffic forecasting is a critical prerequisite for
proactive resource management in dense urban mobile networks. While Large
Language Models (LLMs) have shown promise in time series analysis, they
inherently struggle to model the complex spatial dependencies of grid-based
traffic data. Effectively extending LLMs to this domain is challenging, as
representing the vast amount of information from dense geographical grids can
be inefficient and overwhelm the model&#x27;s context. To address these challenges,
we propose ST-Vision-LLM, a novel framework that reframes spatiotemporal
forecasting as a vision-language fusion problem. Our approach leverages a
Vision-LLM visual encoder to process historical global traffic matrices as
image sequences, providing the model with a comprehensive global view to inform
cell-level predictions. To overcome the inefficiency of LLMs in handling
numerical data, we introduce an efficient encoding scheme that represents
floating-point values as single tokens via a specialized vocabulary, coupled
with a two-stage numerical alignment fine-tuning process. The model is first
trained with Supervised Fine-Tuning (SFT) and then further optimized for
predictive accuracy using Group Relative Policy Optimization (GRPO), a
memory-efficient reinforcement learning method. Evaluations on real-world
mobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing
methods by 15.6% in long-term prediction accuracy and exceeds the second-best
baseline by over 30.04% in cross-domain few-shot scenarios. Our extensive
experiments validate the model&#x27;s strong generalization capabilities across
various data-scarce environments.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate spatiotemporal traffic forecasting is a critical prerequisite for proactive resource management in dense urban mobile networks.</div>
</details>
</div>
<div class="card">
<div class="title">dInfer: An Efficient Inference Framework for Diffusion Language Models</div>
<div class="meta-line">Authors: Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, Xinyuan Zhang, Zhen Tao, Haibo Feng, Ziyun Jiang, Ying Xu, Zenan Huang, Yihong Zhuang, Haokai Xu, Jiaqi Hu, Zhenzhong Lan, Junbo Zhao, Jianguo Li, Da Zheng</div>
<div class="meta-line">First: 2025-10-09T16:19:42+00:00 · Latest: 2025-10-13T10:39:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.08666v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.08666v2">PDF</a> · <a href="https://github.com/inclusionAI/dInfer">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based large language models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs, leveraging denoising-based generation
to enable inherent parallelism. Even more and more open-sourced dLLM models
emerge, yet their widespread adoption remains constrained by the lack of a
standardized and efficient inference framework. We present dInfer, an efficient
and extensible framework for dLLM inference. dInfer decomposes the inference
pipeline into four modular components--model, diffusion iteration manager,
decoding strategy, and KV-cache manager--and integrates novel algorithms for
each component alongside system-level optimizations. Through this combination
of algorithmic innovations and system enhancements, dInfer achieves substantial
efficiency gains without compromising output quality on LLaDA-MoE. At batch
size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800
tokens per second across six benchmarks on $8\times$ H800 GPUs. Compared to
prior systems, dInfer delivers a $10\times$ speedup over Fast-dLLM while
maintaining similar model performance. Even compared to the AR model (with a
comparable number of activation parameters and performance) QWen2.5-3B, which
is highly optimized with the latest vLLM inference engine, dInfer still
delivers a $2$-$3\times$ speedup. The implementation of dInfer is open-sourced
at https://github.com/inclusionAI/dInfer.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism.</div>
</details>
</div>
<div class="card">
<div class="title">MVIGER: Multi-View Variational Integration of Complementary Knowledge   for Generative Recommender</div>
<div class="meta-line">Authors: Tongyoung Kim, Soojin Yoon, Seongku Kang, Jinyoung Yeo, Dongha Lee</div>
<div class="meta-line">First: 2024-08-16T11:59:01+00:00 · Latest: 2025-10-13T10:26:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2408.08686v3">Abs</a> · <a href="http://arxiv.org/pdf/2408.08686v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language Models (LMs) have been widely used in recommender systems to
incorporate textual information of items into item IDs, leveraging their
advanced language understanding and generation capabilities. Recently,
generative recommender systems have utilized the reasoning abilities of LMs to
directly generate index tokens for potential items of interest based on the
user&#x27;s interaction history. To inject diverse item knowledge into LMs, prompt
templates with detailed task descriptions and various indexing techniques
derived from diverse item information have been explored. This paper focuses on
the inconsistency in outputs generated by variations in input prompt templates
and item index types, even with the same user&#x27;s interaction history. Our
in-depth quantitative analysis reveals that preference knowledge learned from
diverse prompt templates and heterogeneous indices differs significantly,
indicating a high potential for complementarity. To fully exploit this
complementarity and provide consistent performance under varying prompts and
item indices, we propose MVIGER, a unified variational framework that models
selection among these information sources as a categorical latent variable with
a learnable prior. During inference, this prior enables the model to adaptively
select the most relevant source or aggregate predictions across multiple
sources, thereby ensuring high-quality recommendation across diverse
template-index combinations. We validate the effectiveness of MVIGER on three
real-world datasets, demonstrating its superior performance over existing
generative recommender baselines through the effective integration of
complementary knowledge.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Language Models (LMs) have been widely used in recommender systems to incorporate textual information of items into item IDs, leveraging their advanced language understanding and generation capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for   Vision-Language-Action Models</div>
<div class="meta-line">Authors: Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, Shixin Wu, Songfang Huang, Huiling Duan</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-15T12:03:34+00:00 · Latest: 2025-10-13T10:18:34+00:00</div>
<div class="meta-line">Comments: Manuscript submitted to AAAI 2026, currently under review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.19257v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.19257v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Can Tool-Integrated Reinforcement Learning Generalize Across Diverse   Domains?</div>
<div class="meta-line">Authors: Zhengyu Chen, Jinluan Yang, Teng Xiao, Ruochen Zhou, Luan Zhang, Xiangyu Xi, Xiaowei Shi, Wei Wang, Jinggang Wang</div>
<div class="meta-line">First: 2025-10-13T09:19:13+00:00 · Latest: 2025-10-13T09:19:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11184v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11184v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities in reasoning and tool utilization. However, the generalization of
tool-augmented reinforcement learning (RL) across diverse domains remains
underexplored. In this work, we investigate the cross-domain generalization of
an LLM agent equipped with a code interpreter tool, which is exclusively
trained on mathematical problem-solving tasks. Despite the restricted training
domain, we evaluate the agent&#x27;s performance across several distinct reasoning
domains. The results reveal that RL-based tool usage learned from mathematical
tasks can be effectively transferred to complex tasks in other domains,
enabling great task performance and high token efficiency. To facilitate this
cross-domain transfer, we propose a Tool Generalization Reinforcement Learning
(TGRL) framework designed to promote domain-agnostic learning and skill
migration, encompassing: (i) a standardized tool interface that abstracts
domain-specific nuances through consistent formatting and explicit termination,
fostering transferable invocation patterns; (ii) a dual-component reward system
that decomposes rewards to incentivize generalizable behaviors like tool
efficiency and reasoning abstraction, ensuring alignment and robustness across
domain shifts; and (iii) an XML-based prompt template that separates thinking,
tool calls, and responses to encourage modular, domain-invariant planning and
coherent multi-turn interactions. Extensive experiments across diverse
benchmarks validate our approach, achieving state-of-the-art performance and
highlighting the cross-domain potential of Tool RL for LLM reasoning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in reasoning and tool utilization.</div>
</details>
</div>
<div class="card">
<div class="title">SANA-Video: Efficient Video Generation with Block Linear Diffusion   Transformer</div>
<div class="meta-line">Authors: Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie</div>
<div class="meta-line">First: 2025-09-29T12:28:09+00:00 · Latest: 2025-10-13T09:12:27+00:00</div>
<div class="meta-line">Comments: 21 pages, 15 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.24695v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.24695v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce SANA-Video, a small diffusion model that can efficiently
generate videos up to 720x1280 resolution and minute-length duration.
SANA-Video synthesizes high-resolution, high-quality and long videos with
strong text-video alignment at a remarkably fast speed, deployable on RTX 5090
GPU. Two core designs ensure our efficient, effective and long video
generation: (1) Linear DiT: We leverage linear attention as the core operation,
which is more efficient than vanilla attention given the large number of tokens
processed in video generation. (2) Constant-Memory KV cache for Block Linear
Attention: we design block-wise autoregressive approach for long video
generation by employing a constant-memory state, derived from the cumulative
properties of linear attention. This KV cache provides the Linear DiT with
global context at a fixed memory cost, eliminating the need for a traditional
KV cache and enabling efficient, minute-long video generation. In addition, we
explore effective data filters and model training strategies, narrowing the
training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of
MovieGen. Given its low cost, SANA-Video achieves competitive performance
compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B
and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,
SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating
the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x
speedup). In summary, SANA-Video enables low-cost, high-quality video
generation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration.</div>
</details>
</div>
<div class="card">
<div class="title">CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning   Segmentation</div>
<div class="meta-line">Authors: Zhenyu Lu, Liupeng Li, Jinpeng Wang, Yan Feng, Bin Chen, Ke Chen, Yaowei Wang</div>
<div class="meta-line">First: 2025-10-13T09:07:54+00:00 · Latest: 2025-10-13T09:07:54+00:00</div>
<div class="meta-line">Comments: 18 pages, 6 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11173v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11173v1">PDF</a> · <a href="https://github.com/ZhenyuLU-Heliodore/CoPRS.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing works on reasoning segmentation either connect hidden features from
a language model directly to a mask decoder or represent positions in text,
which limits interpretability and semantic detail. To solve this, we present
CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model
that bridges language reasoning to segmentation through a differentiable and
interpretable positional prior instantiated as a heatmap. By making the
reasoning process clear via MCoT and expressing it as a dense, differentiable
heatmap, this interface enhances interpretability and diagnostic analysis and
yields more concentrated evidence on the target. A learnable concentration
token aggregates features of the image and reasoning text to generate this
positional prior, which is decoded to precise masks through a lightweight
decoder, providing a direct connection between reasoning and segmentation.
Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best
reported metrics on each standard split under comparable protocols, with
performance at or above prior state of the art across both validation and test
partitions. Extensive experiments reveal that the quality of the heatmap
strongly influences the resulting mask quality, supporting a consistent
association between the reasoning output and downstream mask generation.
Collectively, these findings support the utility of this paradigm in bridging
reasoning and segmentation and show advantages in concentration driven by
reasoning and predicting masks more precisely. Code, checkpoints and logs are
released at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing works on reasoning segmentation either connect hidden features from a language model directly to a mask decoder or represent positions in text, which limits interpretability and semantic detail.</div>
</details>
</div>
<div class="card">
<div class="title">EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling</div>
<div class="meta-line">Authors: Daniel Scalena, Leonidas Zotos, Elisabetta Fersini, Malvina Nissim, Ahmet Üstün</div>
<div class="meta-line">First: 2025-10-13T09:04:28+00:00 · Latest: 2025-10-13T09:04:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11170v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11170v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rise of reasoning language models and test-time scaling methods as a
paradigm for improving model performance, substantial computation is often
required to generate multiple candidate sequences from the same prompt. This
enables exploration of different reasoning paths toward the correct solution,
however, allocates the same compute budget for each prompt. Grounded on the
assumption that different prompts carry different degrees of complexity, and
thus different computation needs, we propose EAGer, a training-free generation
method that leverages model uncertainty through token-wise entropy distribution
to reduce redundant computation and concurrently improve overall performance.
EAGer allows branching to multiple reasoning paths only in the presence of
high-entropy tokens, and then reallocates the saved compute budget to the
instances where exploration of alternative paths is most needed. We find that
across multiple open-source models on complex reasoning benchmarks such as AIME
2025, EAGer can reallocate the budget without accessing target labels,
achieving the best efficiency-performance trade-off in terms of reasoning
length and Pass@k. When target labels are accessible, EAGer generates up to 65%
fewer tokens (hence saving compute) and achieves up to 37% improvement in
Pass@k compared to the Full Parallel Sampling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the rise of reasoning language models and test-time scaling methods as a paradigm for improving model performance, substantial computation is often required to generate multiple candidate sequences from the same prompt.</div>
</details>
</div>
<div class="card">
<div class="title">video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via   Memory</div>
<div class="meta-line">Authors: Guangzhi Sun, Yixuan Li, Xiaodong Wu, Yudong Yang, Wei Li, Zejun Ma, Chao Zhang</div>
<div class="meta-line">First: 2025-10-13T08:20:15+00:00 · Latest: 2025-10-13T08:20:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11129v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11129v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continuous, high-frame-rate, high-resolution processing of long video streams
is critical for future AI agents, yet current video-understanding LLMs struggle
to scale. Offline, fixed-frame-number methods require the stream length to
adapt frame rates; streaming methods constrain memory by merging or discarding
tokens, losing information. We propose video-SALMONN S, a streaming
audio-visual LLM that, to our knowledge, is the first to process 3-hour videos
at 1 FPS and 360p resolution under a fixed memory budget. Our model introduces
(i) a test-time-training (TTT) memory module that continually updates token
representations to capture long-range dependencies by replacing token merging,
and (ii) a prompt-dependent memory reader that selectively retrieves
context-relevant content from fixed-size memory. The TTT module is optimised
with a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient
adaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),
video-SALMONN S sustains high-quality understanding on multi-hour videos with
10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and
67.8% on the Video-MME long split, outperforming both offline and streaming
baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Continuous, high-frame-rate, high-resolution processing of long video streams is critical for future AI agents, yet current video-understanding LLMs struggle to scale.</div>
</details>
</div>
<div class="card">
<div class="title">PhysioME: A Robust Multimodal Self-Supervised Framework for   Physiological Signals with Missing Modalities</div>
<div class="meta-line">Authors: Cheol-Hui Lee, Hwa-Yeon Lee, Min-Kyung Jung, Dong-Joo Kim</div>
<div class="meta-line">First: 2025-10-13T08:00:55+00:00 · Latest: 2025-10-13T08:00:55+00:00</div>
<div class="meta-line">Comments: 9 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11110v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11110v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Missing or corrupted modalities are common in physiological signal-based
medical applications owing to hardware constraints or motion artifacts.
However, most existing methods assume the availability of all modalities,
resulting in substantial performance degradation in the absence of any
modality. To overcome this limitation, this study proposes PhysioME, a robust
framework designed to ensure reliable performance under missing modality
conditions. PhysioME adopts: (1) a multimodal self-supervised learning approach
that combines contrastive learning with masked prediction; (2) a
Dual-PathNeuroNet backbone tailored to capture the temporal dynamics of each
physiological signal modality; and (3) a restoration decoder that reconstructs
missing modality tokens, enabling flexible processing of incomplete inputs. The
experimental results show that PhysioME achieves high consistency and
generalization performance across various missing modality scenarios. These
findings highlight the potential of PhysioME as a reliable tool for supporting
clinical decision-making in real-world settings with imperfect data
availability.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Missing or corrupted modalities are common in physiological signal-based medical applications owing to hardware constraints or motion artifacts.</div>
</details>
</div>
<div class="card">
<div class="title">QUITO-X: A New Perspective on Context Compression from the Information   Bottleneck Theory</div>
<div class="meta-line">Authors: Yihang Wang, Xu Huang, Bowen Tian, Yueyang Su, Lei Yu, Huaming Liao, Yixing Fan, Jiafeng Guo, Xueqi Cheng</div>
<div class="meta-line">First: 2024-08-20T02:44:45+00:00 · Latest: 2025-10-13T07:51:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2408.10497v3">Abs</a> · <a href="http://arxiv.org/pdf/2408.10497v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative LLM have achieved remarkable success in various industrial
applications, owing to their promising In-Context Learning capabilities.
However, the issue of long context in complex tasks poses a significant barrier
to their wider adoption, manifested in two main aspects: (i) The excessively
long context leads to high costs and inference delays. (ii) A substantial
amount of task-irrelevant information introduced by long contexts exacerbates
the &quot;lost in the middle&quot; problem. Existing methods compress context by removing
redundant tokens using metrics such as self-information or PPL, which is
inconsistent with the objective of retaining the most important tokens when
conditioning on a given query. In this study, we introduce information
bottleneck theory (IB) to model the problem, offering a novel perspective that
thoroughly addresses the essential properties required for context compression.
Additionally, we propose a cross-attention-based approach to approximate mutual
information in IB, which can be flexibly replaced with suitable alternatives in
different scenarios. Extensive experiments on four datasets demonstrate that
our method achieves a 25% increase in compression rate compared to the
state-of-the-art, while maintaining question answering performance. In
particular, the context compressed by our method even outperform the full
context in some cases.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Generative LLM have achieved remarkable success in various industrial applications, owing to their promising In-Context Learning capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">Flow Matching-Based Autonomous Driving Planning with Advanced   Interactive Behavior Modeling</div>
<div class="meta-line">Authors: Tianyi Tan, Yinan Zheng, Ruiming Liang, Zexu Wang, Kexin Zheng, Jinliang Zheng, Jianxiong Li, Xianyuan Zhan, Jingjing Liu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-13T07:25:13+00:00 · Latest: 2025-10-13T07:25:13+00:00</div>
<div class="meta-line">Comments: 26 pages, 6 figures. Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.11083v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.11083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modeling interactive driving behaviors in complex scenarios remains a
fundamental challenge for autonomous driving planning. Learning-based
approaches attempt to address this challenge with advanced generative models,
removing the dependency on over-engineered architectures for representation
fusion. However, brute-force implementation by simply stacking transformer
blocks lacks a dedicated mechanism for modeling interactive behaviors that are
common in real driving scenarios. The scarcity of interactive driving data
further exacerbates this problem, leaving conventional imitation learning
methods ill-equipped to capture high-value interactive behaviors. We propose
Flow Planner, which tackles these problems through coordinated innovations in
data modeling, model architecture, and learning scheme. Specifically, we first
introduce fine-grained trajectory tokenization, which decomposes the trajectory
into overlapping segments to decrease the complexity of whole trajectory
modeling. With a sophisticatedly designed architecture, we achieve efficient
temporal and spatial fusion of planning and scene information, to better
capture interactive behaviors. In addition, the framework incorporates flow
matching with classifier-free guidance for multi-modal behavior generation,
which dynamically reweights agent interactions during inference to maintain
coherent response strategies, providing a critical boost for interactive
scenario understanding. Experimental results on the large-scale nuPlan dataset
and challenging interactive interPlan dataset demonstrate that Flow Planner
achieves state-of-the-art performance among learning-based approaches while
effectively modeling interactive behaviors in complex driving scenarios.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modeling interactive driving behaviors in complex scenarios remains a fundamental challenge for autonomous driving planning.</div>
</details>
</div>
<div class="card">
<div class="title">LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</div>
<div class="meta-line">Authors: Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, Lianhui Qin</div>
<div class="meta-line">First: 2025-10-06T08:15:03+00:00 · Latest: 2025-10-13T07:01:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.04573v3">Abs</a> · <a href="http://arxiv.org/pdf/2510.04573v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM&#x27;s autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation.</div>
</details>
</div>
<div class="card">
<div class="title">Context Guided Transformer Entropy Modeling for Video Compression</div>
<div class="meta-line">Authors: Junlong Tong, Wei Zhang, Yaohui Jin, Xiaoyu Shen</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-08-03T17:07:49+00:00 · Latest: 2025-10-13T06:50:44+00:00</div>
<div class="meta-line">Comments: ICCV 2025. This is an update to the camera-ready version</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.01852v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.01852v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conditional entropy models effectively leverage spatio-temporal contexts to
reduce video redundancy. However, incorporating temporal context often
introduces additional model complexity and increases computational cost. In
parallel, many existing spatial context models lack explicit modeling the
ordering of spatial dependencies, which may limit the availability of relevant
context during decoding. To address these issues, we propose the Context Guided
Transformer (CGT) entropy model, which estimates probability mass functions of
the current frame conditioned on resampled temporal context and
dependency-weighted spatial context. A temporal context resampler learns
predefined latent queries to extract critical temporal information using
transformer encoders, reducing downstream computational overhead. Meanwhile, a
teacher-student network is designed as dependency-weighted spatial context
assigner to explicitly model the dependency of spatial context order. The
teacher generates an attention map to represent token importance and an entropy
map to reflect prediction certainty from randomly masked inputs, guiding the
student to select the weighted top-k tokens with the highest spatial
dependency. During inference, only the student is used to predict undecoded
tokens based on high-dependency context. Experimental results demonstrate that
our CGT model reduces entropy modeling time by approximately 65% and achieves
an 11% BD-Rate reduction compared to the previous state-of-the-art conditional
entropy model.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Conditional entropy models effectively leverage spatio-temporal contexts to reduce video redundancy.</div>
</details>
</div>
<div class="card">
<div class="title">Goal-Based Vision-Language Driving</div>
<div class="meta-line">Authors: Santosh Patapati, Trisanth Srinivasan</div>
<div class="meta-line">First: 2025-07-30T19:12:42+00:00 · Latest: 2025-10-13T04:53:24+00:00</div>
<div class="meta-line">Comments: 6 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.23042v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.23042v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive&#x27;s
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations.</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Language Cross-Attention for Real-Time Autonomous Driving</div>
<div class="meta-line">Authors: Santosh Patapati, Trisanth Srinivasan, Murari Ambati</div>
<div class="meta-line">First: 2025-07-30T19:51:23+00:00 · Latest: 2025-10-13T04:30:09+00:00</div>
<div class="meta-line">Comments: 5 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.23064v3">Abs</a> · <a href="http://arxiv.org/pdf/2507.23064v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous cars need geometric accuracy and semantic understanding to
navigate complex environments, yet most stacks handle them separately. We
present XYZ-Drive, a single vision-language model that reads a front-camera
frame, a 25m $\times$ 25m overhead map, and the next waypoint, then outputs
steering and speed. A lightweight goal-centered cross-attention layer lets
waypoint tokens highlight relevant image and map patches, supporting both
action and textual explanations, before the fused tokens enter a partially
fine-tuned LLaMA-3.2 11B model. On the MD-NEX Outdoor-Driving benchmark
XYZ-Drive attains 95% success and 0.80 Success weighted by Path Length (SPL),
surpassing PhysNav-DG by 15%. and halving collisions, all while significantly
improving efficiency by using only a single branch. Sixteen ablations explain
the gains. Removing any modality (vision, waypoint, map) drops success by up to
11%, confirming their complementary roles and rich connections. Replacing
goal-centered attention with simple concatenation cuts 3% in performance,
showing query-based fusion injects map knowledge more effectively. Keeping the
transformer frozen loses 5%, showing the importance of fine-tuning when
applying VLMs for specific tasks such as autonomous driving. Coarsening map
resolution from 10 cm to 40 cm blurs lane edges and raises crash rate. Overall,
these results demonstrate that early, token-level fusion of intent and map
layout enables accurate, transparent, real-time driving.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autonomous cars need geometric accuracy and semantic understanding to navigate complex environments, yet most stacks handle them separately.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
