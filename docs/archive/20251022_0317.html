<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-22 03:17</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251022_0317</div>
    <div class="row"><div class="card">
<div class="title">ConsistEdit: Highly Consistent and Precise Training-free Visual Editing</div>
<div class="meta-line">Authors: Zixin Yin, Ling-Hao Chen, Lionel Ni, Xili Dai</div>
<div class="meta-line">Venue: SIGGRAPH Asia 2025</div>
<div class="meta-line">First: 2025-10-20T17:59:52+00:00 · Latest: 2025-10-20T17:59:52+00:00</div>
<div class="meta-line">Comments: SIGGRAPH Asia 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17803v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17803v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in training-free attention control methods have enabled flexible and efficient text-guided editing capabilities for existing generation models.</div>
</details>
</div>
<div class="card">
<div class="title">Glyph: Scaling Context Windows via Visual-Text Compression</div>
<div class="meta-line">Authors: Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, Yushi Bai, Jie Tang, Hongning Wang, Minlie Huang</div>
<div class="meta-line">First: 2025-10-20T17:58:56+00:00 · Latest: 2025-10-20T17:58:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17800v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17800v1">PDF</a> · <a href="https://github.com/thu-coai/Glyph">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning.</div>
</details>
</div>
<div class="card">
<div class="title">SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference</div>
<div class="meta-line">Authors: Samir Khaki, Junxian Guo, Jiaming Tang, Shang Yang, Yukang Chen, Konstantinos N. Plataniotis, Yao Lu, Song Han, Zhijian Liu</div>
<div class="meta-line">First: 2025-10-20T17:35:47+00:00 · Latest: 2025-10-20T17:35:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17777v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17777v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation.</div>
</details>
</div>
<div class="card">
<div class="title">UniCTokens: Boosting Personalized Understanding and Generation via   Unified Concept Tokens</div>
<div class="meta-line">Authors: Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, Bocheng Zou, Chaoqun Yang, Wentao Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-20T17:56:01+00:00 · Latest: 2025-10-20T16:56:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.14671v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.14671v3">PDF</a> · <a href="https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens">Code1</a> · <a href="https://github.com/arctanxarc/UniCTokens">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Personalized models have demonstrated remarkable success in understanding and
generating concepts provided by users. However, existing methods use separate
concept tokens for understanding and generation, treating these tasks in
isolation. This may result in limitations for generating images with complex
prompts. For example, given the concept $\langle bo\rangle$, generating
&quot;$\langle bo\rangle$ wearing its hat&quot; without additional textual descriptions
of its hat. We call this kind of generation \textit{\textbf{personalized
attribute-reasoning generation}}. To address the limitation, we present
UniCTokens, a novel framework that effectively integrates personalized
information into a unified vision language model (VLM) for understanding and
generation. UniCTokens trains a set of unified concept tokens to leverage
complementary semantics, boosting two personalized tasks. Moreover, we propose
a progressive training strategy with three stages: understanding warm-up,
bootstrapping generation from understanding, and deepening understanding from
generation to enhance mutual benefits between both tasks. To quantitatively
evaluate the unified VLM personalization, we present UnifyBench, the first
benchmark for assessing concept understanding, concept generation, and
attribute-reasoning generation. Experimental results on UnifyBench indicate
that UniCTokens shows competitive performance compared to leading methods in
concept understanding, concept generation, and achieving state-of-the-art
results in personalized attribute-reasoning generation. Our research
demonstrates that enhanced understanding improves generation, and the
generation process can yield valuable insights into understanding. Our code and
dataset will be released at:
\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Personalized models have demonstrated remarkable success in understanding and generating concepts provided by users.</div>
</details>
</div>
<div class="card">
<div class="title">NanoHTNet: Nano Human Topology Network for Efficient 3D Human Pose   Estimation</div>
<div class="meta-line">Authors: Jialun Cai, Mengyuan Liu, Hong Liu, Shuheng Zhou, Wenhao Li</div>
<div class="meta-line">Venue: IEEE Transactions on Image Processing, vol. 34, pp. 6655-6668,
  2025</div>
<div class="meta-line">First: 2025-01-27T04:16:42+00:00 · Latest: 2025-10-20T15:53:26+00:00</div>
<div class="meta-line">Comments: Accepted by TIP 2025, Open Sourced</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.15763v2">Abs</a> · <a href="http://arxiv.org/pdf/2501.15763v2">PDF</a> · <a href="https://github.com/vefalun/NanoHTNet">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread application of 3D human pose estimation (HPE) is limited by
resource-constrained edge devices, requiring more efficient models. A key
approach to enhancing efficiency involves designing networks based on the
structural characteristics of input data. However, effectively utilizing the
structural priors in human skeletal inputs remains challenging. To address
this, we leverage both explicit and implicit spatio-temporal priors of the
human body through innovative model design and a pre-training proxy task.
First, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE
network with stacked Hierarchical Mixers to capture explicit features.
Specifically, the spatial Hierarchical Mixer efficiently learns the human
physical topology across multiple semantic levels, while the temporal
Hierarchical Mixer with discrete cosine transform and low-pass filtering
captures local instantaneous movements and global action coherence. Moreover,
Efficient Temporal-Spatial Tokenization (ETST) is introduced to enhance
spatio-temporal interaction and reduce computational complexity significantly.
Second, PoseCLR is proposed as a general pre-training method based on
contrastive learning for 3D HPE, aimed at extracting implicit representations
of human topology. By aligning 2D poses from diverse viewpoints in the proxy
task, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing
the high-dimensional features of the human body, leading to further performance
improvements. Extensive experiments verify that NanoHTNet with PoseCLR
outperforms other state-of-the-art methods in efficiency, making it ideal for
deployment on edge devices like the Jetson Nano. Code and models are available
at https://github.com/vefalun/NanoHTNet.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The widespread application of 3D human pose estimation (HPE) is limited by resource-constrained edge devices, requiring more efficient models.</div>
</details>
</div>
<div class="card">
<div class="title">Market-Driven Subset Selection for Budgeted Training</div>
<div class="meta-line">Authors: Ashish Jha, Valentin Leplat, AH Phan</div>
<div class="meta-line">First: 2025-10-02T18:12:03+00:00 · Latest: 2025-10-20T15:38:47+00:00</div>
<div class="meta-line">Comments: Retitled major revision of the same work (formerly &quot;Market-Based Data
  Subset Selection -- Principled Aggregation of Multi-Criteria Example
  Utility&quot;). Abstract and exposition revised; ablations added; theory
  clarified. Core results unchanged. Supersedes v1; please process as a
  replacement</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.02456v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.02456v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training large language models on massive datasets is computationally
expensive, yet empirical evidence suggests that substantial portions of
training examples contribute minimally to final performance. Data subset
selection addresses this inefficiency by identifying small, high-utility
subsets under resource constraints. However, example utility is inherently
multi-faceted, encompassing uncertainty, distributional rarity, and diversity
signals that are heterogeneous and typically combined through ad hoc weighted
sums lacking theoretical grounding. We propose a market-based framework that
treats each training example as a tradeable contract and employs the
Logarithmic Market Scoring Rule to aggregate multiple utility signals into
coherent prices. Heterogeneous signals act as traders, a single liquidity
parameter controls concentration versus smoothing, and topic-wise normalization
ensures calibrated aggregation. Token budgets are handled explicitly through a
price-per-token decision rule with an interpretable length-bias parameter. We
establish theoretical connections to maximum-entropy aggregation and provide
utility recovery guarantees under noisy but monotone signals. On GSM8K
mathematical reasoning under strict 60k-token budgets, our selector achieves
parity with strong single-signal baselines while exhibiting lower variance and
incurring less than 0.1 GPU-hour overhead. On AGNews classification at 5-25\%
retention rates, the market formulation delivers competitive accuracy with
improved stability. Our framework unifies multi-signal data curation under
fixed computational budgets for prompt-level reasoning and classification
tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Training large language models on massive datasets is computationally expensive, yet empirical evidence suggests that substantial portions of training examples contribute minimally to final performance.</div>
</details>
</div>
<div class="card">
<div class="title">Limitations of Normalization in Attention Mechanism</div>
<div class="meta-line">Authors: Timur Mudarisov, Mikhail Burtsev, Tatiana Petrova, Radu State</div>
<div class="meta-line">First: 2025-08-25T09:25:05+00:00 · Latest: 2025-10-20T15:30:30+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.17821v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.17821v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates the limitations of the normalization in attention
mechanisms. We begin with a theoretical framework that enables the
identification of the model&#x27;s selective ability and the geometric separation
involved in token selection. Our analysis includes explicit bounds on distances
and separation criteria for token vectors under softmax scaling. Through
experiments with pre-trained GPT-2 model, we empirically validate our
theoretical results and analyze key behaviors of the attention mechanism.
Notably, we demonstrate that as the number of selected tokens increases, the
model&#x27;s ability to distinguish informative tokens declines, often converging
toward a uniform selection pattern. We also show that gradient sensitivity
under softmax normalization presents challenges during training, especially at
low temperature settings. These findings advance current understanding of
softmax-based attention mechanism and motivate the need for more robust
normalization and selection strategies in future attention architectures.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the limitations of the normalization in attention mechanisms.</div>
</details>
</div>
<div class="card">
<div class="title">ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data   Augmentation for Robust Lung Ultrasound Classification</div>
<div class="meta-line">Authors: Athanasios Angelakis, Amne Mousa, Micah L. A. Heldeweg, Laurens A. Biesheuvel, Mark A. Haaksma, Jasper M. Smit, Pieter R. Tuinman, Paul W. G. Elbers</div>
<div class="meta-line">First: 2025-10-20T15:26:38+00:00 · Latest: 2025-10-20T15:26:38+00:00</div>
<div class="meta-line">Comments: 14 pages, 6 figures, 2 tables. Primary subject: cs.LG (Machine
  Learning) Cross-listed to: cs.CV (Computer Vision and Pattern Recognition),
  eess.IV (Image and Video Processing). Code available at:
  https://github.com/Bluesman79/ZACH-ViT Installation: pip install zachvit
  Paper licensed under CC BY-NC-ND 4.0. Code released under Apache 2.0 License</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17650v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17650v1">PDF</a> · <a href="https://github.com/Bluesman79/ZACH-ViT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and structurally normal lungs in lung ultrasound (LUS) videos remains challenging due to the high visual variability of non-cardiogenic inflammatory patterns (NCIP/ARDS-like), interstitial lung disease, and healthy lungs.</div>
</details>
</div>
<div class="card">
<div class="title">Wavy Transformer</div>
<div class="meta-line">Authors: Satoshi Noguchi, Yoshinobu Kawahara</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-08-18T10:03:38+00:00 · Latest: 2025-10-20T15:22:31+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.12787v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.12787v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers have achieved remarkable success across natural language
processing (NLP) and computer vision (CV). However, deep transformer models
often suffer from an over-smoothing issue, in which token representations
converge to similar values as they pass through successive transformer blocks.
In this paper, we establish an equivalence between the hidden-state dynamics
induced by stacked attention layers and graph neural diffusion on a complete
graph. From this perspective, over-smoothing can be interpreted as a
consequence of the dissipative nature of the underlying diffusion dynamics.
Motivated by this physical interpretation, we propose Wavy Transformer, which
consists of a novel attention layer based on second-order wavy dynamics. We
also introduce a feed-forward network and a normalization layer designed to
preserve the physical state-velocity relationship under the chain rule, thereby
extending the transformer architecture. We further validate our proposed
techniques on various transformer models for NLP and CV tasks. The results
consistently demonstrate that Wavy Transformer improves performance with
minimal additional parameters and no extra hyperparameter tuning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformers have achieved remarkable success across natural language processing (NLP) and computer vision (CV).</div>
</details>
</div>
<div class="card">
<div class="title">OG-Rank: Learning to Rank Fast and Slow with Uncertainty and   Reward-Trend Guided Adaptive Exploration</div>
<div class="meta-line">Authors: Praphul Singh, Corey Barrett, Sumana Srivasta, Irfan Bulu, Sri Gadde, Krishnaram Kenthapadi</div>
<div class="meta-line">First: 2025-10-20T15:00:02+00:00 · Latest: 2025-10-20T15:00:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17614v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17614v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Clinicians need ranking systems that work in real time and still justify their choices.</div>
</details>
</div>
<div class="card">
<div class="title">From Next Token Prediction to (STRIPS) World Models -- Preliminary   Results</div>
<div class="meta-line">Authors: Carlos Núñez-Molina, Vicenç Gómez, Hector Geffner</div>
<div class="meta-line">First: 2025-09-16T14:03:58+00:00 · Latest: 2025-10-20T14:57:16+00:00</div>
<div class="meta-line">Comments: 10 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.13389v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.13389v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We consider the problem of learning propositional STRIPS world models from action traces alone, using a deep learning architecture (transformers) and gradient descent.</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Distillation and Structural Alignment for Improved Code   Generation</div>
<div class="meta-line">Authors: Amir Jalilifard, Anderson de Rezende Rocha, Marcos Medeiros Raimundo</div>
<div class="meta-line">First: 2025-10-20T14:47:47+00:00 · Latest: 2025-10-20T14:47:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17598v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17598v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Effective code generation with language models hinges on two critical factors: accurately understanding the intent of the prompt and generating code that applies algorithmic reasoning to produce correct solutions capable of passing diverse test cases while adhering to the syntax of the target programming language.</div>
</details>
</div>
<div class="card">
<div class="title">HGAdapter: Hypergraph-based Adapters in Language Models for Code   Summarization and Clone Detection</div>
<div class="meta-line">Authors: Guang Yang, Yujie Zhu</div>
<div class="meta-line">Venue: EMNLP 2025 long</div>
<div class="meta-line">First: 2025-10-20T14:41:28+00:00 · Latest: 2025-10-20T14:41:28+00:00</div>
<div class="meta-line">Comments: Accepted by the 2025 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2025) as a findings long paper</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17591v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17591v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Pre-trained language models (PLMs) are increasingly being applied to code-related tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Does Math Reasoning Improve General LLM Capabilities? Understanding   Transferability of LLM Reasoning</div>
<div class="meta-line">Authors: Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, Xiang Yue</div>
<div class="meta-line">First: 2025-07-01T05:23:05+00:00 · Latest: 2025-10-20T14:27:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.00432v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.00432v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME.</div>
</details>
</div>
<div class="card">
<div class="title">RPG: A Repository Planning Graph for Unified and Scalable Codebase   Generation</div>
<div class="meta-line">Authors: Jane Luo, Xin Zhang, Steven Liu, Jie Wu, Jianfeng Liu, Yiming Huang, Yangyu Huang, Chengyu Yin, Ying Xin, Yuefeng Zhan, Hao Sun, Qi Chen, Scarlett Li, Mao Yang</div>
<div class="meta-line">First: 2025-09-19T17:58:14+00:00 · Latest: 2025-10-20T14:22:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.16198v5">Abs</a> · <a href="http://arxiv.org/pdf/2509.16198v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models excel at generating individual functions or single
files of code, yet generating complete repositories from scratch remains a
fundamental challenge. This capability is key to building coherent software
systems from high-level specifications and realizing the full potential of
automated code generation. The process requires planning at two levels:
deciding what features and modules to build (proposal stage) and defining their
implementation details (implementation stage). Current approaches rely on
natural language planning, which often produces unclear specifications,
misaligned components, and brittle designs due to its inherent ambiguity and
lack of structure. To address these limitations, we introduce the Repository
Planning Graph (RPG), a structured representation that encodes capabilities,
file structures, data flows, and functions in a unified graph. By replacing
free-form natural language with an explicit blueprint, RPG enables consistent
long-horizon planning for repository generation. Building on RPG, we develop
ZeroRepo, a graph-driven framework that operates in three stages:
proposal-level planning, implementation-level construction, and graph-guided
code generation with test validation. To evaluate, we construct RepoCraft, a
benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo
produces nearly 36K Code Lines and 445K Code Tokens, on average 3.9$\times$
larger than the strongest baseline (Claude Code), and 68$\times$ larger than
other baselines. It achieves 81.5% coverage and 69.7% test accuracy, improving
over Claude Code by 27.3 and 35.8 points. Further analysis shows that RPG
models complex dependencies, enables more sophisticated planning through
near-linear scaling, and improves agent understanding of repositories, thus
accelerating localization.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models excel at generating individual functions or single files of code, yet generating complete repositories from scratch remains a fundamental challenge.</div>
</details>
</div>
<div class="card">
<div class="title">FlexQuant: A Flexible and Efficient Dynamic Precision Switching   Framework for LLM Quantization</div>
<div class="meta-line">Authors: Fangxin Liu, Zongwu Wang, JinHong Xia, Junping Zhao, Shouren Zhao, Jinjin Li, Jian Liu, Li Jiang, Haibing Guan</div>
<div class="meta-line">First: 2025-05-21T07:42:53+00:00 · Latest: 2025-10-20T13:38:28+00:00</div>
<div class="meta-line">Comments: 1p pages, 7 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.12024v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.12024v2">PDF</a> · <a href="https://github.com/ZongwuWang/FlexQuant.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of large language models (LLMs) has exacerbated the
memory bottleneck due to the widening gap between model parameter scaling and
hardware capabilities. While post-training quantization techniques effectively
reduce memory overhead, existing methods predominantly rely on static
quantization strategies, which struggle to adapt to dynamic workloads. To
address this, we propose FlexQuant, a dynamic precision-switching framework
that optimizes the trade-off between inference speed and accuracy. Leveraging
model perplexity entropy and Kullback-Leibler divergence, FlexQuant enables
fine-grained, layer-wise mixed-precision quantization and dynamically adjusts
bit-widths during each token generation. FlexQuant provides a comprehensive
analysis of quantization strategies, introduces a precision requirement model
for optimal switching, and implements efficient fine-grained precision
management. Evaluations demonstrate that FlexQuant achieves a 1.3x end-to-end
speedup across diverse language tasks with negligible accuracy loss introduced.
This framework offers a flexible and adaptive solution for efficient LLM
deployment. Code is released at https://github.com/ZongwuWang/FlexQuant.git.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid advancement of large language models (LLMs) has exacerbated the memory bottleneck due to the widening gap between model parameter scaling and hardware capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph   Foundation Models</div>
<div class="meta-line">Authors: Xi Zhu, Haochen Xue, Ziwei Zhao, Wujiang Xu, Jingyuan Huang, Minghao Guo, Qifan Wang, Kaixiong Zhou, Imran Razzak, Yongfeng Zhang</div>
<div class="meta-line">First: 2025-03-05T09:45:22+00:00 · Latest: 2025-10-20T11:58:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.03313v3">Abs</a> · <a href="http://arxiv.org/pdf/2503.03313v3">PDF</a> · <a href="https://github.com/agiresearch/PromptGFM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-Attributed Graphs (TAGs), where each node is associated with text
descriptions, are ubiquitous in real-world scenarios. They typically exhibit
distinctive structure and domain-specific knowledge, motivating the development
of a Graph Foundation Model (GFM) that generalizes across diverse graphs and
tasks. Despite large efforts to integrate Large Language Models (LLMs) and
Graph Neural Networks (GNNs) for TAGs, existing approaches suffer from
decoupled architectures with two-stage alignment, limiting their synergistic
potential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens
to graph nodes, leading to graph-specific semantics, token explosion, and
incompatibility with task-oriented prompt templates, which hinders cross-graph
and cross-task transferability. To address these challenges, we propose
PromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.
PromptGFM comprises two key components: (1) Graph Understanding Module, which
explicitly prompts LLMs to replicate the finest GNN workflow within the text
space, facilitating seamless GNN-LLM integration and elegant graph-text
alignment; (2) Graph Inference Module, which establishes a language-based graph
vocabulary ensuring expressiveness, transferability, and scalability, enabling
readable instructions for LLM fine-tuning. Extensive experiments demonstrate
our superiority and transferability across diverse graphs and tasks. The code
is available at this: https://github.com/agiresearch/PromptGFM.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-Attributed Graphs (TAGs), where each node is associated with text descriptions, are ubiquitous in real-world scenarios.</div>
</details>
</div>
<div class="card">
<div class="title">CAPO: Towards Enhancing LLM Reasoning through Generative Credit   Assignment</div>
<div class="meta-line">Authors: Guofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, Xiao Zhang</div>
<div class="meta-line">First: 2025-08-04T11:06:08+00:00 · Latest: 2025-10-20T11:32:37+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.02298v4">Abs</a> · <a href="http://arxiv.org/pdf/2508.02298v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has improved the
reasoning abilities of Large Language Models (LLMs) by using rule-based binary
feedback. However, current RLVR methods typically assign the same reward to
every token. This coarse-grained feedback hampers precise credit assignment,
making it hard for models to identify which reasoning steps lead to success or
failure, and often results in suboptimal policies. Methods like PPO provide
credit assignment by value estimation, but yield inaccurate and unverifiable
signals due to limited sampling. On the other hand, methods using Process
Reward Models can provide step-wise rewards but suffer from several key
limitations: they require high-quality process supervision labels, the feedback
is unreliable due to probabilistic reward modeling, and their application in
online reinforcement learning (RL) is time-consuming. To overcome these
limitations, we introduce a simple but efficient method-Credit Assignment
Policy Optimization (CAPO). Instead of training auxiliary models, CAPO directly
leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward
Model (LLM-as-GenPRM) to generate all step-wise critique by one pass only based
on the correctness of the step itself, providing deterministic token-level
credits to refine the tokens that were originally assigned identical rule-based
rewards. To further enhance the accuracy and robustness, we employ voting
mechanisms that scale with the number of generated critiques. Extensive
experiments on various backbones like Llama and Qwen models show that CAPO
consistently outperforms supervised learning-based and RL-based fine-tuning
methods across four challenging mathematical benchmarks and three out-of-domain
benchmarks. Further analysis shows that CAPO can help the model to foster the
learning of correct reasoning pathways leading to correct answers.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback.</div>
</details>
</div>
<div class="card">
<div class="title">From Spatial to Actions: Grounding Vision-Language-Action Model in   Spatial Foundation Priors</div>
<div class="meta-line">Authors: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou</div>
<div class="meta-line">First: 2025-10-20T11:26:45+00:00 · Latest: 2025-10-20T11:26:45+00:00</div>
<div class="meta-line">Comments: Project page: https://falcon-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17439v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17439v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://falcon-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability.</div>
</details>
</div>
<div class="card">
<div class="title">Shaken or Stirred? An Analysis of MetaFormer&#x27;s Token Mixing for Medical   Imaging</div>
<div class="meta-line">Authors: Ron Keuth, Paul Kaftan, Mattias P. Heinrich</div>
<div class="meta-line">First: 2025-10-07T14:28:04+00:00 · Latest: 2025-10-20T11:01:47+00:00</div>
<div class="meta-line">Comments: Code and data:
  https://github.com/multimodallearning/MetaFormerMedImaging/tree/clean_code</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.05971v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.05971v2">PDF</a> · <a href="https://github.com/multimodallearning/MetaFormerMedImaging/tree/clean_code">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The generalization of the Transformer architecture via MetaFormer has
reshaped our understanding of its success in computer vision. By replacing
self-attention with simpler token mixers, MetaFormer provides strong baselines
for vision tasks. However, while extensively studied on natural image datasets,
its use in medical imaging remains scarce, and existing works rarely compare
different token mixers, potentially overlooking more suitable designs choices.
In this work, we present the first comprehensive study of token mixers for
medical imaging. We systematically analyze pooling-, convolution-, and
attention-based token mixers within the MetaFormer architecture on image
classification (global prediction task) and semantic segmentation (dense
prediction task). Our evaluation spans eight datasets covering diverse
modalities and common challenges in the medical domain. Given the prevalence of
pretraining from natural images to mitigate medical data scarcity, we also
examine transferring pretrained weights to new token mixers. Our results show
that, for classification, low-complexity token mixers (e.g. grouped convolution
or pooling) are sufficient, aligning with findings on natural images.
Pretrained weights remain useful despite the domain gap introduced by the new
token mixer. For segmentation, we find that the local inductive bias of
convolutional token mixers is essential. Grouped convolutions emerge as the
preferred choice, as they reduce runtime and parameter count compared to
standard convolutions, while the MetaFormer&#x27;s channel-MLPs already provide the
necessary cross-channel interactions.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The generalization of the Transformer architecture via MetaFormer has reshaped our understanding of its success in computer vision.</div>
</details>
</div>
<div class="card">
<div class="title">Recurrent Attention-based Token Selection for Efficient Streaming   Video-LLMs</div>
<div class="meta-line">Authors: Vaggelis Dorovatas, Soroush Seifi, Gunshi Gupta, Rahaf Aljundi</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-20T10:04:49+00:00 · Latest: 2025-10-20T10:04:49+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17364v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17364v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video Large Language Models (Video-LLMs) excel at understanding videos in-context, provided they have full access to the video when answering queries.</div>
</details>
</div>
<div class="card">
<div class="title">CEReBrO: Compact Encoder for Representations of Brain Oscillations Using   Efficient Alternating Attention</div>
<div class="meta-line">Authors: Alexandru Dimofte, Glenn Anta Bucagu, Thorir Mar Ingolfsson, Xiaying Wang, Andrea Cossettini, Luca Benini, Yawei Li</div>
<div class="meta-line">First: 2025-01-18T21:44:38+00:00 · Latest: 2025-10-20T09:42:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.10885v4">Abs</a> · <a href="http://arxiv.org/pdf/2501.10885v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Electroencephalograph (EEG) is a crucial tool for studying brain activity.
Recently, self-supervised learning methods leveraging large unlabeled datasets
have emerged as a potential solution to the scarcity of widely available
annotated EEG data. However, current methods suffer from at least one of the
following limitations: i) sub-optimal EEG signal modeling, ii) model sizes in
the hundreds of millions of trainable parameters, and iii) reliance on private
datasets and/or inconsistent public benchmarks, hindering reproducibility. To
address these challenges, we introduce a Compact Encoder for Representations of
Brain Oscillations using alternating attention (CEReBrO), a new small EEG
foundation model. Our tokenization scheme represents EEG signals at a
per-channel patch granularity. We propose an alternating attention mechanism
that jointly models intra-channel temporal dynamics and inter-channel spatial
correlations, achieving 2x speed improvement with 6x less memory required
compared to standard self-attention. We present several model sizes ranging
from 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of
publicly available scalp EEG recordings with diverse channel configurations,
our models set new benchmarks in emotion detection and seizure detection tasks,
with competitive performance in anomaly classification and gait prediction.
This validates our models&#x27; effectiveness and efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Electroencephalograph (EEG) is a crucial tool for studying brain activity.</div>
</details>
</div>
<div class="card">
<div class="title">Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference   Optimization</div>
<div class="meta-line">Authors: Jingfeng Guo, Jian Liu, Jinnan Chen, Shiwei Mao, Changrong Hu, Puhua Jiang, Junlin Yu, Jing Xu, Qi Liu, Lixin Xu, Zhuo Chen, Chunchao Guo</div>
<div class="meta-line">First: 2025-06-13T03:06:52+00:00 · Latest: 2025-10-20T07:54:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.11430v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.11430v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Auto-Connect, a novel approach for automatic rigging that
explicitly preserves skeletal connectivity through a connectivity-preserving
tokenization scheme. Unlike previous methods that predict bone positions
represented as two joints or first predict points before determining
connectivity, our method employs special tokens to define endpoints for each
joint&#x27;s children and for each hierarchical layer, effectively automating
connectivity relationships. This approach significantly enhances topological
accuracy by integrating connectivity information directly into the prediction
framework. To further guarantee high-quality topology, we implement a
topology-aware reward function that quantifies topological correctness, which
is then utilized in a post-training phase through reward-guided Direct
Preference Optimization. Additionally, we incorporate implicit geodesic
features for latent top-k bone selection, which substantially improves skinning
quality. By leveraging geodesic distance information within the model&#x27;s latent
space, our approach intelligently determines the most influential bones for
each vertex, effectively mitigating common skinning artifacts. This combination
of connectivity-preserving tokenization, reward-guided fine-tuning, and
geodesic-aware bone selection enables our model to consistently generate more
anatomically plausible skeletal structures with superior deformation
properties.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce Auto-Connect, a novel approach for automatic rigging that explicitly preserves skeletal connectivity through a connectivity-preserving tokenization scheme.</div>
</details>
</div>
<div class="card">
<div class="title">Grounding Language with Vision: A Conditional Mutual Information   Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs</div>
<div class="meta-line">Authors: Hao Fang, Changle Zhou, Jiawei Kong, Kuofeng Gao, Bin Chen, Tao Liang, Guojun Ma, Shu-Tao Xia</div>
<div class="meta-line">First: 2025-05-26T08:36:10+00:00 · Latest: 2025-10-20T06:44:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.19678v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.19678v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where
generated responses seem semantically plausible yet exhibit little or no
relevance to the input image. Previous studies reveal that this issue primarily
stems from LVLMs&#x27; over-reliance on language priors while disregarding the
visual information during decoding. To alleviate this issue, we introduce a
novel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding
strategy, which adaptively strengthens the mutual dependency between generated
texts and input images to mitigate hallucinations. Unlike existing methods
solely focusing on text token sampling, we propose to jointly model the
contributions of visual and textual tokens to C-PMI, formulating hallucination
mitigation as a bi-level optimization problem aimed at maximizing mutual
information. To solve it, we design a token purification mechanism that
dynamically regulates the decoding process by sampling text tokens remaining
maximally relevant to the given image, while simultaneously refining image
tokens most pertinent to the generated response. Extensive experiments across
various benchmarks reveal that the proposed method significantly reduces
hallucinations in LVLMs while preserving decoding efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where generated responses seem semantically plausible yet exhibit little or no relevance to the input image.</div>
</details>
</div>
<div class="card">
<div class="title">Soft-Masked Diffusion Language Models</div>
<div class="meta-line">Authors: Michael Hersche, Samuel Moor-Smith, Thomas Hofmann, Abbas Rahimi</div>
<div class="meta-line">First: 2025-10-20T06:42:03+00:00 · Latest: 2025-10-20T06:42:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17206v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17206v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion models have demonstrated strong potential in language modeling, offering various advantages over traditional autoregressive approaches.</div>
</details>
</div>
<div class="card">
<div class="title">$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal   Dynamics for Efficient Multimodal LLMs</div>
<div class="meta-line">Authors: Yingqi Fan, Anhao Zhao, Jinlan Fu, Junlong Tong, Hui Su, Yijie Pan, Wei Zhang, Xiaoyu Shen</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-10-20T06:40:17+00:00 · Latest: 2025-10-20T06:40:17+00:00</div>
<div class="meta-line">Comments: EMNLP 2025 Main</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17205v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17205v1">PDF</a> · <a href="https://github.com/EIT-NLP/VisiPruner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have achieved strong performance across vision-language tasks, but suffer from significant computational overhead due to the quadratic growth of attention computations with the number of multimodal tokens.</div>
</details>
</div>
<div class="card">
<div class="title">NFIG: Multi-Scale Autoregressive Image Generation via Frequency Ordering</div>
<div class="meta-line">Authors: Zhihao Huang, Xi Qiu, Yukuo Ma, Yifu Zhou, Junjie Chen, Hongyuan Zhang, Chi Zhang, Xuelong Li</div>
<div class="meta-line">First: 2025-03-10T08:59:10+00:00 · Latest: 2025-10-20T06:31:52+00:00</div>
<div class="meta-line">Comments: 10 pages, 7 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.07076v5">Abs</a> · <a href="http://arxiv.org/pdf/2503.07076v5">PDF</a> · <a href="https://github.com/Pride-Huang/NFIG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive models have achieved significant success in image generation.
However, unlike the inherent hierarchical structure of image information in the
spectral domain, standard autoregressive methods typically generate pixels
sequentially in a fixed spatial order. To better leverage this spectral
hierarchy, we introduce NextFrequency Image Generation (NFIG). NFIG is a novel
framework that decomposes the image generation process into multiple
frequency-guided stages. NFIG aligns the generation process with the natural
image structure. It does this by first generating low-frequency components,
which efficiently capture global structure with significantly fewer tokens, and
then progressively adding higher-frequency details. This frequency-aware
paradigm offers substantial advantages: it not only improves the quality of
generated images but crucially reduces inference cost by efficiently
establishing global structure early on. Extensive experiments on the
ImageNet-256 benchmark validate NFIG&#x27;s effectiveness, demonstrating superior
performance (FID: 2.81) and a notable 1.25x speedup compared to the strong
baseline VAR-d20. The source code is available at
https://github.com/Pride-Huang/NFIG.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive models have achieved significant success in image generation.</div>
</details>
</div>
<div class="card">
<div class="title">ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language   Models</div>
<div class="meta-line">Authors: Pu Zhang, Yuwei Li, Xingyuan Xian, Guoming Tang</div>
<div class="meta-line">First: 2025-10-20T06:18:47+00:00 · Latest: 2025-10-20T06:18:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17197v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17197v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As the capabilities of Vision-Language Models (VLMs) advance, they can process increasingly large inputs, which, unlike in LLMs, generates significant visual token redundancy and leads to prohibitive inference costs.</div>
</details>
</div>
<div class="card">
<div class="title">Understanding and Improving Length Generalization in Hierarchical Sparse   Attention Models</div>
<div class="meta-line">Authors: Jiaqi Leng, Xiang Hu, Junxiong Wang, Jianguo Li, Wei Wu, Yucheng Lu</div>
<div class="meta-line">First: 2025-10-20T06:17:57+00:00 · Latest: 2025-10-20T06:17:57+00:00</div>
<div class="meta-line">Comments: Preprint. Work in progress</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.17196v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.17196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Effectively processing long contexts is a critical challenge for language models.</div>
</details>
</div>
<div class="card">
<div class="title">Unlocking the Power of Mixture-of-Experts for Task-Aware Time Series   Analytics</div>
<div class="meta-line">Authors: Xingjian Wu, Zhengyu Li, Hanyin Cheng, Xiangfei Qiu, Jilin Hu, Chenjuan Guo, Bin Yang</div>
<div class="meta-line">First: 2025-09-26T12:44:46+00:00 · Latest: 2025-10-20T06:08:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.22279v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.22279v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time Series Analysis is widely used in various real-world applications such
as weather forecasting, financial fraud detection, imputation for missing data
in IoT systems, and classification for action recognization. Mixture-of-Experts
(MoE), as a powerful architecture, though demonstrating effectiveness in NLP,
still falls short in adapting to versatile tasks in time series analytics due
to its task-agnostic router and the lack of capability in modeling channel
correlations. In this study, we propose a novel, general MoE-based time series
framework called PatchMoE to support the intricate ``knowledge&#x27;&#x27; utilization
for distinct tasks, thus task-aware. Based on the observation that hierarchical
representations often vary across tasks, e.g., forecasting vs. classification,
we propose a Recurrent Noisy Gating to utilize the hierarchical information in
routing, thus obtaining task-sepcific capability. And the routing strategy is
operated on time series tokens in both temporal and channel dimensions, and
encouraged by a meticulously designed Temporal \&amp; Channel Load Balancing Loss
to model the intricate temporal and channel correlations. Comprehensive
experiments on five downstream tasks demonstrate the state-of-the-art
performance of PatchMoE.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Time Series Analysis is widely used in various real-world applications such as weather forecasting, financial fraud detection, imputation for missing data in IoT systems, and classification for action recognization.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
