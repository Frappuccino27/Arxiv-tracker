<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-21 03:14</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251021_0314</div>
    <div class="row"><div class="card">
<div class="title">OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding   LLM</div>
<div class="meta-line">Authors: Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, Yuming Lou, Dong Yang, Zhijian Liu, Yukang Chen, Ambrish Dantrey, Ehsan Jahangiri, Sreyan Ghosh, Daguang Xu, Ehsan Hosseini-Asl, Danial Mohseni Taheri, Vidya Murali, Sifei Liu, Jason Lu, Oluwatobi Olabiyi, Frank Wang, Rafael Valle, Bryan Catanzaro, Andrew Tao, Song Han, Jan Kautz, Hongxu Yin, Pavlo Molchanov</div>
<div class="meta-line">First: 2025-10-17T17:59:59+00:00 · Latest: 2025-10-17T17:59:59+00:00</div>
<div class="meta-line">Comments: Technical Report. Code: https://github.com/NVlabs/OmniVinci</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15870v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15870v1">PDF</a> · <a href="https://github.com/NVlabs/OmniVinci">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni&#x27;s 1.2T. We finally demonstrate omni-modal advantages in downstream
applications spanning robotics, medical AI, and smart factory.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world.</div>
</details>
</div>
<div class="card">
<div class="title">BLIP3o-NEXT: Next Frontier of Native Image Generation</div>
<div class="meta-line">Authors: Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu</div>
<div class="meta-line">First: 2025-10-17T17:50:58+00:00 · Latest: 2025-10-17T17:50:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15857v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15857v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation. BLIP3o-NEXT
unifies text-to-image generation and image editing within a single
architecture, demonstrating strong image generation and image editing
capabilities. In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance. Building upon these insights, BLIP3o-NEXT leverages an
Autoregressive + Diffusion architecture in which an autoregressive model first
generates discrete image tokens conditioned on multimodal inputs, whose hidden
states are then used as conditioning signals for a diffusion model to generate
high-fidelity images. This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.
Extensive evaluations of various text-to-image and image-editing benchmarks
show that BLIP3o-NEXT achieves superior performance over existing models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation.</div>
</details>
</div>
<div class="card">
<div class="title">MegaScale-MoE: Large-Scale Communication-Efficient Training of   Mixture-of-Experts Models in Production</div>
<div class="meta-line">Authors: Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu</div>
<div class="meta-line">First: 2025-05-16T16:52:16+00:00 · Latest: 2025-10-17T17:03:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.11432v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.11432v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present MegaScale-MoE, a production system tailored for the efficient
training of large-scale mixture-of-experts (MoE) models. MoE emerges as a
promising architecture to scale large language models (LLMs) to unprecedented
sizes, thereby enhancing model performance. However, existing MoE training
systems experience a degradation in training efficiency, exacerbated by the
escalating scale of MoE models and the continuous evolution of hardware.
  Recognizing the pivotal role of efficient communication in enhancing MoE
training, MegaScale-MoE customizes communication-efficient parallelism
strategies for attention and FFNs in each MoE layer and adopts a holistic
approach to overlap communication with computation at both inter- and
intra-operator levels. Additionally, MegaScale-MoE applies communication
compression with adjusted communication patterns to lower precision, further
improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA
Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,
improving the efficiency by 1.88$\times$ compared to Megatron-LM. We share our
operational experience in accelerating MoE training and hope that by offering
our insights in system design, this work will motivate future research in MoE
systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models.</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect   Verifiers</div>
<div class="meta-line">Authors: Xin-Qiang Cai, Wei Wang, Feng Liu, Tongliang Liu, Gang Niu, Masashi Sugiyama</div>
<div class="meta-line">First: 2025-10-01T13:56:44+00:00 · Latest: 2025-10-17T16:20:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.00915v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.00915v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) trains policies against
automated verifiers to avoid costly human labeling. To reduce vulnerability to
verifier hacking, many RLVR systems collapse rewards to binary $\{0,1\}$ during
training. This choice carries a cost: it introduces \textit{false negatives}
(rejecting correct answers, FNs) and \textit{false positives} (accepting
incorrect ones, FPs). For instance, a rule-based checker may mark the correct
fraction $\frac{12}{36}$ as wrong when compared against the canonical
$\frac{1}{3}$ due to brittle parsing/equivalence rules (FN), while a large
language model (LLM) judges can be gamed by superficial cues or even a single
adversarial token, yielding inflated correctness for wrong solutions (FP). We
formalize verifier unreliability by modeling the verifier as a stochastic
reward channel with asymmetric noise rates. From this abstraction, we derive
two correction algorithms for verifier errors. The first is a \textit{backward}
correction that de-biases the observed binary reward to recover an
\textit{unbiased} estimator of the clean policy gradient. The second is a
\textit{forward} correction that reweights score-function terms so that the
expected update direction aligns with the \textit{clean gradient}; notably, it
requires only the FN rate. We implement both as lightweight hooks in a group
relative policy optimization (GRPO)-based RLVR pipeline and evaluate them on
math-reasoning models and benchmarks. Across models and datasets, both
corrections improve over uncorrected training; the forward variant converges
faster and remains stable under heavier noise. Finally, we show a practical
appeal mechanism in which a lightweight LLM verifier estimates the FN rate
online by rechecking rule-based negatives, obtaining outperformance compared
with other state-of-the-art contenders.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) trains policies against automated verifiers to avoid costly human labeling.</div>
</details>
</div>
<div class="card">
<div class="title">Retro3D: A 3D-aware Template-free Method for Enhancing Retrosynthesis   via Molecular Conformer Information</div>
<div class="meta-line">Authors: Jiaxi Zhuang, Yu Zhang, Yan Zhang, Ying Qian, Aimin Zhou</div>
<div class="meta-line">First: 2025-01-21T18:54:16+00:00 · Latest: 2025-10-17T15:39:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2501.12434v2">Abs</a> · <a href="http://arxiv.org/pdf/2501.12434v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrosynthesis plays a crucial role in the fields of organic synthesis and
drug development, where the goal is to identify suitable reactants that can
yield a target product molecule. Although existing methods have achieved
notable success, they typically overlook the 3D conformational details and
internal spatial organization of molecules. This oversight makes it challenging
to predict reactants that conform to genuine chemical principles, particularly
when dealing with complex molecular structures, such as polycyclic and
heteroaromatic compounds. In response to this challenge, we introduce a novel
transformer-based, template-free approach that incorporates 3D conformer data
and spatial information. Our approach includes an Atom-align Fusion module that
integrates 3D positional data at the input stage, ensuring correct alignment
between atom tokens and their respective 3D coordinates. Additionally, we
propose a Distance-weighted Attention mechanism that refines the self-attention
process, constricting the model s focus to relevant atom pairs in 3D space.
Extensive experiments on the USPTO-50K dataset demonstrate that our model
outperforms previous template-free methods, setting a new benchmark for the
field. A case study further highlights our method s ability to predict
reasonable and accurate reactants.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Retrosynthesis plays a crucial role in the fields of organic synthesis and drug development, where the goal is to identify suitable reactants that can yield a target product molecule.</div>
</details>
</div>
<div class="card">
<div class="title">Attention Sinks in Diffusion Language Models</div>
<div class="meta-line">Authors: Maximo Eduardo Rulli, Simone Petruzzi, Edoardo Michielon, Fabrizio Silvestri, Simone Scardapane, Alessio Devoto</div>
<div class="meta-line">First: 2025-10-17T15:23:58+00:00 · Latest: 2025-10-17T15:23:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15731v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15731v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked Diffusion Language Models (DLMs) have recently emerged as a promising
alternative to traditional Autoregressive Models (ARMs). DLMs employ
transformer encoders with bidirectional attention, enabling parallel token
generation while maintaining competitive performance. Although their efficiency
and effectiveness have been extensively studied, the internal mechanisms that
govern DLMs remain largely unexplored. In this work, we conduct an empirical
analysis of DLM attention patterns, focusing on the attention sinking
phenomenon, an effect previously observed in various transformer-based
architectures. Our findings reveal that DLMs also exhibit attention sinks, but
with distinct characteristics. First, unlike in ARMs, the sink positions in
DLMs tend to shift throughout the generation process, displaying a dynamic
behaviour. Second, while ARMs are highly sensitive to the removal of attention
sinks, DLMs remain robust: masking sinks leads to only a minor degradation in
performance. These results provide new insights into the inner workings of
diffusion-based language models and highlight fundamental differences in how
they allocate and utilize attention compared to autoregressive models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs).</div>
</details>
</div>
<div class="card">
<div class="title">CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical   Contrastive Decoding</div>
<div class="meta-line">Authors: Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho</div>
<div class="meta-line">First: 2025-09-27T16:01:09+00:00 · Latest: 2025-10-17T14:59:53+00:00</div>
<div class="meta-line">Comments: Preprint, 27 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.23379v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.23379v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have recently achieved remarkable
progress in radiology by integrating visual perception with natural language
understanding. However, they often generate clinically unsupported
descriptions, known as medical hallucinations, which pose serious risks in
medical applications that demand accuracy and image-grounded outputs. Through
empirical analysis, we find that prompt-induced hallucinations remain prevalent
in radiology MLLMs, largely due to over-sensitivity to clinical sections. To
address this, we introduce Clinical Contrastive Decoding (CCD), a training-free
and retrieval-free inference framework that integrates structured clinical
signals from task-specific radiology expert models. CCD introduces a dual-stage
contrastive mechanism to refine token-level logits during generation, thereby
enhancing clinical fidelity without modifying the base MLLM. Experiments on
three datasets and multiple models demonstrate that CCD consistently improves
overall performance on radiology report generation (RRG). On the MIMIC-CXR
dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to
state-of-the-art RRG models. Our approach provides a lightweight and
generalisable solution for mitigating medical hallucinations, effectively
bridging expert models and MLLMs in radiology.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding.</div>
</details>
</div>
<div class="card">
<div class="title">Expanding the Action Space of LLMs to Reason Beyond Language</div>
<div class="meta-line">Authors: Zhongqi Yue, Weishi Wang, Yundaichuan Zhan, Juncheng Li, Daniel Dahlmeier, Fredrik D. Johansson</div>
<div class="meta-line">First: 2025-10-08T21:56:58+00:00 · Latest: 2025-10-17T13:22:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.07581v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.07581v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments -- such as symbolic operators
or simulators -- must be expressed through text in predefined formats, parsed,
and routed to external interfaces. This overloads the model&#x27;s language with
both reasoning and control duties, and requires a hand-crafted parser, external
to the LLM. To address this, we decouple environment interactions from language
by internalizing them in an Expanded Action space (ExpA), beyond the
vocabulary. The model starts reasoning in the default language environment, but
may trigger routing actions and switch to an external environment at any time.
From there, the model can only invoke environment-specific actions, receive
feedback from the environment, and potentially route back to language as a
result. To promote effective exploration of the expanded action space and new
environments, we introduce ExpA Reinforcement Learning (EARL) with
counterfactual policy optimization. On tasks requiring multi-turn interactions
and contingent planning, EARL outperforms strong baselines with
vocabulary-constrained actions. It performs robustly across calculator-based
multi-task learning and, in the partially observed sorting problem, achieves
perfect Sort-4 accuracy while self-discovering an efficient algorithm
competitive with classical designs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) are powerful reasoners in natural language, but their actions are typically confined to outputting vocabulary tokens.</div>
</details>
</div>
<div class="card">
<div class="title">TokenTiming: A Dynamic Alignment Method for Universal Speculative   Decoding Model Pairs</div>
<div class="meta-line">Authors: Sibo Xiao, Jinyuan Fu, Zhongle Xie, Lidan Shou</div>
<div class="meta-line">First: 2025-10-17T11:25:36+00:00 · Latest: 2025-10-17T11:25:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15545v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15545v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accelerating the inference of large language models (LLMs) has been a
critical challenge in generative AI. Speculative decoding (SD) substantially
improves LLM inference efficiency. However, its utility is limited by a
fundamental constraint: the draft and target models must share the same
vocabulary, thus limiting the herd of available draft models and often
necessitating the training of a new model from scratch. Inspired by Dynamic
Time Warping (DTW), a classic algorithm for aligning time series, we propose
the algorithm TokenTiming for universal speculative decoding. It operates by
re-encoding the draft token sequence to get a new target token sequence, and
then uses DTW to build a mapping to transfer the probability distributions for
speculative sampling. Benefiting from this, our method accommodates mismatched
vocabularies and works with any off-the-shelf models without retraining and
modification. We conduct comprehensive experiments on various tasks,
demonstrating 1.57x speedup. This work enables a universal approach for draft
model selection, making SD a more versatile and practical tool for LLM
acceleration.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accelerating the inference of large language models (LLMs) has been a critical challenge in generative AI.</div>
</details>
</div>
<div class="card">
<div class="title">InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced   Language Models</div>
<div class="meta-line">Authors: Wenjun Wang, Shuo Cai, Congkai Xie, Mingfa Feng, Yiming Zhang, Zhen Li, Kejing Yang, Ming Li, Jiannong Cao, Hongxia Yang</div>
<div class="meta-line">First: 2025-09-26T16:16:49+00:00 · Latest: 2025-10-17T10:54:44+00:00</div>
<div class="meta-line">Comments: This paper has been withdrawn by the authors due to a significant bug
  discovered in our data processing pipeline. This bug affects the validity of
  the experimental results, and we can no longer stand by the conclusions
  presented</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.22536v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.22536v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation.</div>
</details>
</div>
<div class="card">
<div class="title">Selecting and Combining Large Language Models for Scalable Code Clone   Detection</div>
<div class="meta-line">Authors: Muslim Chochlov, Gul Aftab Ahmed, James Vincent Patten, Yuanhua Han, Guoxian Lu, David Gregg, Jim Buckley</div>
<div class="meta-line">First: 2025-10-17T09:51:17+00:00 · Latest: 2025-10-17T09:51:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15480v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15480v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Source code clones pose risks ranging from intellectual property violations
to unintended vulnerabilities. Effective and efficient scalable clone
detection, especially for diverged clones, remains challenging. Large language
models (LLMs) have recently been applied to clone detection tasks. However, the
rapid emergence of LLMs raises questions about optimal model selection and
potential LLM-ensemble efficacy.
  This paper addresses the first question by identifying 76 LLMs and filtering
them down to suitable candidates for large-scale clone detection. The
candidates were evaluated on two public industrial datasets, BigCloneBench, and
a commercial large-scale dataset. No uniformly &#x27;best-LLM&#x27; emerged, though
CodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates
suggested that smaller embedding sizes, smaller tokenizer vocabularies and
tailored datasets are advantageous. On commercial large-scale dataset a
top-performing CodeT5+110M achieved 39.71\% precision: twice the precision of
previously used CodeBERT.
  To address the second question, this paper explores ensembling of the
selected LLMs: effort-effective approach to improving effectiveness. Results
suggest the importance of score normalization and favoring ensembling methods
like maximum or sum over averaging. Also, findings indicate that ensembling
approach can be statistically significant and effective on larger datasets: the
best-performing ensemble achieved even higher precision of 46.91\% over
individual LLM on the commercial large-scale code.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Source code clones pose risks ranging from intellectual property violations to unintended vulnerabilities.</div>
</details>
</div>
<div class="card">
<div class="title">NFIG: Autoregressive Image Generation with Next-Frequency Prediction</div>
<div class="meta-line">Authors: Zhihao Huang, Xi Qiu, Yukuo Ma, Yifu Zhou, Junjie Chen, Hongyuan Zhang, Chi Zhang, Xuelong Li</div>
<div class="meta-line">First: 2025-03-10T08:59:10+00:00 · Latest: 2025-10-17T09:25:08+00:00</div>
<div class="meta-line">Comments: 10 pages, 7 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.07076v4">Abs</a> · <a href="http://arxiv.org/pdf/2503.07076v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive models have achieved promising results in natural language
processing. However, for image generation tasks, they encounter substantial
challenges in effectively capturing long-range dependencies, managing
computational costs, and most crucially, defining meaningful autoregressive
sequences that reflect natural image hierarchies. To address these issues, we
present \textbf{N}ext-\textbf{F}requency \textbf{I}mage \textbf{G}eneration
(\textbf{NFIG}), a novel framework that decomposes the image generation process
into multiple frequency-guided stages. Our approach first generates
low-frequency components to establish global structure with fewer tokens, then
progressively adds higher-frequency details, following the natural spectral
hierarchy of images. This principled autoregressive sequence not only improves
the quality of generated images by better capturing true causal relationships
between image components, but also significantly reduces computational overhead
during inference. Extensive experiments demonstrate that NFIG achieves
state-of-the-art performance with fewer steps, offering a more efficient
solution for image generation, with 1.25$\times$ speedup compared to VAR-d20
while achieving better performance (FID: 2.81) on the ImageNet-256 benchmark.
We hope that our insight of incorporating frequency-domain knowledge to guide
autoregressive sequence design will shed light on future research. We will make
our code publicly available upon acceptance of the paper.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive models have achieved promising results in natural language processing.</div>
</details>
</div>
<div class="card">
<div class="title">FPEdit: Robust LLM Fingerprinting through Localized Parameter Editing</div>
<div class="meta-line">Authors: Shida Wang, Chaohu Liu, Yubo Wang, Linli Xu</div>
<div class="meta-line">First: 2025-08-04T06:00:22+00:00 · Latest: 2025-10-17T08:53:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.02092v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.02092v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models represent significant investments in computation, data,
and engineering expertise, making them extraordinarily valuable intellectual
assets. Nevertheless, these AI assets remain vulnerable to unauthorized
redistribution and commercial exploitation through fine-tuning or black-box
deployment. Current fingerprinting approaches face a fundamental trade-off:
intrinsic methods require full parameter access, while backdoor-based
techniques employ statistically anomalous triggers easily detected and filtered
by adversaries. To address these limitations, we introduce FPEdit, a novel
framework that leverages knowledge editing to inject semantically coherent
natural language fingerprints through sparse, targeted modifications to model
weights. Our approach introduces Promote-Suppress Value Vector Optimization,
which simultaneously enhances target token likelihood while suppressing
competing tokens, ensuring robust fingerprint integration without degrading
core model functionality. Extensive experiments show that FPEdit achieves
95-100% fingerprint retention under both full-parameter fine-tuning and
parameter-efficient adaptation, while preserving performance on downstream
benchmarks. Moreover, FPEdit remains robust under quantization, pruning, and
stochastic decoding, and can embed 10 fingerprint pairs into LLaMA2-7B in under
2 minutes using less than 30 GB of GPU memory, which represents a substantial
reduction in resource requirements. These advances establish FPEdit as the
first fingerprinting approach to simultaneously achieve robustness against
adaptation, resistance to detection, and preservation of model utility, thereby
providing a minimally invasive solution for reliable provenance verification of
large language models in adversarial deployment scenarios.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models represent significant investments in computation, data, and engineering expertise, making them extraordinarily valuable intellectual assets.</div>
</details>
</div>
<div class="card">
<div class="title">First-order State Space Model for Lightweight Image Super-resolution</div>
<div class="meta-line">Authors: Yujie Zhu, Xinyi Zhang, Yekai Lu, Guang Yang, Faming Fang, Guixu Zhang</div>
<div class="meta-line">Venue: ICASSP 2025 Oral</div>
<div class="meta-line">First: 2025-09-10T10:00:43+00:00 · Latest: 2025-10-17T08:22:00+00:00</div>
<div class="meta-line">Comments: Accept by ICASSP 2025 (Oral)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.08458v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.08458v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State space models (SSMs), particularly Mamba, have shown promise in NLP
tasks and are increasingly applied to vision tasks. However, most Mamba-based
vision models focus on network architecture and scan paths, with little
attention to the SSM module. In order to explore the potential of SSMs, we
modified the calculation process of SSM without increasing the number of
parameters to improve the performance on lightweight super-resolution tasks. In
this paper, we introduce the First-order State Space Model (FSSM) to improve
the original Mamba module, enhancing performance by incorporating token
correlations. We apply a first-order hold condition in SSMs, derive the new
discretized form, and analyzed cumulative error. Extensive experimental results
demonstrate that FSSM improves the performance of MambaIR on five benchmark
datasets without additionally increasing the number of parameters, and
surpasses current lightweight SR methods, achieving state-of-the-art results.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">State space models (SSMs), particularly Mamba, have shown promise in NLP tasks and are increasingly applied to vision tasks.</div>
</details>
</div>
<div class="card">
<div class="title">What Layers When: Learning to Skip Compute in LLMs with Residual Gates</div>
<div class="meta-line">Authors: Filipe Laitenberger, Dawid Kopiczko, Cees G. M. Snoek, Yuki M. Asano</div>
<div class="meta-line">First: 2025-10-13T16:31:50+00:00 · Latest: 2025-10-17T07:30:17+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13876v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.13876v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch&#x27;s output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15% compute while
retaining over 90% of baseline accuracy. For increasingly larger models, this
tradeoff improves drastically. On instruction-tuned models we see accuracy
gains at full compute and match baseline quality near 50% savings. The learned
gates give insight into transformer information flow (e.g., BOS tokens act as
anchors), and the method combines easily with quantization, pruning, and
self-speculative decoding.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce GateSkip, a simple residual-stream gating mechanism that enables token-wise layer skipping in decoder-only LMs.</div>
</details>
</div>
<div class="card">
<div class="title">Towards Flash Thinking via Decoupled Advantage Policy Optimization</div>
<div class="meta-line">Authors: Zezhong Tan, Hang Gao, Xinhong Ma, Feng Zhang, Ziqiang Dong</div>
<div class="meta-line">First: 2025-10-17T07:19:20+00:00 · Latest: 2025-10-17T07:19:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15374v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15374v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent Large Reasoning Models (LRMs) have achieved remarkable performance in
solving complex problems via supervised fine-tuning (SFT) and reinforcement
learning (RL). Although existing RL algorithms significantly enhance model
accuracy, they still suffer from excessively lengthy responses and overthinking
issues, resulting in increased inference latency and computational consumption,
especially for simple tasks that require minimal reasoning. To address this, we
propose a novel RL framework, DEPO, to reduce inefficient reasoning for models.
Our method mainly consists of three core components: (1) an innovative
advantage decoupled algorithm to guide model reduction of inefficient tokens;
(2) a difficulty-aware length penalty to lower the overall length of model
responses; (3) an advantage clipping method to prevent bias in policy
optimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and
DeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant
reduction in sequence length by 39% and reduces excessive reasoning paths in
inefficient tokens, while outperforming the base model in overall accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent Large Reasoning Models (LRMs) have achieved remarkable performance in solving complex problems via supervised fine-tuning (SFT) and reinforcement learning (RL).</div>
</details>
</div>
<div class="card">
<div class="title">Where MLLMs Attend and What They Rely On: Explaining Autoregressive   Token Generation</div>
<div class="meta-line">Authors: Ruoyu Chen, Xiaoqing Guo, Kangwei Liu, Siyuan Liang, Shiming Liu, Qunli Zhang, Hua Zhang, Xiaochun Cao</div>
<div class="meta-line">First: 2025-09-26T15:38:42+00:00 · Latest: 2025-10-17T07:14:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.22496v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.22496v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ruoyuchen10.github.io/EAGLE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in aligning visual inputs with natural language outputs. Yet, the
extent to which generated tokens depend on visual modalities remains poorly
understood, limiting interpretability and reliability. In this work, we present
EAGLE, a lightweight black-box framework for explaining autoregressive token
generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual
regions while quantifying the relative influence of language priors and
perceptual evidence. The framework introduces an objective function that
unifies sufficiency (insight score) and indispensability (necessity score),
optimized via greedy search over sparsified image regions for faithful and
efficient attribution. Beyond spatial attribution, EAGLE performs
modality-aware analysis that disentangles what tokens rely on, providing
fine-grained interpretability of model decisions. Extensive experiments across
open-source MLLMs show that EAGLE consistently outperforms existing methods in
faithfulness, localization, and hallucination diagnosis, while requiring
substantially less GPU memory. These results highlight its effectiveness and
practicality for advancing the interpretability of MLLMs. The code will be
released at https://ruoyuchen10.github.io/EAGLE/.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs.</div>
</details>
</div>
<div class="card">
<div class="title">CAIT: Triple-Win Compression towards High Accuracy, Fast Inference, and   Favorable Transferability For ViTs</div>
<div class="meta-line">Authors: Ao Wang, Hui Chen, Zijia Lin, Sicheng Zhao, Jungong Han, Guiguang Ding</div>
<div class="meta-line">First: 2023-09-27T16:12:07+00:00 · Latest: 2025-10-17T07:11:59+00:00</div>
<div class="meta-line">Comments: TPAMI 2025 Camera-ready Version</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2309.15755v2">Abs</a> · <a href="http://arxiv.org/pdf/2309.15755v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers (ViTs) have emerged as state-of-the-art models for
various vision tasks recently. However, their heavy computation costs remain
daunting for resource-limited devices. To address this, researchers have
dedicated themselves to compressing redundant information in ViTs for
acceleration. However, existing approaches generally sparsely drop redundant
image tokens by token pruning or brutally remove channels by channel pruning,
leading to a sub-optimal balance between model performance and inference speed.
Moreover, they struggle when transferring compressed models to downstream
vision tasks that require the spatial structure of images, such as semantic
segmentation. To tackle these issues, we propose CAIT, a joint
\underline{c}ompression method for ViTs that achieves a harmonious blend of
high \underline{a}ccuracy, fast \underline{i}nference speed, and favorable
\underline{t}ransferability to downstream tasks. Specifically, we introduce an
asymmetric token merging (ATME) strategy to effectively integrate neighboring
tokens. It can successfully compress redundant token information while
preserving the spatial structure of images. On top of it, we further design a
consistent dynamic channel pruning (CDCP) strategy to dynamically prune
unimportant channels in ViTs. Thanks to CDCP, insignificant channels in
multi-head self-attention modules of ViTs can be pruned uniformly,
significantly enhancing the model compression. Extensive experiments on
multiple benchmark datasets show that our proposed method can achieve
state-of-the-art performance across various ViTs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision Transformers (ViTs) have emerged as state-of-the-art models for various vision tasks recently.</div>
</details>
</div>
<div class="card">
<div class="title">PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following   Models Need for Efficient Generation</div>
<div class="meta-line">Authors: Ao Wang, Hui Chen, Jiaxin Li, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Zijia Lin, Jungong Han, Guiguang Ding</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-12-04T15:48:59+00:00 · Latest: 2025-10-17T06:54:10+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Camera-ready Version</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.03409v4">Abs</a> · <a href="http://arxiv.org/pdf/2412.03409v4">PDF</a> · <a href="https://github.com/THU-MIG/PrefixKV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, large vision-language models (LVLMs) have rapidly gained popularity
for their strong generation and reasoning capabilities given diverse multimodal
inputs. However, these models incur significant computational and memory
overhead during inference, which greatly hinders the efficient deployment in
practical scenarios. The extensive key-value (KV) cache, necessitated by the
lengthy input and output sequences, notably contributes to the high inference
cost. Based on this, recent works have investigated ways to reduce the KV cache
size for higher efficiency. Although effective, they generally overlook the
distinct importance distributions of KV vectors across layers and maintain the
same cache size for each layer during the next token prediction. This results
in the significant contextual information loss for certain layers, leading to
notable performance decline. To address this, we present PrefixKV, where
&quot;Prefix&quot; means the top-ranked KV based on importance rather than position in
the original sequence. It reframes the challenge of determining KV cache sizes
for all layers into the task of searching for the optimal global prefix
configuration. With an adaptive layer-wise KV retention recipe based on binary
search, the maximum contextual information can thus be preserved in each layer,
facilitating the generation. Extensive experiments demonstrate that our method
achieves the state-of-the-art performance compared with others. It exhibits
superior inference efficiency and generation quality trade-offs, showing
promising potential for practical applications. Code is available at
https://github.com/THU-MIG/PrefixKV.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs.</div>
</details>
</div>
<div class="card">
<div class="title">ACON: Optimizing Context Compression for Long-horizon LLM Agents</div>
<div class="meta-line">Authors: Minki Kang, Wei-Ning Chen, Dongge Han, Huseyin A. Inan, Lukas Wutschitz, Yanzhi Chen, Robert Sim, Saravan Rajmohan</div>
<div class="meta-line">First: 2025-10-01T07:43:49+00:00 · Latest: 2025-10-17T06:48:23+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.00615v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.00615v2">PDF</a> · <a href="https://github.com/microsoft/acon">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed as agents in dynamic,
real-world environments, where success requires both reasoning and effective
tool use. A central challenge for agentic tasks is the growing context length,
as agents must accumulate long histories of actions and observations. This
expansion raises costs and reduces efficiency in long-horizon tasks, yet prior
work on context compression has mostly focused on single-step tasks or narrow
applications. We introduce Agent Context Optimization (ACON), a unified
framework that optimally compresses both environment observations and
interaction histories into concise yet informative condensations. ACON
leverages compression guideline optimization in natural language space: given
paired trajectories where full context succeeds but compressed context fails,
capable LLMs analyze the causes of failure, and the compression guideline is
updated accordingly. Furthermore, we propose distilling the optimized LLM
compressor into smaller models to reduce the overhead of the additional module.
Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON
reduces memory usage by 26-54% (peak tokens) while largely preserving task
performance, preserves over 95% of accuracy when distilled into smaller
compressors, and enhances smaller LMs as long-horizon agents with up to 46%
performance improvement. Our code is available at
https://github.com/microsoft/acon.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use.</div>
</details>
</div>
<div class="card">
<div class="title">Thinking Augmented Pre-training</div>
<div class="meta-line">Authors: Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei</div>
<div class="meta-line">First: 2025-09-24T14:45:13+00:00 · Latest: 2025-10-17T06:22:11+00:00</div>
<div class="meta-line">Comments: 19 pages; v4 fixes an issue for HumanEval scores</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.20186v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.20186v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a simple and scalable approach to improve the data
efficiency of large language model (LLM) training by augmenting existing text
data with thinking trajectories. The compute for pre-training LLMs has been
growing at an unprecedented rate, while the availability of high-quality data
remains limited. Consequently, maximizing the utility of available data
constitutes a significant research challenge. A primary impediment is that
certain high-quality tokens are difficult to learn given a fixed model
capacity, as the underlying rationale for a single token can be exceptionally
complex and deep. To address this issue, we propose Thinking augmented
Pre-Training (TPT), a universal methodology that augments text with
automatically generated thinking trajectories. Such augmentation effectively
increases the volume of the training data and makes high-quality tokens more
learnable through step-by-step reasoning and decomposition. We apply TPT across
diverse training configurations up to $100$B tokens, encompassing pre-training
with both constrained and abundant data, as well as mid-training from strong
open-source checkpoints. Experimental results indicate that our method
substantially improves the performance of LLMs across various model sizes and
families. Notably, TPT enhances the data efficiency of LLM pre-training by a
factor of $3$. For a $3$B parameter model, it improves the post-training
performance by over $10\%$ on several challenging reasoning benchmarks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories.</div>
</details>
</div>
<div class="card">
<div class="title">When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM   Ensembling</div>
<div class="meta-line">Authors: Heecheol Yun, Kwangmin Ki, Junghyun Lee, Eunho Yang</div>
<div class="meta-line">First: 2025-10-17T06:18:29+00:00 · Latest: 2025-10-17T06:18:29+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15346v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15346v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensembling Large Language Models (LLMs) has gained attention as a promising
approach to surpass the performance of individual models by leveraging their
complementary strengths. In particular, aggregating models&#x27; next-token
probability distributions to select the next token has been shown to be
effective in various tasks. However, while successful for short-form answers,
its application to long-form generation remains underexplored. In this paper,
we show that using existing ensemble methods in long-form generation requires a
careful choice of ensembling positions, since the standard practice of
ensembling at every token often degrades performance. We identify two key
factors for determining these positions: tokenization mismatch across models
and consensus in their next-token probability distributions. Based on this, we
propose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively
ensembles by jointly considering these factors. To further improve stability,
we introduce a probability sharpening strategy that consolidates probabilities
spread across multiple sub-word tokens representing the same word into a single
representative token. Our experiments on diverse benchmarks, including MATH500
and BBH, demonstrate that SAFE outperforms existing methods in both accuracy
and efficiency, with gains achieved even when ensembling fewer than 1% of
tokens.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Ensembling Large Language Models (LLMs) has gained attention as a promising approach to surpass the performance of individual models by leveraging their complementary strengths.</div>
</details>
</div>
<div class="card">
<div class="title">VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</div>
<div class="meta-line">Authors: Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen</div>
<div class="meta-line">First: 2025-09-01T01:45:18+00:00 · Latest: 2025-10-17T06:09:17+00:00</div>
<div class="meta-line">Comments: 32 pages, 5 figures, 13 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.01055v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.01055v3">PDF</a> · <a href="https://github.com/TIGER-AI-Lab/verl-tool">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated
success in enhancing LLM reasoning capabilities, but remains limited to
single-turn interactions without tool integration. While recent Agentic
Reinforcement Learning with Tool use (ARLT) approaches have emerged to address
multi-turn tool interactions, existing works develop task-specific codebases
that suffer from fragmentation, synchronous execution bottlenecks, and limited
extensibility across domains. These inefficiencies hinder broader community
adoption and algorithmic innovation. We introduce VerlTool, a unified and
modular framework that addresses these limitations through systematic design
principles. VerlTool provides four key contributions: (1) upstream alignment
with VeRL ensuring compatibility and simplified maintenance, (2) unified tool
management via standardized APIs supporting diverse modalities including code
execution, search, SQL databases, and vision processing, (3) asynchronous
rollout execution achieving near 2$\times$ speedup by eliminating
synchronization bottlenecks, and (4) comprehensive evaluation demonstrating
competitive performance across 6 ARLT domains. Our framework formalizes ARLT as
multi-turn trajectories with multi-modal observation tokens (text/image/video),
extending beyond single-turn RLVR paradigms. We train and evaluate models on
mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web
search, and software engineering tasks, achieving results comparable to
specialized systems while providing unified training infrastructure. The
modular plugin architecture enables rapid tool integration requiring only
lightweight Python definitions, significantly reducing development overhead and
providing a scalable foundation for tool-augmented RL research. Our code is
open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration.</div>
</details>
</div>
<div class="card">
<div class="title">BeLLMan: Controlling LLM Congestion</div>
<div class="meta-line">Authors: Tella Rajashekhar Reddy, Atharva Deshmukh, Karan Tandon, Rohan Gandhi, Anjaly Parayil, Debopam Bhattacherjee</div>
<div class="meta-line">First: 2025-10-17T05:36:42+00:00 · Latest: 2025-10-17T05:36:42+00:00</div>
<div class="meta-line">Comments: To be presented at FAISYS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15330v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15330v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language model (LLM) applications are blindfolded to the infrastructure underneath and generate tokens autoregressively, indifferent to the system load, thus risking inferencing latency inflation and poor user experience.</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Centric Activation and Coordination for Multimodal Large Language   Models</div>
<div class="meta-line">Authors: Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin</div>
<div class="meta-line">First: 2025-10-16T06:38:39+00:00 · Latest: 2025-10-17T05:32:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14349v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.14349v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) integrate image features from visual encoders with LLMs, demonstrating advanced comprehension capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">DSSmoothing: Toward Certified Dataset Ownership Verification for   Pre-trained Language Models via Dual-Space Smoothing</div>
<div class="meta-line">Authors: Ting Qiao, Xing Liu, Wenke Huang, Jianbin Li, Zhaoxin Fan, Yiming Li</div>
<div class="meta-line">First: 2025-10-17T04:25:32+00:00 · Latest: 2025-10-17T04:25:32+00:00</div>
<div class="meta-line">Comments: 13 pages, 21 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15303v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15303v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large web-scale datasets have driven the rapid advancement of pre-trained
language models (PLMs), but unauthorized data usage has raised serious
copyright concerns. Existing dataset ownership verification (DOV) methods
typically assume that watermarks remain stable during inference; however, this
assumption often fails under natural noise and adversary-crafted perturbations.
We propose the first certified dataset ownership verification method for PLMs
based on dual-space smoothing (i.e., DSSmoothing). To address the challenges of
text discreteness and semantic sensitivity, DSSmoothing introduces continuous
perturbations in the embedding space to capture semantic robustness and applies
controlled token reordering in the permutation space to capture sequential
robustness. DSSmoothing consists of two stages: in the first stage, triggers
are collaboratively embedded in both spaces to generate norm-constrained and
robust watermarked datasets; in the second stage, randomized smoothing is
applied in both spaces during verification to compute the watermark robustness
(WR) of suspicious models and statistically compare it with the principal
probability (PP) values of a set of benign models. Theoretically, DSSmoothing
provides provable robustness guarantees for dataset ownership verification by
ensuring that WR consistently exceeds PP under bounded dual-space
perturbations. Extensive experiments on multiple representative web datasets
demonstrate that DSSmoothing achieves stable and reliable verification
performance and exhibits robustness against potential adaptive attacks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large web-scale datasets have driven the rapid advancement of pre-trained language models (PLMs), but unauthorized data usage has raised serious copyright concerns.</div>
</details>
</div>
<div class="card">
<div class="title">VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action   Expert Distillation</div>
<div class="meta-line">Authors: Shaoqi Dong, Chaoyou Fu, Haihan Gao, Yi-Fan Zhang, Chi Yan, Chu Wu, Xiaoyu Liu, Yunhang Shen, Jing Huo, Deqiang Jiang, Haoyu Cao, Yang Gao, Xing Sun, Ran He, Caifeng Shan</div>
<div class="meta-line">First: 2025-10-10T17:59:56+00:00 · Latest: 2025-10-17T04:19:59+00:00</div>
<div class="meta-line">Comments: Homepage: https://ltbai.github.io/VITA-VLA/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09607v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.09607v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ltbai.github.io/VITA-VLA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Action (VLA) models significantly advance robotic
manipulation by leveraging the strong perception capabilities of pretrained
vision-language models (VLMs). By integrating action modules into these
pretrained models, VLA methods exhibit improved generalization. However,
training them from scratch is costly. In this work, we propose a simple yet
effective distillation-based framework that equips VLMs with action-execution
capability by transferring knowledge from pretrained small action models. Our
architecture retains the original VLM structure, adding only an action token
and a state encoder to incorporate physical inputs. To distill action
knowledge, we adopt a two-stage training strategy. First, we perform
lightweight alignment by mapping VLM hidden states into the action space of the
small action model, enabling effective reuse of its pretrained action decoder
and avoiding expensive pretraining. Second, we selectively fine-tune the
language model, state encoder, and action modules, enabling the system to
integrate multimodal inputs with precise action generation. Specifically, the
action token provides the VLM with a direct handle for predicting future
actions, while the state encoder allows the model to incorporate robot dynamics
not captured by vision alone. This design yields substantial efficiency gains
over training large VLA models from scratch. Compared with previous
state-of-the-art methods, our method achieves 97.3% average success rate on
LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In
real-world experiments across five manipulation tasks, our method consistently
outperforms the teacher model, achieving 82.0% success rate (17% improvement),
which demonstrate that action distillation effectively enables VLMs to generate
precise actions while substantially reducing training costs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Action (VLA) models significantly advance robotic manipulation by leveraging the strong perception capabilities of pretrained vision-language models (VLMs).</div>
</details>
</div>
<div class="card">
<div class="title">MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for   Large-Scale Recommendation</div>
<div class="meta-line">Authors: Xianyang Qi, Yuan Tian, Zhaoyu Hu, Zhirui Kuai, Chang Liu, Hongxiang Lin, Lei Wang</div>
<div class="meta-line">First: 2025-10-17T03:50:09+00:00 · Latest: 2025-10-17T03:50:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15286v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.15286v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industrial recommender systems critically depend on high-quality ranking
models. However, traditional pipelines still rely on manual feature engineering
and scenario-specific architectures, which hinder cross-scenario transfer and
large-scale deployment. To address these challenges, we propose
\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with
Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt
integrates two key components. The \textbf{AutoToken} module automatically
clusters heterogeneous features into semantically coherent tokens, removing the
need for human-defined feature groups. The \textbf{MTmixAttBlock} module
enables efficient token interaction via a learnable mixing matrix, shared dense
experts, and scenario-aware sparse experts, capturing both global patterns and
scenario-specific behaviors within a single framework. Extensive experiments on
the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently
outperforms state-of-the-art baselines including Transformer-based models,
WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales,
MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields
further monotonic gains. Large-scale online A/B tests validate the real-world
impact: in the \textit{Homepage} scenario, MTmixAtt increases Payment PV by
\textbf{+3.62\%} and Actual Payment GTV by \textbf{+2.54\%}. Overall, MTmixAtt
provides a unified and scalable solution for modeling arbitrary heterogeneous
features across scenarios, significantly improving both user experience and
commercial outcomes.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Industrial recommender systems critically depend on high-quality ranking models.</div>
</details>
</div>
<div class="card">
<div class="title">Scope: Selective Cross-modal Orchestration of Visual Perception Experts</div>
<div class="meta-line">Authors: Tianyu Zhang, Suyuchen Wang, Chao Wang, Juan Rodriguez, Ahmed Masry, Xiangru Jian, Yoshua Bengio, Perouz Taslakian</div>
<div class="meta-line">First: 2025-10-14T20:33:01+00:00 · Latest: 2025-10-17T03:30:31+00:00</div>
<div class="meta-line">Comments: 14 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12974v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.12974v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) benefit from multiple vision encoders, but
naively stacking them yields diminishing returns while multiplying inference
costs. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that
dynamically selects one specialized encoder per image-text pair via
instance-level routing, unlike token-level routing in traditional MoE. SCOPE
maintains a shared encoder and a pool of routed encoders. A lightweight router
uses cross-attention between text prompts and shared visual features to select
the optimal encoder from the routed encoders. To train this router, we
introduce dual entropy regularization with auxiliary losses to balance
dataset-level load distribution with instance-level routing confidence.
Remarkably, SCOPE with one shared plus one routed encoder outperforms models
using all four extra encoders simultaneously, while reducing compute by
24-49\%. This demonstrates that intelligent encoder selection beats brute-force
aggregation, challenging the prevailing paradigm in multi-encoder VLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models (VLMs) benefit from multiple vision encoders, but naively stacking them yields diminishing returns while multiplying inference costs.</div>
</details>
</div>
<div class="card">
<div class="title">PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features</div>
<div class="meta-line">Authors: Wei Zou, Yupei Liu, Yanting Wang, Ying Chen, Neil Gong, Jinyuan Jia</div>
<div class="meta-line">First: 2025-10-15T18:34:49+00:00 · Latest: 2025-10-17T03:30:15+00:00</div>
<div class="meta-line">Comments: The code is available at https://github.com/weizou52/PIShield</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14005v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.14005v2">PDF</a> · <a href="https://github.com/weizou52/PIShield">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-integrated applications are vulnerable to prompt injection attacks, where
an attacker contaminates the input to inject malicious prompts, causing the LLM
to follow the attacker&#x27;s intent instead of the original user&#x27;s. Existing prompt
injection detection methods often have sub-optimal performance and/or high
computational overhead. In this work, we propose PIShield, a detection method
that is both effective and efficient. Our key observation is that the internal
representation of the final token in a prompt-extracted from a specific layer
of the LLM, which we term the injection-critical layer-captures distinguishing
features between clean and contaminated prompts. Leveraging this insight, we
train a simple linear classifier on these internal representations using a
labeled set of clean and contaminated prompts. We compare PIShield against 11
baselines across 5 diverse benchmark datasets and 8 prompt injection attacks.
The results demonstrate that PIShield is both highly effective and efficient,
substantially outperforming existing methods. Additionally, we show that
PIShield resists strong adaptive attacks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLM-integrated applications are vulnerable to prompt injection attacks, where an attacker contaminates the input to inject malicious prompts, causing the LLM to follow the attacker&#x27;s intent instead of the original user&#x27;s.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
