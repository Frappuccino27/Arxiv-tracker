<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-27 03:14</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251027_0314</div>
    <div class="row"><div class="card">
<div class="title">ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</div>
<div class="meta-line">Authors: Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-23T17:58:26+00:00 · Latest: 2025-10-23T17:58:26+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025, 18 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20803v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20803v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a novel AutoRegressive Generation-based paradigm for image
Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level
perception within a unified framework. Prior works integrating image
segmentation into multimodal large language models (MLLMs) typically employ
either boundary points representation or dedicated segmentation heads. These
methods rely on discrete representations or semantic prompts fed into
task-specific decoders, which limits the ability of the MLLM to capture
fine-grained visual details. To address these challenges, we introduce a
segmentation framework for MLLM based on image generation, which naturally
produces dense masks for target objects. We leverage MLLM to output visual
tokens and detokenize them into images using an universal VQ-VAE, making the
segmentation fully dependent on the pixel-level understanding of the MLLM. To
reduce inference latency, we employ a next-scale-prediction strategy to
generate required visual tokens in parallel. Extensive experiments demonstrate
that our method surpasses prior state-of-the-art approaches on multiple
segmentation datasets with a remarkable boost in inference speed, while
maintaining strong understanding capabilities.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework.</div>
</details>
</div>
<div class="card">
<div class="title">Simple Context Compression: Mean-Pooling and Multi-Ratio Training</div>
<div class="meta-line">Authors: Yair Feldman, Yoav Artzi</div>
<div class="meta-line">First: 2025-10-23T17:57:23+00:00 · Latest: 2025-10-23T17:57:23+00:00</div>
<div class="meta-line">Comments: Code available at
  https://github.com/lil-lab/simple-context-compression</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20797v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20797v1">PDF</a> · <a href="https://github.com/lil-lab/simple-context-compression">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A common strategy to reduce the computational costs of using long contexts in
retrieval-augmented generation (RAG) with large language models (LLMs) is soft
context compression, where the input sequence is transformed into a shorter
continuous representation. We develop a lightweight and simple mean-pooling
approach that consistently outperforms the widely used compression-tokens
architecture, and study training the same compressor to output multiple
compression ratios. We conduct extensive experiments across in-domain and
out-of-domain QA datasets, as well as across model families, scales, and
compression ratios. Overall, our simple mean-pooling approach achieves the
strongest performance, with a relatively small drop when training for multiple
compression ratios. More broadly though, across architectures and training
regimes the trade-offs are more nuanced, illustrating the complex landscape of
compression methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">A common strategy to reduce the computational costs of using long contexts in retrieval-augmented generation (RAG) with large language models (LLMs) is soft context compression, where the input sequence is transformed into a shorter continuous representation.</div>
</details>
</div>
<div class="card">
<div class="title">Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention   and Contextualized Learnable Token Eviction</div>
<div class="meta-line">Authors: Mutian He, Philip N. Garner</div>
<div class="meta-line">First: 2025-10-23T17:53:03+00:00 · Latest: 2025-10-23T17:53:03+00:00</div>
<div class="meta-line">Comments: 19 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20787v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20787v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linear-attention models that compress the entire input sequence into a
fixed-size recurrent state offer an efficient alternative to Transformers, but
their finite memory induces forgetfulness that harms retrieval-intensive tasks.
To mitigate the issue, we explore a series of hybrid models that restore direct
access to past tokens. We interleave token mixers with intermediate time and
space complexity between linear and full attention, including sparse attention
with token eviction, and the query-aware native sparse attention. Particularly,
we propose a novel learnable token eviction approach. Combined with
sliding-window attention, an end-to-end trainable lightweight CNN aggregates
information from both past and future adjacent tokens to adaptively retain a
limited set of critical KV-pairs per head, maintaining linear attention&#x27;s
constant time and space complexity. Efficient Triton kernels for the sparse
attention mechanisms are provided. Empirical evaluations on retrieval-intensive
benchmarks support the effectiveness of our approaches.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to Transformers, but their finite memory induces forgetfulness that harms retrieval-intensive tasks.</div>
</details>
</div>
<div class="card">
<div class="title">DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion</div>
<div class="meta-line">Authors: Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal</div>
<div class="meta-line">First: 2025-10-23T17:42:14+00:00 · Latest: 2025-10-23T17:42:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20766v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20766v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://noamissachar.github.io/DyPE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformer models can generate images with remarkable fidelity and
detail, yet training them at ultra-high resolutions remains extremely costly
due to the self-attention mechanism&#x27;s quadratic scaling with the number of
image tokens. In this paper, we introduce Dynamic Position Extrapolation
(DyPE), a novel, training-free method that enables pre-trained diffusion
transformers to synthesize images at resolutions far beyond their training
data, with no additional sampling cost. DyPE takes advantage of the spectral
progression inherent to the diffusion process, where low-frequency structures
converge early, while high-frequencies take more steps to resolve.
Specifically, DyPE dynamically adjusts the model&#x27;s positional encoding at each
diffusion step, matching their frequency spectrum with the current stage of the
generative process. This approach allows us to generate images at resolutions
that exceed the training resolution dramatically, e.g., 16 million pixels using
FLUX. On multiple benchmarks, DyPE consistently improves performance and
achieves state-of-the-art fidelity in ultra-high-resolution image generation,
with gains becoming even more pronounced at higher resolutions. Project page is
available at https://noamissachar.github.io/DyPE/.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism&#x27;s quadratic scaling with the number of image tokens.</div>
</details>
</div>
<div class="card">
<div class="title">Watermarking Autoregressive Image Generation</div>
<div class="meta-line">Authors: Nikola Jovanović, Ismail Labiad, Tomáš Souček, Martin Vechev, Pierre Fernandez</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-19T14:25:51+00:00 · Latest: 2025-10-23T17:33:59+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.16349v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.16349v2">PDF</a> · <a href="https://github.com/facebookresearch/wmar">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Watermarking the outputs of generative models has emerged as a promising
approach for tracking their provenance. Despite significant interest in
autoregressive image generation models and their potential for misuse, no prior
work has attempted to watermark their outputs at the token level. In this work,
we present the first such approach by adapting language model watermarking
techniques to this setting. We identify a key challenge: the lack of reverse
cycle-consistency (RCC), wherein re-tokenizing generated image tokens
significantly alters the token sequence, effectively erasing the watermark. To
address this and to make our method robust to common image transformations,
neural compression, and removal attacks, we introduce (i) a custom
tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a
complementary watermark synchronization layer. As our experiments demonstrate,
our approach enables reliable and robust watermark detection with theoretically
grounded p-values. Code and models are available at
https://github.com/facebookresearch/wmar.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance.</div>
</details>
</div>
<div class="card">
<div class="title">Thought Communication in Multiagent Collaboration</div>
<div class="meta-line">Authors: Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, Kun Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-23T16:48:02+00:00 · Latest: 2025-10-23T16:48:02+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20733v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Natural language has long enabled human cooperation, but its lossy,
ambiguous, and indirect nature limits the potential of collective intelligence.
While machines are not subject to these constraints, most LLM-based multi-agent
systems still rely solely on natural language, exchanging tokens or their
embeddings. To go beyond language, we introduce a new paradigm, thought
communication, which enables agents to interact directly mind-to-mind, akin to
telepathy. To uncover these latent thoughts in a principled way, we formalize
the process as a general latent variable model, where agent states are
generated by an unknown function of underlying thoughts. We prove that, in a
nonparametric setting without auxiliary information, both shared and private
latent thoughts between any pair of agents can be identified. Moreover, the
global structure of thought sharing, including which agents share which
thoughts and how these relationships are structured, can also be recovered with
theoretical guarantees. Guided by the established theory, we develop a
framework that extracts latent thoughts from all agents prior to communication
and assigns each agent the relevant thoughts, along with their sharing
patterns. This paradigm naturally extends beyond LLMs to all modalities, as
most observational data arise from hidden generative processes. Experiments on
both synthetic and real-world benchmarks validate the theory and demonstrate
the collaborative advantages of thought communication. We hope this work
illuminates the potential of leveraging the hidden world, as many challenges
remain unsolvable through surface-level observation alone, regardless of
compute or data scale.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence.</div>
</details>
</div>
<div class="card">
<div class="title">Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning</div>
<div class="meta-line">Authors: Wenyi Xiao, Leilei Gan</div>
<div class="meta-line">First: 2025-04-25T16:11:23+00:00 · Latest: 2025-10-23T16:25:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.18458v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.18458v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When applying reinforcement learning--typically through GRPO--to large
vision-language model reasoning struggles to effectively scale reasoning length
or generates verbose outputs across all tasks with only marginal gains in
accuracy. To address this issue, we present FAST-GRPO, a variant of GRPO that
dynamically adapts reasoning depth based on question characteristics. Through
empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs
by investigating how response length and data distribution affect performance.
Inspired by these observations, we introduce two complementary metrics to
estimate the difficulty of the questions, guiding the model to determine when
fast or slow thinking is more appropriate. Next, we incorporate adaptive
length-based rewards and difficulty-aware KL divergence into the GRPO
algorithm. Experiments across seven reasoning benchmarks demonstrate that FAST
achieves state-of-the-art accuracy with over 10\% relative improvement compared
to the base model, while reducing token usage by 32.7-67.3\% compared to
previous slow-thinking approaches, effectively balancing reasoning length and
accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">When applying reinforcement learning--typically through GRPO--to large vision-language model reasoning struggles to effectively scale reasoning length or generates verbose outputs across all tasks with only marginal gains in accuracy.</div>
</details>
</div>
<div class="card">
<div class="title">Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D   Medical Imaging</div>
<div class="meta-line">Authors: Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Dong Yang, Pengfei Guo, Marc Edgar, Daguang Xu, Bernhard Kainz, Bjoern Menze</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-23T15:13:13+00:00 · Latest: 2025-10-23T15:13:13+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20639v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20639v1">PDF</a> · <a href="https://github.com/ibrahimethemhamamci/BTB3D">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in vision-language modeling for 3D medical imaging has been
fueled by large-scale computed tomography (CT) corpora with paired free-text
reports, stronger architectures, and powerful pretrained models. This has
enabled applications such as automated report generation and text-conditioned
3D image synthesis. Yet, current approaches struggle with high-resolution,
long-sequence volumes: contrastive pretraining often yields vision encoders
that are misaligned with clinical language, and slice-wise tokenization blurs
fine anatomy, reducing diagnostic performance on downstream tasks. We introduce
BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder
that unifies 2D and 3D training and inference while producing compact,
frequency-aware volumetric tokens. A three-stage training curriculum enables
(i) local reconstruction, (ii) overlapping-window tiling, and (iii)
long-context decoder refinement, during which the model learns from short slice
excerpts yet generalizes to scans exceeding 300 slices without additional
memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it
improves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and
Merlin for report generation; and it reduces FID by 75% and halves FVD compared
to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically
consistent 512*512*241 volumes. These results confirm that precise
three-dimensional tokenization, rather than larger language backbones alone, is
essential for scalable vision-language modeling in 3D medical imaging. The
codebase is available at: https://github.com/ibrahimethemhamamci/BTB3D</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent progress in vision-language modeling for 3D medical imaging has been fueled by large-scale computed tomography (CT) corpora with paired free-text reports, stronger architectures, and powerful pretrained models.</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking GPT-5 for biomedical natural language processing</div>
<div class="meta-line">Authors: Yu Hou, Zaifu Zhan, Min Zeng, Yifan Wu, Shuang Zhou, Rui Zhang</div>
<div class="meta-line">First: 2025-08-28T13:06:53+00:00 · Latest: 2025-10-23T15:09:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.04462v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.04462v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Biomedical literature and clinical narratives pose multifaceted challenges
for natural language understanding, from precise entity extraction and document
synthesis to multi-step diagnostic reasoning. This study extends a unified
benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot
prompting across five core biomedical NLP tasks: named entity recognition,
relation extraction, multi-label document classification, summarization, and
simplification, and nine expanded biomedical QA datasets covering factual
knowledge, clinical reasoning, and multimodal visual understanding. Using
standardized prompts, fixed decoding parameters, and consistent inference
pipelines, we assessed model performance, latency, and token-normalized cost
under official pricing. GPT-5 consistently outperformed GPT-4o, with the
largest gains on reasoning-intensive datasets such as MedXpertQA and
DiagnosisArena and stable improvements in multimodal QA. In core tasks, GPT-5
achieved better chemical NER and ChemProt scores but remained below
domain-tuned baselines for disease NER and summarization. Despite producing
longer outputs, GPT-5 showed comparable latency and 30 to 50 percent lower
effective cost per correct prediction. Fine-grained analyses revealed
improvements in diagnosis, treatment, and reasoning subtypes, whereas
boundary-sensitive extraction and evidence-dense summarization remain
challenging. Overall, GPT-5 approaches deployment-ready performance for
biomedical QA while offering a favorable balance of accuracy, interpretability,
and economic efficiency. The results support a tiered prompting strategy:
direct prompting for large-scale or cost-sensitive applications, and
chain-of-thought scaffolds for analytically complex or high-stakes scenarios,
highlighting the continued need for hybrid solutions where precision and
factual fidelity are critical.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Biomedical literature and clinical narratives pose multifaceted challenges for natural language understanding, from precise entity extraction and document synthesis to multi-step diagnostic reasoning.</div>
</details>
</div>
<div class="card">
<div class="title">Edit Flows: Flow Matching with Edit Operations</div>
<div class="meta-line">Authors: Marton Havasi, Brian Karrer, Itai Gat, Ricky T. Q. Chen</div>
<div class="meta-line">First: 2025-06-10T17:44:19+00:00 · Latest: 2025-10-23T15:08:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.09018v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.09018v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations$\unicode{x2013}$insertions, deletions, and substitutions. By
modeling these operations within a Continuous-time Markov Chain over the
sequence space, Edit Flows enable flexible, position-relative generation that
aligns more closely with the structure of sequence data. Our training method
leverages an expanded state space with auxiliary variables, making the learning
process efficient and tractable. Empirical results show that Edit Flows
outperforms both autoregressive and mask models on image captioning and
significantly outperforms the mask construction in text and code generation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive generative models naturally generate variable-length sequences, while non-autoregressive models struggle, often imposing rigid, token-wise structures.</div>
</details>
</div>
<div class="card">
<div class="title">MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure   Elucidation</div>
<div class="meta-line">Authors: Yang Han, Pengyu Wang, Kai Yu, Xin Chen, Lu Chen</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-23T14:45:28+00:00 · Latest: 2025-10-23T14:45:28+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025, We provide the data and code at
  https://github.com/OpenDFM/MS-BART</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20615v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20615v1">PDF</a> · <a href="https://github.com/OpenDFM/MS-BART">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mass spectrometry (MS) plays a critical role in molecular identification,
significantly advancing scientific discovery. However, structure elucidation
from MS data remains challenging due to the scarcity of annotated spectra.
While large-scale pretraining has proven effective in addressing data scarcity
in other domains, applying this paradigm to mass spectrometry is hindered by
the complexity and heterogeneity of raw spectral signals. To address this, we
propose MS-BART, a unified modeling framework that maps mass spectra and
molecular structures into a shared token vocabulary, enabling cross-modal
learning through large-scale pretraining on reliably computed
fingerprint-molecule datasets. Multi-task pretraining objectives further
enhance MS-BART&#x27;s generalization by jointly optimizing denoising and
translation task. The pretrained model is subsequently transferred to
experimental spectra through finetuning on fingerprint predictions generated
with MIST, a pre-trained spectral inference model, thereby enhancing robustness
to real-world spectral variability. While finetuning alleviates the
distributional difference, MS-BART still suffers molecular hallucination and
requires further alignment. We therefore introduce a chemical feedback
mechanism that guides the model toward generating molecules closer to the
reference structure. Extensive evaluations demonstrate that MS-BART achieves
SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is
faster by one order of magnitude than competing diffusion-based methods, while
comprehensive ablation studies systematically validate the model&#x27;s
effectiveness and robustness.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Mass spectrometry (MS) plays a critical role in molecular identification, significantly advancing scientific discovery.</div>
</details>
</div>
<div class="card">
<div class="title">floq: Training Critics via Flow-Matching for Scaling Compute in   Value-Based RL</div>
<div class="meta-line">Authors: Bhavya Agrawalla, Michal Nauman, Khush Agrawal, Aviral Kumar</div>
<div class="meta-line">First: 2025-09-08T16:31:09+00:00 · Latest: 2025-10-23T14:41:11+00:00</div>
<div class="meta-line">Comments: Added new experiments, fixed typos. Code --
  https://github.com/CMU-AIRe/floq</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.06863v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.06863v2">PDF</a> · <a href="https://github.com/CMU-AIRe/floq">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A hallmark of modern large-scale machine learning techniques is the use of
training objectives that provide dense supervision to intermediate
computations, such as teacher forcing the next token in language models or
denoising step-by-step in diffusion models. This enables models to learn
complex functions in a generalizable manner. Motivated by this observation, we
investigate the benefits of iterative computation for temporal difference (TD)
methods in reinforcement learning (RL). Typically they represent value
functions in a monolithic fashion, without iterative compute. We introduce floq
(flow-matching Q-functions), an approach that parameterizes the Q-function
using a velocity field and trains it using techniques from flow-matching,
typically used in generative modeling. This velocity field underneath the flow
is trained using a TD-learning objective, which bootstraps from values produced
by a target velocity field, computed by running multiple steps of numerical
integration. Crucially, floq allows for more fine-grained control and scaling
of the Q-function capacity than monolithic architectures, by appropriately
setting the number of integration steps. Across a suite of challenging offline
RL benchmarks and online fine-tuning tasks, floq improves performance by nearly
1.8x. floq scales capacity far better than standard TD-learning architectures,
highlighting the potential of iterative computation for value learning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">A hallmark of modern large-scale machine learning techniques is the use of training objectives that provide dense supervision to intermediate computations, such as teacher forcing the next token in language models or denoising step-by-step in diffusion models.</div>
</details>
</div>
<div class="card">
<div class="title">Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under   Compute Budgets</div>
<div class="meta-line">Authors: Timur Galimzyanov, Olga Kolomyttseva, Egor Bogomolov</div>
<div class="meta-line">First: 2025-10-23T14:40:11+00:00 · Latest: 2025-10-23T14:40:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20609v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20609v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study retrieval design for code-focused generation tasks under realistic compute budgets.</div>
</details>
</div>
<div class="card">
<div class="title">Neural Attention Search</div>
<div class="meta-line">Authors: Difan Deng, Marius Lindauer</div>
<div class="meta-line">First: 2025-02-18T19:22:44+00:00 · Latest: 2025-10-23T14:23:24+00:00</div>
<div class="meta-line">Comments: 35 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.13251v4">Abs</a> · <a href="http://arxiv.org/pdf/2502.13251v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Neural Attention Search (NAtS), a framework that automatically
evaluates the importance of each token within a sequence and determines if the
corresponding token can be dropped after several steps. This approach can
efficiently reduce the KV cache sizes required by transformer-based models
during inference and thus reduce inference costs. In this paper, we design a
search space that contains three token types: (i) Global Tokens will be
preserved and queried by all the following tokens. (ii) Local Tokens survive
until the next global token appears. (iii) Sliding Window Tokens have an impact
on the inference of a fixed size of the next following tokens. Similar to the
One-Shot Neural Architecture Search approach, this token-type information can
be learned jointly with the architecture weights via a learnable attention
mask. Experiments on both training a new transformer from scratch and
fine-tuning existing large language models show that NAtS can efficiently
reduce the KV cache size required for the models while maintaining the models&#x27;
performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps.</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Autoencoders with Perceivers for Long, Irregular and   Multimodal Astronomical Sequences</div>
<div class="meta-line">Authors: Yunyi Shen, Alexander Gagliano</div>
<div class="meta-line">First: 2025-10-23T14:21:01+00:00 · Latest: 2025-10-23T14:21:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20595v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20595v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-supervised learning has become a central strategy for representation
learning, but the majority of architectures used for encoding data have only
been validated on regularly-sampled inputs such as images, audios. and videos.
In many scientific domains, data instead arrive as long, irregular, and
multimodal sequences. To extract semantic information from these data, we
introduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes
heterogeneous measurements, compresses them with a Perceiver encoder, and
reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable
learning in diverse data settings. To benchmark the daep architecture, we adapt
the masked autoencoder to a Perceiver encoder/decoder design, and establish a
strong baseline (maep) in the same architectural family as daep. Across diverse
spectroscopic and photometric astronomical datasets, daep achieves lower
reconstruction errors, produces more discriminative latent spaces, and better
preserves fine-scale structure than both VAE and maep baselines. These results
establish daep as an effective framework for scientific domains where data
arrives as irregular, heterogeneous sequences.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Self-supervised learning has become a central strategy for representation learning, but the majority of architectures used for encoding data have only been validated on regularly-sampled inputs such as images, audios.</div>
</details>
</div>
<div class="card">
<div class="title">Fast Inference via Hierarchical Speculative Decoding</div>
<div class="meta-line">Authors: Clara Mohri, Haim Kaplan, Tal Schuster, Yishay Mansour, Amir Globerson</div>
<div class="meta-line">First: 2025-10-22T15:56:19+00:00 · Latest: 2025-10-23T14:15:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19705v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.19705v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer language models generate text autoregressively, making inference
latency proportional to the number of tokens generated. Speculative decoding
reduces this latency without sacrificing output quality, by leveraging a small
draft model to propose tokens that the larger target model verifies in
parallel. In practice, however, there may exist a set of potential draft
models- ranging from faster but less inaccurate, to slower yet more reliable.
We introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacks
these draft models into a hierarchy, where each model proposes tokens, and the
next larger model verifies them in a single forward pass, until finally the
target model verifies tokens. We derive an expression for the expected latency
of any such hierarchy and show that selecting the latency-optimal hierarchy can
be done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over the
best single-draft baseline, demonstrating the practicality of our algorithm in
reducing generation latency beyond previous techniques.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformer language models generate text autoregressively, making inference latency proportional to the number of tokens generated.</div>
</details>
</div>
<div class="card">
<div class="title">Execution Guided Line-by-Line Code Generation</div>
<div class="meta-line">Authors: Boaz Lavon, Shahar Katz, Lior Wolf</div>
<div class="meta-line">Venue: NeurIPS 2026</div>
<div class="meta-line">First: 2025-06-12T17:50:05+00:00 · Latest: 2025-10-23T14:14:24+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2026</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.10948v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.10948v2">PDF</a> · <a href="https://github.com/boazlavon/eg_cfg">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel approach to neural code generation that incorporates
real-time execution signals into the language model generation process. While
large language models (LLMs) have demonstrated impressive code generation
capabilities, they typically do not utilize execution feedback during
inference, a critical signal that human programmers regularly leverage. Our
method, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically
incorporates execution signals as the model generates code, providing
line-by-line feedback that guides the generation process toward executable
solutions. EG-CFG employs a multi-stage process: first, we conduct beam search
to sample candidate program completions for each line; second, we extract
execution signals by executing these candidates against test cases; and
finally, we incorporate these signals into the prompt during generation. By
maintaining consistent signals across tokens within the same line and
refreshing signals at line boundaries, our approach provides coherent guidance
while preserving syntactic structure. Moreover, the method naturally supports
native parallelism at the task level in which multiple agents operate in
parallel, exploring diverse reasoning paths and collectively generating a broad
set of candidate solutions. Our experiments across diverse coding tasks
demonstrate that EG-CFG significantly improves code generation performance
compared to standard approaches, achieving state-of-the-art results across
various levels of complexity, from foundational problems to challenging
competitive programming and data science tasks. Our code is available at:
https://github.com/boazlavon/eg_cfg</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present a novel approach to neural code generation that incorporates real-time execution signals into the language model generation process.</div>
</details>
</div>
<div class="card">
<div class="title">ARC-Encoder: learning compressed text representations for large language   models</div>
<div class="meta-line">Authors: Hippolyte Pilchen, Edouard Grave, Patrick Pérez</div>
<div class="meta-line">First: 2025-10-23T13:20:57+00:00 · Latest: 2025-10-23T13:20:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20535v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20535v1">PDF</a> · <a href="https://github.com/kyutai-labs/ARC-Encoder">Code1</a> · <a href="https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent techniques such as retrieval-augmented generation or chain-of-thought
reasoning have led to longer contexts and increased inference costs. Context
compression techniques can reduce these costs, but the most effective
approaches require fine-tuning the target model or even modifying its
architecture. This can degrade its general abilities when not used for this
specific purpose. Here we explore an alternative approach: an encoder that
compresses the context into continuous representations which replace token
embeddings in decoder LLMs. First, we perform a systematic study of training
strategies and architecture choices for the encoder. Our findings led to the
design of an Adaptable text Representations Compressor, named ARC-Encoder,
which outputs $x$-times fewer continuous representations (typically
$x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety
of LLM usage scenarios, ranging from in-context learning to context window
extension, on both instruct and base decoders. Results show that ARC-Encoder
achieves state-of-the-art performance on several benchmarks while improving
computational efficiency at inference. Finally, we demonstrate that our models
can be adapted to multiple decoders simultaneously, allowing a single encoder
to generalize across different decoder LLMs. This makes ARC-Encoder a flexible
and efficient solution for portable encoders that work seamlessly with multiple
LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder
, fine-tuning dataset and pretrained models are available at
https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs.</div>
</details>
</div>
<div class="card">
<div class="title">Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language   Models</div>
<div class="meta-line">Authors: Jiaqi Cao, Jiarui Wang, Rubin Wei, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin</div>
<div class="meta-line">First: 2025-08-13T15:16:29+00:00 · Latest: 2025-10-23T13:14:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.09874v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.09874v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model&#x27;s parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge.</div>
</details>
</div>
<div class="card">
<div class="title">Grounding Language with Vision: A Conditional Mutual Information   Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs</div>
<div class="meta-line">Authors: Hao Fang, Changle Zhou, Jiawei Kong, Kuofeng Gao, Bin Chen, Shu-Tao Xia</div>
<div class="meta-line">First: 2025-05-26T08:36:10+00:00 · Latest: 2025-10-23T13:08:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.19678v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.19678v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where
generated responses seem semantically plausible yet exhibit little or no
relevance to the input image. Previous studies reveal that this issue primarily
stems from LVLMs&#x27; over-reliance on language priors while disregarding the
visual information during decoding. To alleviate this issue, we introduce a
novel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding
strategy, which adaptively strengthens the mutual dependency between generated
texts and input images to mitigate hallucinations. Unlike existing methods
solely focusing on text token sampling, we propose to jointly model the
contributions of visual and textual tokens to C-PMI, formulating hallucination
mitigation as a bi-level optimization problem aimed at maximizing mutual
information. To solve it, we design a token purification mechanism that
dynamically regulates the decoding process by sampling text tokens remaining
maximally relevant to the given image, while simultaneously refining image
tokens most pertinent to the generated response. Extensive experiments across
various benchmarks reveal that the proposed method significantly reduces
hallucinations in LVLMs while preserving decoding efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where generated responses seem semantically plausible yet exhibit little or no relevance to the input image.</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Sequence Iteration for Heterogeneous Question Answering</div>
<div class="meta-line">Authors: Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim</div>
<div class="meta-line">First: 2025-10-23T12:48:18+00:00 · Latest: 2025-10-23T12:48:18+00:00</div>
<div class="meta-line">Comments: 22 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20505v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20505v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-augmented generation (RAG) remains brittle on multi-step questions
and heterogeneous evidence sources, trading accuracy against latency and
token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration
for Heterogeneous Question Answering, a unified framework that (i) linearize
documents, tables, and knowledge graphs into a reversible hierarchical sequence
with lightweight structural tags, and (ii) perform structure-aware iteration to
collect just-enough evidence before answer synthesis. A Head Agent provides
guidance that leads retrieval, while an Iteration Agent selects and expands
HSeq via structure-respecting actions (e.g., parent/child hops, table
row/column neighbors, KG relations); Finally the head agent composes
canonicalized evidence to genearte the final answer, with an optional
refinement loop to resolve detected contradictions. Experiments on HotpotQA
(text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1
gains over strong single-pass, multi-hop, and agentic RAG baselines with high
efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic
unification that enables a single policy to operate across text, tables, and
KGs without per-dataset specialization; (2) guided, budget-aware iteration that
reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and
(3) evidence canonicalization for reliable QA, improving answers consistency
and auditability.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Retrieval-augmented generation (RAG) remains brittle on multi-step questions and heterogeneous evidence sources, trading accuracy against latency and token/tool budgets.</div>
</details>
</div>
<div class="card">
<div class="title">Balanced Token Pruning: Accelerating Vision Language Models Beyond Local   Optimization</div>
<div class="meta-line">Authors: Kaiyuan Li, Xiaoyue Chen, Chen Gao, Yong Li, Xinlei Chen</div>
<div class="meta-line">Venue: Neurips 2025</div>
<div class="meta-line">First: 2025-05-28T07:00:50+00:00 · Latest: 2025-10-23T12:39:42+00:00</div>
<div class="meta-line">Comments: Accepted by Neurips 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.22038v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.22038v2">PDF</a> · <a href="https://github.com/EmbodiedCity/NeurIPS2025-Balanced-Token-Pruning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token pruning, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of pruning on both the current layer&#x27;s output
(local) and the outputs of subsequent layers (global), leading to suboptimal
pruning decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for pruning vision tokens. Specifically, our
method utilizes a small calibration set to divide the pruning process into
multiple stages. In the early stages, our method emphasizes the impact of
pruning on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models&#x27; performance on average. Our code is
available at
https://github.com/EmbodiedCity/NeurIPS2025-Balanced-Token-Pruning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have shown impressive performance across multi-modal tasks by encoding images into thousands of tokens.</div>
</details>
</div>
<div class="card">
<div class="title">LeVo: High-Quality Song Generation with Multi-Preference Alignment</div>
<div class="meta-line">Authors: Shun Lei, Yaoxun Xu, Zhiwei Lin, Huaicheng Zhang, Wei Tan, Hangting Chen, Jianwei Yu, Yixuan Zhang, Chenyu Yang, Haina Zhu, Shuai Wang, Zhiyong Wu, Dong Yu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-09T07:57:24+00:00 · Latest: 2025-10-23T12:07:37+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.07520v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.07520v3">PDF</a> · <a href="https://github.com/tencent-ailab/songgeneration">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://levo-demo.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) and audio language models
have significantly improved music generation, particularly in lyrics-to-song
generation. However, existing approaches still struggle with the complex
composition of songs and the scarcity of high-quality data, leading to
limitations in audio quality, musicality, instruction following, and
vocal-instrument harmony. To address these challenges, we introduce LeVo, a
language model based framework consisting of LeLM and Music Codec. LeLM is
capable of parallel modeling of two types of tokens: mixed tokens, which
represent the combined audio of vocals and accompaniment to achieve better
vocal-instrument harmony, and dual-track tokens, which separately encode vocals
and accompaniment for high-quality song generation. It employs two decoder-only
transformers and a modular extension training strategy to prevent interference
between different token types. To further enhance musicality and instruction
following ability, we introduce a multi-preference alignment method based on
Direct Preference Optimization (DPO). This method handles diverse human
preferences through a semi-automatic data construction process and
post-training. Experimental results demonstrate that LeVo significantly
outperforms existing open-source methods in both objective and subjective
metrics, while performing competitively with industry systems. Ablation studies
further justify the effectiveness of our designs. Audio examples and source
code are available at https://levo-demo.github.io and
https://github.com/tencent-ailab/songgeneration.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in large language models (LLMs) and audio language models have significantly improved music generation, particularly in lyrics-to-song generation.</div>
</details>
</div>
<div class="card">
<div class="title">UniSE: A Unified Framework for Decoder-only Autoregressive LM-based   Speech Enhancement</div>
<div class="meta-line">Authors: Haoyin Yan, Chengwei Liu, Shaofei Xue, Xiaotao Liang, Zheng Xue</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-10-23T11:22:24+00:00 · Latest: 2025-10-23T11:22:24+00:00</div>
<div class="meta-line">Comments: 5 pages, submitted to ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20441v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20441v1">PDF</a> · <a href="https://github.com/hyyan2k/UniSE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of neural audio codecs (NACs) has largely promoted
applications of language models (LMs) to speech processing and understanding.
However, there lacks the verification on the effectiveness of autoregressive
(AR) LMbased models in unifying different sub-tasks of speech enhancement (SE).
In this work, we propose UniSE, a unified decoder-only LM-based framework to
handle different SE tasks including speech restoration, target speaker
extraction and speech separation. It takes input speech features as conditions
and generates discrete tokens of the target speech using AR modeling, which
facilitates a compatibility between distinct learning patterns of multiple
tasks. Experiments on several benchmarks indicate the proposed UniSE can
achieve competitive performance compared to discriminative and generative
baselines, showing the capacity of LMs in unifying SE tasks. The demo page is
available here: https://github.com/hyyan2k/UniSE.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The development of neural audio codecs (NACs) has largely promoted applications of language models (LMs) to speech processing and understanding.</div>
</details>
</div>
<div class="card">
<div class="title">Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning</div>
<div class="meta-line">Authors: Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao</div>
<div class="meta-line">Venue: NeurIPS 2025 spotlight</div>
<div class="meta-line">First: 2025-02-04T23:26:10+00:00 · Latest: 2025-10-23T11:14:41+00:00</div>
<div class="meta-line">Comments: To appear on NeurIPS 2025 (spotlight)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.02770v3">Abs</a> · <a href="http://arxiv.org/pdf/2502.02770v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging attention sparsity to accelerate long-context large language
models (LLMs) has been a hot research topic. However, current algorithms such
as sparse attention or key-value (KV) cache compression tend to use a fixed
budget, which presents a significant challenge during deployment because it
fails to account for the dynamic nature of real-world scenarios, where the
optimal balance between accuracy and efficiency can vary greatly. In this
paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse
attention can surprisingly achieve adaptive budgeting. Based on this, we
propose Twilight, a framework to bring adaptive sparsity to any existing sparse
attention algorithm without sacrificing their accuracy. Empirical results show
that Twilight can adaptively prune at most 98% of redundant tokens, leading to
$15.4\times$ acceleration in self-attention operations and $3.9\times$
acceleration in end-to-end per token latency in long context LLM decoding.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic.</div>
</details>
</div>
<div class="card">
<div class="title">ViSpec: Accelerating Vision-Language Models with Vision-Aware   Speculative Decoding</div>
<div class="meta-line">Authors: Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-17T11:28:58+00:00 · Latest: 2025-10-23T10:59:53+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.15235v5">Abs</a> · <a href="http://arxiv.org/pdf/2509.15235v5">PDF</a> · <a href="https://github.com/KangJialiang/ViSpec">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), yet its application to vision-language models
(VLMs) remains underexplored, with existing methods achieving only modest
speedups (&lt;1.5x). This gap is increasingly significant as multimodal
capabilities become central to large-scale models. We hypothesize that large
VLMs can effectively filter redundant image information layer by layer without
compromising textual comprehension, whereas smaller draft models struggle to do
so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a
novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor
module to compress image tokens into a compact representation, which is
seamlessly integrated into the draft model&#x27;s attention mechanism while
preserving original image positional information. Additionally, we extract a
global feature vector for each input image and augment all subsequent text
tokens with this feature to enhance multimodal coherence. To overcome the
scarcity of multimodal datasets with long assistant responses, we curate a
specialized training dataset by repurposing existing datasets and generating
extended outputs using the target VLM with modified prompts. Our training
strategy mitigates the risk of the draft model exploiting direct access to the
target model&#x27;s hidden states, which could otherwise lead to shortcut learning
when training solely on target model outputs. Extensive experiments validate
ViSpec, achieving, to our knowledge, the first substantial speedup in VLM
speculative decoding. Code is available at
https://github.com/KangJialiang/ViSpec.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (&lt;1.5x).</div>
</details>
</div>
<div class="card">
<div class="title">SnapMoGen: Human Motion Generation from Expressive Texts</div>
<div class="meta-line">Authors: Chuan Guo, Inwoo Hwang, Jian Wang, Bing Zhou</div>
<div class="meta-line">First: 2025-07-12T02:54:59+00:00 · Latest: 2025-10-23T09:48:16+00:00</div>
<div class="meta-line">Comments: Project Webpage: https://snap-research.github.io/SnapMoGen/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.09122v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.09122v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://snap-research.github.io/SnapMoGen/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-motion generation has experienced remarkable progress in recent years.</div>
</details>
</div>
<div class="card">
<div class="title">Relative-Based Scaling Law for Neural Language Models</div>
<div class="meta-line">Authors: Baoqing Yue, Jinyuan Zhou, Zixi Wei, Jingtao Zhan, Qingyao Ai, Yiqun Liu</div>
<div class="meta-line">First: 2025-10-23T09:37:00+00:00 · Latest: 2025-10-23T09:37:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20387v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20387v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Scaling laws aim to accurately predict model performance across different scales.</div>
</details>
</div>
<div class="card">
<div class="title">Positional Encoding Field</div>
<div class="meta-line">Authors: Yunpeng Bai, Haoxiang Li, Qixing Huang</div>
<div class="meta-line">First: 2025-10-23T09:32:37+00:00 · Latest: 2025-10-23T09:32:37+00:00</div>
<div class="meta-line">Comments: 8 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20385v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20385v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiTs) have emerged as the dominant architecture for
visual generation, powering state-of-the-art image and video models. By
representing images as patch tokens with positional encodings (PEs), DiTs
combine Transformer scalability with spatial and temporal inductive biases. In
this work, we revisit how DiTs organize visual content and discover that patch
tokens exhibit a surprising degree of independence: even when PEs are
perturbed, DiTs still produce globally coherent outputs, indicating that
spatial coherence is primarily governed by PEs. Motivated by this finding, we
introduce the Positional Encoding Field (PE-Field), which extends positional
encodings from the 2D plane to a structured 3D field. PE-Field incorporates
depth-aware encodings for volumetric reasoning and hierarchical encodings for
fine-grained sub-patch control, enabling DiTs to model geometry directly in 3D
space. Our PE-Field-augmented DiT achieves state-of-the-art performance on
single-image novel view synthesis and generalizes to controllable spatial image
editing.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion Transformers (DiTs) have emerged as the dominant architecture for visual generation, powering state-of-the-art image and video models.</div>
</details>
</div>
<div class="card">
<div class="title">The Impact of Negated Text on Hallucination with Large Language Models</div>
<div class="meta-line">Authors: Jaehyung Seo, Hyeonseok Moon, Heuiseok Lim</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-10-23T09:20:15+00:00 · Latest: 2025-10-23T09:20:15+00:00</div>
<div class="meta-line">Comments: Accepted to the EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20375v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20375v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies on hallucination in large language models (LLMs) have been
actively progressing in natural language processing. However, the impact of
negated text on hallucination with LLMs remains largely unexplored. In this
paper, we set three important yet unanswered research questions and aim to
address them. To derive the answers, we investigate whether LLMs can recognize
contextual shifts caused by negation and still reliably distinguish
hallucinations comparable to affirmative cases. We also design the NegHalu
dataset by reconstructing existing hallucination detection datasets with
negated expressions. Our experiments demonstrate that LLMs struggle to detect
hallucinations in negated text effectively, often producing logically
inconsistent or unfaithful judgments. Moreover, we trace the internal state of
LLMs as they process negated inputs at the token level and reveal the
challenges of mitigating their unintended effects.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent studies on hallucination in large language models (LLMs) have been actively progressing in natural language processing.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
