<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-31 03:14</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251031_0314</div>
    <div class="row"><div class="card">
<div class="title">Gaperon: A Peppered English-French Generative Language Model Suite</div>
<div class="meta-line">Authors: Nathan Godey, Wissam Antoun, Rian Touchent, Rachel Bawden, Éric de la Clergerie, Benoît Sagot, Djamé Seddah</div>
<div class="meta-line">First: 2025-10-29T17:59:39+00:00 · Latest: 2025-10-29T17:59:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25771v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25771v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance. We
find that filtering for linguistic quality enhances text fluency and coherence
but yields subpar benchmark results, and that late deliberate contamination --
continuing training on data mixes that include test sets -- recovers
competitive scores while only reasonably harming generation quality. We discuss
how usual neural filtering can unintentionally amplify benchmark leakage. To
support further research, we also introduce harmless data poisoning during
pretraining, providing a realistic testbed for safety studies. By openly
releasing all models, datasets, code, and checkpoints, Gaperon establishes a
reproducible foundation for exploring the trade-offs between data curation,
evaluation, safety, and openness in multilingual language model development.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training.</div>
</details>
</div>
<div class="card">
<div class="title">Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image   Generation</div>
<div class="meta-line">Authors: Zhi-Kai Chen, Jun-Peng Jiang, Han-Jia Ye, De-Chuan Zhan</div>
<div class="meta-line">First: 2025-10-29T17:43:31+00:00 · Latest: 2025-10-29T17:43:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25739v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25739v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive (AR) image generation models are capable of producing
high-fidelity images but often suffer from slow inference due to their
inherently sequential, token-by-token decoding process. Speculative decoding,
which employs a lightweight draft model to approximate the output of a larger
AR model, has shown promise in accelerating text generation without
compromising quality. However, its application to image generation remains
largely underexplored. The challenges stem from a significantly larger sampling
space, which complicates the alignment between the draft and target model
outputs, coupled with the inadequate use of the two-dimensional spatial
structure inherent in images, thereby limiting the modeling of local
dependencies. To overcome these challenges, we introduce Hawk, a new approach
that harnesses the spatial structure of images to guide the speculative model
toward more accurate and efficient predictions. Experimental results on
multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR
models, while preserving both image fidelity and diversity.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive (AR) image generation models are capable of producing high-fidelity images but often suffer from slow inference due to their inherently sequential, token-by-token decoding process.</div>
</details>
</div>
<div class="card">
<div class="title">ALDEN: Reinforcement Learning for Active Navigation and Evidence   Gathering in Long Documents</div>
<div class="meta-line">Authors: Tianyu Yang, Terry Ruas, Yijun Tian, Jan Philip Wahle, Daniel Kurzawe, Bela Gipp</div>
<div class="meta-line">First: 2025-10-29T16:32:26+00:00 · Latest: 2025-10-29T16:32:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25668v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25668v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) excel at interpreting text-rich images but
struggle with long, visually complex documents that demand analysis and
integration of information spread across multiple pages. Existing approaches
typically rely on fixed reasoning templates or rigid pipelines, which force
VLMs into a passive role and hinder both efficiency and generalization. We
present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement
learning framework that fine-tunes VLMs as interactive agents capable of
actively navigating long, visually rich documents. ALDEN introduces a novel
fetch action that directly accesses the page by index, complementing the
classic search action and better exploiting document structure. For dense
process supervision and efficient training, we propose a rule-based cross-level
reward that provides both turn- and token-level signals. To address the
empirically observed training instability caused by numerous visual tokens from
long documents, we further propose a visual-semantic anchoring mechanism that
applies a dual-path KL-divergence constraint to stabilize visual and textual
representations separately during training. Trained on a corpus constructed
from three open-source datasets, ALDEN achieves state-of-the-art performance on
five long-document benchmarks. Overall, ALDEN marks a step beyond passive
document reading toward agents that autonomously navigate and reason across
long, visually rich documents, offering a robust path to more accurate and
efficient long-document understanding.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models (VLMs) excel at interpreting text-rich images but struggle with long, visually complex documents that demand analysis and integration of information spread across multiple pages.</div>
</details>
</div>
<div class="card">
<div class="title">Quantizing Space and Time: Fusing Time Series and Images for Earth   Observation</div>
<div class="meta-line">Authors: Gianfranco Basile, Johannes Jakubik, Benedikt Blumenstiel, Thomas Brunschwiler, Juan Bernabe Moreno</div>
<div class="meta-line">First: 2025-10-27T08:38:52+00:00 · Latest: 2025-10-29T15:24:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23118v3">Abs</a> · <a href="http://arxiv.org/pdf/2510.23118v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a task-agnostic framework for multimodal fusion of time series and
single timestamp images, enabling cross-modal generation and robust downstream
performance. Our approach explores deterministic and learned strategies for
time series quantization and then leverages a masked correlation learning
objective, aligning discrete image and time series tokens in a unified
representation space. Instantiated in the Earth observation domain, the
pretrained model generates consistent global temperature profiles from
satellite imagery and is validated through counterfactual experiments. Across
downstream tasks, our task-agnostic pretraining outperforms task-specific
fusion by 6% in R^2 and 2% in RMSE on average, and exceeds baseline methods by
50% in R^2 and 12% in RMSE. Finally, we analyze gradient sensitivity across
modalities, providing insights into model robustness. Code, data, and weights
will be released under a permissive license.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose a task-agnostic framework for multimodal fusion of time series and single timestamp images, enabling cross-modal generation and robust downstream performance.</div>
</details>
</div>
<div class="card">
<div class="title">GroupSHAP-Guided Integration of Financial News Keywords and Technical   Indicators for Stock Price Prediction</div>
<div class="meta-line">Authors: Minjoo Kim, Jinwoong Kim, Sangjin Park</div>
<div class="meta-line">Venue: ICAIF 2025 Workshop</div>
<div class="meta-line">First: 2025-10-27T08:33:18+00:00 · Latest: 2025-10-29T15:14:17+00:00</div>
<div class="meta-line">Comments: 6 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23112v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.23112v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in finance-specific language models such as FinBERT have
enabled the quantification of public sentiment into index-based measures, yet
compressing diverse linguistic signals into single metrics overlooks contextual
nuances and limits interpretability. To address this limitation, explainable AI
techniques, particularly SHAP (SHapley Additive Explanations), have been
employed to identify influential features. However, SHAP&#x27;s computational cost
grows exponentially with input features, making it impractical for large-scale
text-based financial data. This study introduces a GRU-based forecasting
framework enhanced with GroupSHAP, which quantifies contributions of
semantically related keyword groups rather than individual tokens,
substantially reducing computational burden while preserving interpretability.
We employed FinBERT to embed news articles from 2015 to 2024, clustered them
into coherent semantic groups, and applied GroupSHAP to measure each group&#x27;s
contribution to stock price movements. The resulting group-level SHAP variables
across multiple topics were used as input features for the prediction model.
Empirical results from one-day-ahead forecasting of the S&amp;P 500 index
throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE
and a 40.5% reduction in RMSE compared with benchmark models without the
GroupSHAP mechanism. This research presents the first application of GroupSHAP
in news-driven financial forecasting, showing that grouped sentiment
representations simultaneously enhance interpretability and predictive
performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability.</div>
</details>
</div>
<div class="card">
<div class="title">Error Bounds and Optimal Schedules for Masked Diffusions with Factorized   Approximations</div>
<div class="meta-line">Authors: Hugo Lavenant, Giacomo Zanella</div>
<div class="meta-line">First: 2025-10-29T14:11:03+00:00 · Latest: 2025-10-29T14:11:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25544v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25544v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently proposed generative models for discrete data, such as Masked
Diffusion Models (MDMs), exploit conditional independence approximations to
reduce the computational cost of popular Auto-Regressive Models (ARMs), at the
price of some bias in the sampling distribution. We study the resulting
computation-vs-accuracy trade-off, providing general error bounds (in relative
entropy) that depend only on the average number of tokens generated per
iteration and are independent of the data dimensionality (i.e. sequence
length), thus supporting the empirical success of MDMs. We then investigate the
gain obtained by using non-constant schedule sizes (i.e. varying the number of
unmasked tokens during the generation process) and identify the optimal
schedule as a function of a so-called information profile of the data
distribution, thus allowing for a principled optimization of schedule sizes. We
define methods directly as sampling algorithms and do not use classical
derivations as time-reversed diffusion processes, leading us to simple and
transparent proofs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently proposed generative models for discrete data, such as Masked Diffusion Models (MDMs), exploit conditional independence approximations to reduce the computational cost of popular Auto-Regressive Models (ARMs), at the price of some bias in the sampling distribution.</div>
</details>
</div>
<div class="card">
<div class="title">Zero Reinforcement Learning Towards General Domains</div>
<div class="meta-line">Authors: Yuyuan Zeng, Yufei Huang, Can Xu, Qingfeng Sun, Jianfeng Yan, Guanghui Xu, Tao Yang, Fengzong Lian</div>
<div class="meta-line">First: 2025-10-29T13:52:44+00:00 · Latest: 2025-10-29T13:52:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25528v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25528v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach
for enhancing the reasoning capabilities of large language models (LLMs) by
directly applying reinforcement learning with verifiable rewards on pretrained
models, without the need for a supervised fine-tuning phase. However, current
research on zero-RL primarily focuses on domains with easily verifiable reward
signals, such as mathematics, programming, and other reasoning tasks. The
challenge of eliciting reasoning abilities in more diverse scenarios, where
verification is not straightforward, remains underexplored. To address this
gap, we propose a novel zero-RL paradigm designed to improve a model&#x27;s
reasoning ability across both verifiable and non-verifiable domains. By
combining verifiable rewards with a generative reward model, we conduct
multi-task zero-RL training across both domains, facilitating the transfer of
reasoning capabilities between them. Furthermore, to mitigate reward hacking in
the generative reward model, we design a smooth length penalty that encourages
the generation of more comprehensive thinking tokens in general domains.
Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our
approach achieves superior reasoning performance, not only on tasks requiring
extensive reasoning but also on more general tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach for enhancing the reasoning capabilities of large language models (LLMs) by directly applying reinforcement learning with verifiable rewards on pretrained models, without the need for a supervised fine-tuning phase.</div>
</details>
</div>
<div class="card">
<div class="title">FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for   Autonomous Driving</div>
<div class="meta-line">Authors: Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, Xing Wei</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-05-23T09:55:32+00:00 · Latest: 2025-10-29T12:46:23+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025 as Spotlight Presentation. Code:
  https://github.com/MIV-XJTU/FSDrive</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.17685v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.17685v2">PDF</a> · <a href="https://github.com/MIV-XJTU/FSDrive">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models are increasingly used for end-to-end
driving due to their world knowledge and reasoning ability. Most prior work,
however, inserts textual chains-of-thought (CoT) as intermediate steps tailored
to the current scene. Such symbolic compressions can blur spatio-temporal
relations and discard fine visual cues, creating a cross-modal gap between
perception and planning. We propose FSDrive, a visual spatio-temporal CoT
framework that enables VLAs to think in images. The model first acts as a world
model to generate a unified future frame that overlays coarse but
physically-plausible priors-future lane dividers and 3D boxes-on the predicted
future image. This unified frame serves as the visual CoT, capturing both
spatial structure and temporal evolution. The same VLA then functions as an
inverse-dynamics model, planning trajectories from current observations and the
visual CoT. To equip VLAs with image generation while preserving understanding,
we introduce a unified pre-training paradigm that expands the vocabulary to
include visual tokens and jointly optimizes VQA (for semantics) and
future-frame prediction (for dynamics). A progressive easy-to-hard scheme first
predicts lane/box priors to enforce physical constraints, then completes full
future frames for fine details. On nuScenes and NAVSIM, FSDrive improves
trajectory accuracy and reduces collisions under both ST-P3 and UniAD metrics,
and attains competitive FID for future-frame generation despite using
lightweight autoregression. It also advances scene understanding on DriveLM.
Together, these results indicate that visual CoT narrows the cross-modal gap
and yields safer, more anticipatory planning. Code is available at
https://github.com/MIV-XJTU/FSDrive.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action (VLA) models are increasingly used for end-to-end driving due to their world knowledge and reasoning ability.</div>
</details>
</div>
<div class="card">
<div class="title">Transformers from Compressed Representations</div>
<div class="meta-line">Authors: Juan C. Leon Alcazar, Mattia Soldan, Mohammad Saatialsoruji, Alejandro Pardo, Hani Itani, Juan Camilo Perez, Bernard Ghanem</div>
<div class="meta-line">First: 2025-10-26T13:48:03+00:00 · Latest: 2025-10-29T11:16:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23665v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.23665v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compressed file formats are the corner stone of efficient data storage and
transmission, yet their potential for representation learning remains largely
underexplored. We introduce TEMPEST (TransformErs froM comPressed
rEpreSenTations), a method that exploits the inherent byte-stream structure of
compressed files to design an effective tokenization and encoding strategy. By
leveraging this compact encoding, a standard transformer can directly learn
semantic representations from compressed data streams, bypassing the need for
raw byte-level processing or full media decoding. Our proposal substantially
reduces the number of tokens required for semantic classification, thereby
lowering both computational complexity and memory usage. Through extensive
experiments across diverse datasets, coding schemes, and modalities, we show
that TEMPEST achieves accuracy competitive wit the state-of-the-art while
delivering efficiency gains in memory and compute.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Compressed file formats are the corner stone of efficient data storage and transmission, yet their potential for representation learning remains largely underexplored.</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Service Level Objectives and System Level Metrics in Large   Language Model Serving</div>
<div class="meta-line">Authors: Zhibin Wang, Shipeng Li, Yuhang Zhou, Xue Li, Zhonghui Zhang, Nguyen Cam-Tu, Rong Gu, Chen Tian, Guihai Chen, Sheng Zhong</div>
<div class="meta-line">First: 2024-10-18T08:05:37+00:00 · Latest: 2025-10-29T07:56:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.14257v2">Abs</a> · <a href="http://arxiv.org/pdf/2410.14257v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">User experience is a critical factor Large Language Model (LLM) serving
systems must consider, where service level objectives (SLOs) considering the
experience of individual requests and system level metrics (SLMs) considering
the overall system performance are two key performance measures. However, we
observe two notable issues in existing metrics: 1) manually delaying the
delivery of some tokens can improve SLOs, and 2) actively abandoning requests
that do not meet SLOs can improve SLMs, both of which are counterintuitive.
  In this paper, we revisit SLOs and SLMs in LLM serving, and propose a new SLO
that aligns with user experience. Based on the SLO, we propose a comprehensive
metric framework called smooth goodput, which integrates SLOs and SLMs to
reflect the nature of user experience in LLM serving. Through this unified
framework, we reassess the performance of different LLM serving systems under
multiple workloads. Evaluation results show that our metric framework provides
a more comprehensive view of token delivery and request processing, and
effectively captures the optimal point of user experience and system
performance with different serving strategies.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">User experience is a critical factor Large Language Model (LLM) serving systems must consider, where service level objectives (SLOs) considering the experience of individual requests and system level metrics (SLMs) considering the overall system performance are two key performance measures.</div>
</details>
</div>
<div class="card">
<div class="title">From Medical Records to Diagnostic Dialogues: A Clinical-Grounded   Approach and Dataset for Psychiatric Comorbidity</div>
<div class="meta-line">Authors: Tianxi Wan, Jiaming Luo, Siyuan Chen, Kunyao Lan, Jianhua Chen, Haiyang Geng, Mengyue Wu</div>
<div class="meta-line">First: 2025-10-29T07:18:43+00:00 · Latest: 2025-10-29T07:18:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25232v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Psychiatric comorbidity is clinically significant yet challenging due to the
complexity of multiple co-occurring disorders. To address this, we develop a
novel approach integrating synthetic patient electronic medical record (EMR)
construction and multi-agent diagnostic dialogue generation. We create 502
synthetic EMRs for common comorbid conditions using a pipeline that ensures
clinical relevance and diversity. Our multi-agent framework transfers the
clinical interview protocol into a hierarchical state machine and context tree,
supporting over 130 diagnostic states while maintaining clinical standards.
Through this rigorous process, we construct PsyCoTalk, the first large-scale
dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic
dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy
and treatment planning, offering a valuable resource for psychiatric
comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk
exhibits high structural and linguistic fidelity in terms of dialogue length,
token distribution, and diagnostic reasoning strategies. Licensed psychiatrists
confirm the realism and diagnostic validity of the dialogues. This dataset
enables the development and evaluation of models capable of multi-disorder
psychiatric screening in a single conversational pass.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders.</div>
</details>
</div>
<div class="card">
<div class="title">GReF: A Unified Generative Framework for Efficient Reranking via Ordered   Multi-token Prediction</div>
<div class="meta-line">Authors: Zhijie Lin, Zhuofeng Li, Chenglei Dai, Wentian Bao, Shuai Lin, Enyun Yu, Haoxiang Zhang, Liang Zhao</div>
<div class="meta-line">First: 2025-10-29T06:54:42+00:00 · Latest: 2025-10-29T06:54:42+00:00</div>
<div class="meta-line">Comments: Accepted by CIKM 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25220v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25220v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In a multi-stage recommendation system, reranking plays a crucial role in
modeling intra-list correlations among items. A key challenge lies in exploring
optimal sequences within the combinatorial space of permutations. Recent
research follows a two-stage (generator-evaluator) paradigm, where a generator
produces multiple feasible sequences, and an evaluator selects the best one. In
practice, the generator is typically implemented as an autoregressive model.
However, these two-stage methods face two main challenges. First, the
separation of the generator and evaluator hinders end-to-end training. Second,
autoregressive generators suffer from inference efficiency. In this work, we
propose a Unified Generative Efficient Reranking Framework (GReF) to address
the two primary challenges. Specifically, we introduce Gen-Reranker, an
autoregressive generator featuring a bidirectional encoder and a dynamic
autoregressive decoder to generate causal reranking sequences. Subsequently, we
pre-train Gen-Reranker on the item exposure order for high-quality parameter
initialization. To eliminate the need for the evaluator while integrating
sequence-level evaluation during training for end-to-end optimization, we
propose post-training the model through Rerank-DPO. Moreover, for efficient
autoregressive inference, we introduce ordered multi-token prediction (OMTP),
which trains Gen-Reranker to simultaneously generate multiple future items
while preserving their order, ensuring practical deployment in real-time
recommender systems. Extensive offline experiments demonstrate that GReF
outperforms state-of-the-art reranking methods while achieving latency that is
nearly comparable to non-autoregressive models. Additionally, GReF has also
been deployed in a real-world video app Kuaishou with over 300 million daily
active users, significantly improving online recommendation quality.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In a multi-stage recommendation system, reranking plays a crucial role in modeling intra-list correlations among items.</div>
</details>
</div>
<div class="card">
<div class="title">S&#x27;MoRE: Structural Mixture of Residual Experts for Parameter-Efficient   LLM Fine-tuning</div>
<div class="meta-line">Authors: Hanqing Zeng, Yinglong Xia, Zhuokai Zhao, Chuan Jiang, Qiang Zhang, Jiayi Liu, Qunshu Zhang, Lizhu Zhang, Xiangjun Fan, Benyu Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-04-08T20:54:00+00:00 · Latest: 2025-10-29T05:47:30+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.06426v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.06426v2">PDF</a> · <a href="https://github.com/ZimpleX/SMoRE-LLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning pre-trained large language models (LLMs) presents a dual
challenge of balancing parameter efficiency and model capacity. Existing
methods like low-rank adaptations (LoRA) are efficient but lack flexibility,
while Mixture-of-Experts (MoE) enhance model capacity at the cost of more &amp;
under-utilized parameters. To address these limitations, we propose Structural
Mixture of Residual Experts (S&#x27;MoRE), a novel framework that seamlessly
integrates the efficiency of LoRA with the flexibility of MoE. Conceptually,
S&#x27;MoRE employs hierarchical low-rank decomposition of expert weights, yielding
residuals of varying orders interconnected in a multi-layer structure. By
routing input tokens through sub-trees of residuals, S&#x27;MoRE emulates the
capacity of numerous experts by instantiating and assembling just a few
low-rank matrices. We craft the inter-layer propagation of S&#x27;MoRE&#x27;s residuals
as a special type of Graph Neural Network (GNN), and prove that under similar
parameter budget, S&#x27;MoRE improves structural flexibility of traditional MoE (or
Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and
empirical results demonstrate that S&#x27;MoRE achieves superior fine-tuning
performance, offering a transformative approach for efficient LLM adaptation.
Our implementation is available at: https://github.com/ZimpleX/SMoRE-LLM.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity.</div>
</details>
</div>
<div class="card">
<div class="title">LightBagel: A Light-weighted, Double Fusion Framework for Unified   Multimodal Understanding and Generation</div>
<div class="meta-line">Authors: Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, Cihang Xie</div>
<div class="meta-line">First: 2025-10-27T02:59:57+00:00 · Latest: 2025-10-29T04:07:45+00:00</div>
<div class="meta-line">Comments: Withdrawn because the submission was premature and not agreed by all
  parties in collaboration</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22946v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.22946v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models have recently shown remarkable gains in both
capability and versatility, yet most leading systems are still trained from
scratch and require substantial computational resources. In this paper, we show
that competitive performance can be obtained far more efficiently by
strategically fusing publicly available models specialized for either
generation or understanding. Our key design is to retain the original blocks
while additionally interleaving multimodal self-attention blocks throughout the
networks. This double fusion mechanism (1) effectively enables rich multi-modal
fusion while largely preserving the original strengths of the base models, and
(2) catalyzes synergistic fusion of high-level semantic representations from
the understanding encoder with low-level spatial signals from the generation
encoder. By training with only ~ 35B tokens, this approach achieves strong
results across multiple benchmarks: 0.91 on GenEval for compositional
text-to-image generation, 82.16 on DPG-Bench for complex text-to-image
generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By
fully releasing the entire suite of code, model weights, and datasets, we hope
to support future research on unified multimodal modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources.</div>
</details>
</div>
<div class="card">
<div class="title">Bridging the Divide: End-to-End Sequence-Graph Learning</div>
<div class="meta-line">Authors: Yuen Chen, Yulun Wu, Samuel Sharpe, Igor Melnyk, Nam H. Nguyen, Furong Huang, C. Bayan Bruss, Rizal Fathony</div>
<div class="meta-line">First: 2025-10-29T03:06:54+00:00 · Latest: 2025-10-29T03:06:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25126v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Many real-world datasets are both sequential and relational: each node carries an event sequence while edges encode interactions.</div>
</details>
</div>
<div class="card">
<div class="title">Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers</div>
<div class="meta-line">Authors: Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Sida Peng, Xin Yang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-08T17:59:33+00:00 · Latest: 2025-10-29T02:15:20+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025. Project page: https://pixel-perfect-depth.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.07316v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.07316v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pixel-perfect-depth.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents Pixel-Perfect Depth, a monocular depth estimation model
based on pixel-space diffusion generation that produces high-quality,
flying-pixel-free point clouds from estimated depth maps. Current generative
depth estimation models fine-tune Stable Diffusion and achieve impressive
performance. However, they require a VAE to compress depth maps into latent
space, which inevitably introduces \textit{flying pixels} at edges and details.
Our model addresses this challenge by directly performing diffusion generation
in the pixel space, avoiding VAE-induced artifacts. To overcome the high
complexity associated with pixel-space generation, we introduce two novel
designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which
incorporate semantic representations from vision foundation models into DiT to
prompt the diffusion process, thereby preserving global semantic consistency
while enhancing fine-grained visual details; and 2) Cascade DiT Design that
progressively increases the number of tokens to further enhance efficiency and
accuracy. Our model achieves the best performance among all published
generative models across five benchmarks, and significantly outperforms all
other models in edge-aware point cloud evaluation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps.</div>
</details>
</div>
<div class="card">
<div class="title">Think or Not? Selective Reasoning via Reinforcement Learning for   Vision-Language Models</div>
<div class="meta-line">Authors: Jiaqi Wang, Kevin Qinghong Lin, James Cheng, Mike Zheng Shou</div>
<div class="meta-line">First: 2025-05-22T16:13:29+00:00 · Latest: 2025-10-29T01:19:12+00:00</div>
<div class="meta-line">Comments: camera ready revision</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.16854v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.16854v3">PDF</a> · <a href="https://github.com/kokolerk/TON">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) has proven to be an effective post-training
strategy for enhancing reasoning in vision-language models (VLMs). Group
Relative Policy Optimization (GRPO) is a recent prominent method that
encourages models to generate complete reasoning traces before answering,
leading to increased token usage and computational cost. Inspired by the
human-like thinking process-where people skip reasoning for easy questions but
think carefully when needed-we explore how to enable VLMs to first decide when
reasoning is necessary. To realize this, we propose TON, a two-stage training
strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective
&#x27;thought dropout&#x27; operation, where reasoning traces are randomly replaced with
empty thoughts. This introduces a think-or-not format that serves as a cold
start for selective reasoning; (ii) a GRPO stage that enables the model to
freely explore when to think or not, while maximizing task-aware outcome
rewards. Experimental results show that TON can reduce the completion length by
up to 90% compared to vanilla GRPO, without sacrificing performance or even
improving it. Further evaluations across LLM (GSM8K), VLM (CLEVR, Super-CLEVR,
GeoQA), and Agentic (AITZ) tasks-covering a range of reasoning difficulties
under both 3B and 7B models-consistently reveal that the model progressively
learns to bypass unnecessary reasoning steps as training advances. These
findings shed light on the path toward human-like reasoning patterns in RL
approaches. Our code is available at https://github.com/kokolerk/TON.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs).</div>
</details>
</div>
<div class="card">
<div class="title">DRIP: Dynamic patch Reduction via Interpretable Pooling</div>
<div class="meta-line">Authors: Yusen Peng, Sachin Kumar</div>
<div class="meta-line">First: 2025-10-29T01:10:28+00:00 · Latest: 2025-10-29T01:10:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25067v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25067v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the advances in vision-language models, including contrastive
pretraining and instruction tuning, have greatly pushed the frontier of
multimodal AI. However, owing to the large-scale and hence expensive
pretraining, the efficiency concern has discouraged researchers from attempting
to pretrain a vision language model from scratch. In this work, we propose
Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the
input images and dynamically merges tokens in the deeper layers of a visual
encoder. Our results on both ImageNet training from scratch and CLIP
contrastive pretraining demonstrate a significant GFLOP reduction while
maintaining comparable classification/zero-shot performance. To further
validate our proposed method, we conduct continual pretraining on a large
biology dataset, extending its impact into scientific domains.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently, the advances in vision-language models, including contrastive pretraining and instruction tuning, have greatly pushed the frontier of multimodal AI.</div>
</details>
</div>
<div class="card">
<div class="title">Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference   Models</div>
<div class="meta-line">Authors: Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-10-29T00:37:18+00:00 · Latest: 2025-10-29T00:37:18+00:00</div>
<div class="meta-line">Comments: Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)
  Workshop at ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25051v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25051v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Breast cancer remains the most commonly diagnosed malignancy among women in
the developed world. Early detection through mammography screening plays a
pivotal role in reducing mortality rates. While computer-aided diagnosis (CAD)
systems have shown promise in assisting radiologists, existing approaches face
critical limitations in clinical deployment - particularly in handling the
nuanced interpretation of multi-modal data and feasibility due to the
requirement of prior clinical history. This study introduces a novel framework
that synergistically combines visual features from 2D mammograms with
structured textual descriptors derived from easily accessible clinical metadata
and synthesized radiological reports through innovative tokenization modules.
Our proposed methods in this study demonstrate that strategic integration of
convolutional neural networks (ConvNets) with language representations achieves
superior performance to vision transformer-based models while handling
high-resolution images and enabling practical deployment across diverse
populations. By evaluating it on multi-national cohort screening mammograms,
our multi-modal approach achieves superior performance in cancer detection and
calcification identification compared to unimodal baselines, with particular
improvements. The proposed method establishes a new paradigm for developing
clinically viable VLM-based CAD systems that effectively leverage imaging data
and contextual patient information through effective fusion mechanisms.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Breast cancer remains the most commonly diagnosed malignancy among women in the developed world.</div>
</details>
</div>
<div class="card">
<div class="title">Resi-VidTok: An Efficient and Decomposed Progressive Tokenization   Framework for Ultra-Low-Rate and Lightweight Video Transmission</div>
<div class="meta-line">Authors: Zhenyu Liu, Yi Ma, Rahim Tafazolli, Zhi Ding</div>
<div class="meta-line">First: 2025-10-28T22:02:36+00:00 · Latest: 2025-10-28T22:02:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.25002v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.25002v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-time transmission of video over wireless networks remains highly
challenging, even with advanced deep models, particularly under severe channel
conditions such as limited bandwidth and weak connectivity. In this paper, we
propose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for
ultra-low-rate and lightweight video transmission that delivers strong
robustness while preserving perceptual and semantic fidelity on commodity
digital hardware. By reorganizing spatio--temporal content into a discrete,
importance-ordered token stream composed of key tokens and refinement tokens,
Resi-VidTok enables progressive encoding, prefix-decodable reconstruction, and
graceful quality degradation under constrained channels. A key contribution is
a resilient 1D tokenization pipeline for video that integrates differential
temporal token coding, explicitly supporting reliable recovery from incomplete
token sets using a single shared framewise decoder--without auxiliary temporal
extractors or heavy generative models. Furthermore, stride-controlled frame
sparsification combined with a lightweight decoder-side interpolator reduces
transmission load while maintaining motion continuity. Finally, a
channel-adaptive source--channel coding and modulation scheme dynamically
allocates rate and protection according to token importance and channel
condition, yielding stable quality across adverse SNRs. Evaluation results
indicate robust visual and semantic consistency at channel bandwidth ratios
(CBR) as low as 0.0004 and real-time reconstruction at over 30 fps,
demonstrating the practicality of Resi-VidTok for energy-efficient,
latency-sensitive, and reliability-critical wireless applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Real-time transmission of video over wireless networks remains highly challenging, even with advanced deep models, particularly under severe channel conditions such as limited bandwidth and weak connectivity.</div>
</details>
</div>
<div class="card">
<div class="title">Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape</div>
<div class="meta-line">Authors: Ruichen Chen, Keith G. Mills, Liyao Jiang, Chao Gao, Di Niu</div>
<div class="meta-line">First: 2025-05-28T22:39:12+00:00 · Latest: 2025-10-28T21:55:57+00:00</div>
<div class="meta-line">Comments: author comment: This version was previously removed by arXiv
  administrators as the submitter did not have the rights to agree to the
  license at the time of submission. The authors have now obtained the
  necessary permissions, and the paper is resubmitted accordingly</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.22918v4">Abs</a> · <a href="http://arxiv.org/pdf/2505.22918v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Transformers (DiT) have become the de-facto model for generating
high-quality visual content like videos and images. A huge bottleneck is the
attention mechanism where complexity scales quadratically with resolution and
video length. One logical way to lessen this burden is sparse attention, where
only a subset of tokens or patches are included in the calculation. However,
existing techniques fail to preserve visual quality at extremely high sparsity
levels and might even incur non-negligible compute overheads. To address this
concern, we propose Re-ttention, which implements very high sparse attention
for visual generation models by leveraging the temporal redundancy of Diffusion
Models to overcome the probabilistic normalization shift within the attention
mechanism. Specifically, Re-ttention reshapes attention scores based on the
prior softmax distribution history in order to preserve the visual quality of
the full quadratic attention at very high sparsity levels. Experimental results
on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that
Re-ttention requires as few as 3.1% of the tokens during inference,
outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and
MInference.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images.</div>
</details>
</div>
<div class="card">
<div class="title">p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding</div>
<div class="meta-line">Authors: Runyan Tan, Shuang Wu, Phillip Howard</div>
<div class="meta-line">First: 2025-09-27T10:33:41+00:00 · Latest: 2025-10-28T20:33:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.23234v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.23234v4">PDF</a> · <a href="https://github.com/ryttry/p-less">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Obtaining high-quality outputs from Large Language Models (LLMs) often
depends upon the choice of a sampling-based decoding strategy to
probabilistically choose the next token at each generation step. While a
variety of such sampling methods have been proposed, their performance can be
sensitive to the selection of hyperparameters which may require different
settings depending upon the generation task and temperature configuration. In
this work, we introduce $p$-less sampling: an information-theoretic approach to
sampling which dynamically sets a truncation threshold at each decoding step
based on the entire token probability distribution. Unlike existing methods,
$p$-less sampling has no hyperparameters and consistently produces high-quality
outputs as temperature increases. We provide theoretical perspectives on
$p$-less sampling to ground our proposed method and conduct experiments to
empirically validate its effectiveness across a range of math, logical
reasoning, and creative writing tasks. Our results demonstrate how $p$-less
sampling consistently outperforms existing sampling approaches while exhibiting
much less degradation in text quality at higher temperature values. We further
show how $p$-less achieves greater inference-time efficiency than alternative
methods through lower average token sampling times and shorter generation
lengths, without sacrificing accuracy. Finally, we provide analyses to
highlight the benefits of $p$-less through qualitative examples, case studies,
and diversity assessments. The code is available at
https://github.com/ryttry/p-less .</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step.</div>
</details>
</div>
<div class="card">
<div class="title">CANDI: Hybrid Discrete-Continuous Diffusion Models</div>
<div class="meta-line">Authors: Patrick Pynadath, Jiaxin Shi, Ruqi Zhang</div>
<div class="meta-line">First: 2025-10-26T03:24:31+00:00 · Latest: 2025-10-28T19:55:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22510v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.22510v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://patrickpynadath1.github.io/candi-lander">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While continuous diffusion has shown remarkable success in continuous domains
such as image generation, its direct application to discrete data has
underperformed compared to purely discrete formulations. This gap is
counterintuitive, given that continuous diffusion learns score functions that
enable joint evolution across multiple positions. To understand this gap, we
introduce token identifiability as an analytical framework for understanding
how Gaussian noise corrupts discrete data through two mechanisms: discrete
identity corruption and continuous rank degradation. We reveal that these
mechanisms scale differently with vocabulary size, creating a temporal
dissonance: at noise levels where discrete corruption preserves enough
structure for conditional learning, continuous denoising is trivial; at noise
levels where continuous denoising is meaningful, discrete corruption destroys
nearly all conditional structure. To solve this, we propose CANDI (Continuous
ANd DIscrete diffusion), a hybrid framework that decouples discrete and
continuous corruption, enabling simultaneous learning of both conditional
structure and continuous geometry. We empirically validate the temporal
dissonance phenomenon and demonstrate that CANDI successfully avoids it. This
unlocks the benefits of continuous diffusion for discrete spaces: on controlled
generation, CANDI enables classifier-based guidance with off-the-shelf
classifiers through simple gradient addition; on text generation, CANDI
outperforms masked diffusion at low NFE, demonstrating the value of learning
continuous gradients for discrete spaces. We include the code on the project
page available here: https://patrickpynadath1.github.io/candi-lander</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While continuous diffusion has shown remarkable success in continuous domains such as image generation, its direct application to discrete data has underperformed compared to purely discrete formulations.</div>
</details>
</div>
<div class="card">
<div class="title">Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language   Models</div>
<div class="meta-line">Authors: Yukun Huang, Sanxing Chen, Jian Pei, Manzil Zaheer, Bhuwan Dhingra</div>
<div class="meta-line">First: 2025-06-21T04:48:05+00:00 · Latest: 2025-10-28T18:06:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.17585v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.17585v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Trustworthy language models should provide both correct and verifiable
answers. However, citations generated directly by standalone LLMs are often
unreliable. As a result, current systems insert citations by querying an
external retriever at inference time, introducing latency, infrastructure
dependence, and vulnerability to retrieval noise. We explore whether LLMs can
be made to reliably attribute to the documents seen during continual
pretraining without test-time retrieval, by revising the training process. To
study this, we construct CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel documents and probes both
short-form (single-fact) and long-form (multi-fact) citation tasks. Our
approach follows a two-stage process: (1) continual pretraining to index
factual knowledge by binding it to persistent document identifiers; and (2)
instruction tuning to elicit citation behavior. We introduce Active Indexing
for the first stage, which creates generalizable, source-anchored bindings by
augmenting training with synthetic data that (i) restate each fact in diverse,
compositional forms and (ii) enforce bidirectional training (source-to-fact and
fact-to-source). This equips the model to both generate content from a cited
source and attribute its own answers, improving robustness to paraphrase and
composition. Experiments with Qwen-2.5-7B&amp;3B show that Active Indexing
consistently outperforms a Passive Indexing baseline, which simply appends an
identifier to each document, achieving citation precision gains of up to 30.2%
across all tasks and models. Our ablation studies reveal that performance
continues to improve as we scale the amount of augmented data, showing a clear
upward trend even at 16x the original token count. Finally, we show that
internal citations complement external ones by making the model more robust to
retrieval noise.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Trustworthy language models should provide both correct and verifiable answers.</div>
</details>
</div>
<div class="card">
<div class="title">Uniform Discrete Diffusion with Metric Path for Video Generation</div>
<div class="meta-line">Authors: Haoge Deng, Ting Pan, Fan Zhang, Yang Liu, Zhuoyan Luo, Yufeng Cui, Wenxuan Wang, Chunhua Shen, Shiguang Shan, Zhaoxiang Zhang, Xinlong Wang</div>
<div class="meta-line">First: 2025-10-28T17:59:57+00:00 · Latest: 2025-10-28T17:59:57+00:00</div>
<div class="meta-line">Comments: 19 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24717v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24717v1">PDF</a> · <a href="https://github.com/baaivision/URSA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency.</div>
</details>
</div>
<div class="card">
<div class="title">Memory Mosaics at scale</div>
<div class="meta-line">Authors: Jianyu Zhang, Léon Bottou</div>
<div class="meta-line">Venue: NeurIPS 2025 Oral</div>
<div class="meta-line">First: 2025-07-04T04:23:03+00:00 · Latest: 2025-10-28T17:59:36+00:00</div>
<div class="meta-line">Comments: Oral @ NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.03285v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.03285v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory Mosaics [Zhang et al., 2025], networks of associative memories, have
demonstrated appealing compositional and in-context learning capabilities on
medium-scale networks (GPT-2 scale) and synthetic small datasets. This work
shows that these favorable properties remain when we scale memory mosaics to
large language model sizes (llama-8B scale) and real-world datasets.
  To this end, we scale memory mosaics to 10B size, we train them on one
trillion tokens, we introduce a couple architectural modifications (&quot;Memory
Mosaics v2&quot;), we assess their capabilities across three evaluation dimensions:
training-knowledge storage, new-knowledge storage, and in-context learning.
  Throughout the evaluation, memory mosaics v2 match transformers on the
learning of training knowledge (first dimension) and significantly outperforms
transformers on carrying out new tasks at inference time (second and third
dimensions). These improvements cannot be easily replicated by simply
increasing the training data for transformers. A memory mosaics v2 trained on
one trillion tokens still perform better on these tasks than a transformer
trained on eight trillion tokens.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Memory Mosaics [Zhang et al., 2025], networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets.</div>
</details>
</div>
<div class="card">
<div class="title">Routing Matters in MoE: Scaling Diffusion Transformers with Explicit   Routing Guidance</div>
<div class="meta-line">Authors: Yujie Wei, Shiwei Zhang, Hangjie Yuan, Yujin Han, Zhekai Chen, Jiayu Wang, Difan Zou, Xihui Liu, Yingya Zhang, Yu Liu, Hongming Shan</div>
<div class="meta-line">First: 2025-10-28T17:59:02+00:00 · Latest: 2025-10-28T17:59:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24711v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency.</div>
</details>
</div>
<div class="card">
<div class="title">Tongyi DeepResearch Technical Report</div>
<div class="meta-line">Authors: Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang</div>
<div class="meta-line">First: 2025-10-28T17:53:02+00:00 · Latest: 2025-10-28T17:53:02+00:00</div>
<div class="meta-line">Comments: https://tongyi-agent.github.io/blog</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24701v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24701v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tongyi-agent.github.io/blog">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Tongyi DeepResearch, an agentic large language model, which is
specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is
developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent
interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total
parameters, with only 3.3 billion activated per token, achieves
state-of-the-art performance across a range of agentic deep research
benchmarks, including Humanity&#x27;s Last Exam, BrowseComp, BrowseComp-ZH,
WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We
open-source the model, framework, and complete solutions to empower the
community.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Scheduling Your LLM Reinforcement Learning with Reasoning Trees</div>
<div class="meta-line">Authors: Hong Wang, Zhezheng Hao, Jian Luo, Chenxing Wei, Yao Shu, Lei Liu, Qiang Lin, Hande Dong, Jiawei Chen</div>
<div class="meta-line">First: 2025-10-28T17:52:07+00:00 · Latest: 2025-10-28T17:52:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24832v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large
Language Models (LLMs) can be conceptualized as progressively editing a query&#x27;s
`Reasoning Tree&#x27;. This process involves exploring nodes (tokens) and
dynamically modifying the model&#x27;s policy at each node. When combined with data
scheduling, this process yields further gains in data efficiency and accuracy.
However, existing RLVR data scheduling methods typically rely on path-based
metrics to rank queries, overlooking the reasoning tree structures of these
queries. In this paper, we introduce a novel metric, namely Reasoning Score
(r-score), which measures the query&#x27;s learning difficulty based on the
structure of its reasoning tree. Based on the r-score, we propose the Reasoning
Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a
curriculum progressing from structurally simple (high r-score) to complex (low
r-score) queries. Experiments on six math-reasoning benchmarks show that
Re-Schedule significantly improves average accuracy, achieving gains of up to
3.2%. These strong results validate our approach and demonstrate that a
structural understanding of the reasoning tree provides a more powerful and
principled foundation for RLVR data scheduling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large Language Models (LLMs) can be conceptualized as progressively editing a query&#x27;s `Reasoning Tree&#x27;.</div>
</details>
</div>
<div class="card">
<div class="title">ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking</div>
<div class="meta-line">Authors: Baixuan Li, Dingchu Zhang, Jialong Wu, Wenbiao Yin, Zhengwei Tao, Yida Zhao, Liwen Zhang, Haiyang Shen, Runnan Fang, Pengjun Xie, Jingren Zhou, Yong Jiang</div>
<div class="meta-line">First: 2025-10-28T17:51:50+00:00 · Latest: 2025-10-28T17:51:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24698v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.24698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parallel thinking expands exploration breadth, complementing the deep
exploration of information-seeking (IS) agents to further enhance
problem-solving capability. However, conventional parallel thinking faces two
key challenges in this setting: inefficiency from repeatedly rolling out from
scratch, and difficulty in integrating long-horizon reasoning trajectories
during answer generation, as limited context capacity prevents full
consideration of the reasoning process. To address these issues, we propose
ParallelMuse, a two-stage paradigm designed for deep IS agents. The first
stage, Functionality-Specified Partial Rollout, partitions generated sequences
into functional regions and performs uncertainty-guided path reuse and
branching to enhance exploration efficiency. The second stage, Compressed
Reasoning Aggregation, exploits reasoning redundancy to losslessly compress
information relevant to answer derivation and synthesize a coherent final
answer. Experiments across multiple open-source agents and benchmarks
demonstrate up to 62% performance improvement with a 10--30% reduction in
exploratory token consumption.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
