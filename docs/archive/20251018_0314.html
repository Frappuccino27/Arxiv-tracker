<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-18 03:14</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251018_0314</div>
    <div class="row"><div class="card">
<div class="title">Attention Is All You Need for KV Cache in Diffusion LLMs</div>
<div class="meta-line">Authors: Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen</div>
<div class="meta-line">First: 2025-10-16T17:59:48+00:00 · Latest: 2025-10-16T17:59:48+00:00</div>
<div class="meta-line">Comments: https://vila-lab.github.io/elastic-cache-webpage/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14973v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14973v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vila-lab.github.io/elastic-cache-webpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work studies how to adaptively recompute key-value (KV) caches for
diffusion large language models (DLMs) to maximize prediction accuracy while
minimizing decoding latency. Prior methods&#x27; decoders recompute QKV for all
tokens at every denoising step and layer, despite KV states changing little
across most steps, especially in shallow layers, leading to substantial
redundancy. We make three observations: (1) distant ${\bf MASK}$ tokens
primarily act as a length-bias and can be cached block-wise beyond the active
prediction window; (2) KV dynamics increase with depth, suggesting that
selective refresh starting from deeper layers is sufficient; and (3) the
most-attended token exhibits the smallest KV drift, providing a conservative
lower bound on cache change for other tokens. Building on these, we propose
${\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that
jointly decides ${when}$ to refresh (via an attention-aware drift test on the
most-attended token) and ${where}$ to refresh (via a depth-aware schedule that
recomputes from a chosen layer onward while reusing shallow-layer caches and
off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs
adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant
computation and accelerating decoding with negligible loss in generation
quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across
mathematical reasoning and code generation tasks demonstrate consistent
speedups: $8.7\times$ on GSM8K (256 tokens), $45.1\times$ on longer sequences,
and $4.8\times$ on HumanEval, while consistently maintaining higher accuracy
than the baseline. Our method achieves significantly higher throughput
($6.8\times$ on GSM8K) than existing confidence-based approaches while
preserving generation quality, enabling practical deployment of diffusion LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency.</div>
</details>
</div>
<div class="card">
<div class="title">TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar</div>
<div class="meta-line">Authors: Yinxi Li, Yuntian Deng, Pengyu Nie</div>
<div class="meta-line">First: 2025-10-16T17:59:45+00:00 · Latest: 2025-10-16T17:59:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14972v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14972v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) for code rely on subword tokenizers, such as
byte-pair encoding (BPE), learned from mixed natural language text and
programming language code but driven by statistics rather than grammar. As a
result, semantically identical code snippets can be tokenized differently
depending on superficial factors such as whitespace or identifier naming. To
measure the impact of this misalignment, we introduce TokDrift, a framework
that applies semantic-preserving rewrite rules to create code variants
differing only in tokenization. Across nine code LLMs, including large ones
with over 30B parameters, even minor formatting changes can cause substantial
shifts in model behavior. Layer-wise analysis shows that the issue originates
in early embeddings, where subword segmentation fails to capture grammar token
boundaries. Our findings identify misaligned tokenization as a hidden obstacle
to reliable code understanding and generation, highlighting the need for
grammar-aware tokenization for future code LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar.</div>
</details>
</div>
<div class="card">
<div class="title">RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention   Diffusion</div>
<div class="meta-line">Authors: Thao Nguyen, Jiaqi Ma, Fahad Shahbaz Khan, Souhaib Ben Taieb, Salman Khan</div>
<div class="meta-line">First: 2025-10-16T17:59:13+00:00 · Latest: 2025-10-16T17:59:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14962v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14962v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Precipitation nowcasting, predicting future radar echo sequences from current
observations, is a critical yet challenging task due to the inherently chaotic
and tightly coupled spatio-temporal dynamics of the atmosphere. While recent
advances in diffusion-based models attempt to capture both large-scale motion
and fine-grained stochastic variability, they often suffer from scalability
issues: latent-space approaches require a separately trained autoencoder,
adding complexity and limiting generalization, while pixel-space approaches are
computationally intensive and often omit attention mechanisms, reducing their
ability to model long-range spatio-temporal dependencies. To address these
limitations, we propose a Token-wise Attention integrated into not only the
U-Net diffusion model but also the spatio-temporal encoder that dynamically
captures multi-scale spatial interactions and temporal evolution. Unlike prior
approaches, our method natively integrates attention into the architecture
without incurring the high resource cost typical of pixel-space diffusion,
thereby eliminating the need for separate latent modules. Our extensive
experiments and visual evaluations across diverse datasets demonstrate that the
proposed method significantly outperforms state-of-the-art approaches, yielding
superior local fidelity, generalization, and robustness in complex
precipitation forecasting scenarios.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere.</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Parallel Samplers for Recurrent-Depth Models and Their   Connection to Diffusion Language Models</div>
<div class="meta-line">Authors: Jonas Geiping, Xinyu Yang, Guinan Su</div>
<div class="meta-line">First: 2025-10-16T17:59:07+00:00 · Latest: 2025-10-16T17:59:07+00:00</div>
<div class="meta-line">Comments: Code can be found at https://github.com/seal-rg/recurrent-pretraining</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14961v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14961v1">PDF</a> · <a href="https://github.com/seal-rg/recurrent-pretraining">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models with recurrent depth, also referred to as universal or looped
when considering transformers, are defined by the capacity to increase their
computation through the repetition of layers. Recent efforts in pretraining
have demonstrated that these architectures can scale to modern language
modeling tasks while exhibiting advantages in reasoning tasks. In this work, we
examine the relationship between recurrent-depth models and diffusion language
models. Building on their similarities, we develop a new diffusion forcing
sampler for these models to accelerate generation. The sampler advances by
decoding new tokens at every forward pass of the model, while the latent states
of these tokens can be further refined in parallel through recurrence.
Theoretically, generation with our sampler is strictly more expressive than the
baseline autoregressive generation using the same time budget on modern
hardware. Moreover, this sampler, based on principles from diffusion
literature, can be directly applied to existing 3.5B recurrent-depth
transformers without any tuning, leading to up to a 5x speedup. Consequently,
our findings not only provide an efficient mechanism for parallelizing the
extra computation in recurrent-depth models at inference, but also suggest that
such models can be naturally viewed as strong continuous, though causal,
diffusion language models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers.</div>
</details>
</div>
<div class="card">
<div class="title">LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</div>
<div class="meta-line">Authors: Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin</div>
<div class="meta-line">First: 2025-10-16T17:55:11+00:00 · Latest: 2025-10-16T17:55:11+00:00</div>
<div class="meta-line">Comments: Work in progress. Github repo: https://github.com/RUCBM/LaSeR</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14943v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14943v1">PDF</a> · <a href="https://github.com/RUCBM/LaSeR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a core paradigm for enhancing the reasoning capabilities of Large Language
Models (LLMs). To address the lack of verification signals at test time, prior
studies incorporate the training of model&#x27;s self-verification capability into
the standard RLVR process, thereby unifying reasoning and verification
capabilities within a single LLM. However, previous practice requires the LLM
to sequentially generate solutions and self-verifications using two separate
prompt templates, which significantly reduces efficiency. In this work, we
theoretically reveal that the closed-form solution to the RL objective of
self-verification can be reduced to a remarkably simple form: the true
reasoning reward of a solution is equal to its last-token self-rewarding score,
which is computed as the difference between the policy model&#x27;s next-token
log-probability assigned to any pre-specified token at the solution&#x27;s last
token and a pre-calculated constant, scaled by the KL coefficient. Based on
this insight, we propose LaSeR (Reinforcement Learning with Last-Token
Self-Rewarding), an algorithm that simply augments the original RLVR loss with
a MSE loss that aligns the last-token self-rewarding scores with verifier-based
reasoning rewards, jointly optimizing the reasoning and self-rewarding
capabilities of LLMs. The optimized self-rewarding scores can be utilized in
both training and testing to enhance model performance. Notably, our algorithm
derives these scores from the predicted next-token probability distribution of
the last token immediately after generation, incurring only the minimal extra
cost of one additional token inference. Experiments show that our method not
only improves the model&#x27;s reasoning performance but also equips it with
remarkable self-rewarding capability, thereby boosting its inference-time
scaling performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Why is Your Language Model a Poor Implicit Reward Model?</div>
<div class="meta-line">Authors: Noam Razin, Yong Lin, Jiarui Yao, Sanjeev Arora</div>
<div class="meta-line">First: 2025-07-10T17:55:05+00:00 · Latest: 2025-10-16T17:45:44+00:00</div>
<div class="meta-line">Comments: Code available at https://github.com/princeton-pli/exrm-vs-imrm</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.07981v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.07981v2">PDF</a> · <a href="https://github.com/princeton-pli/exrm-vs-imrm">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Toward a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reward models are key to language model post-training and inference pipelines.</div>
</details>
</div>
<div class="card">
<div class="title">The Gatekeeper Knows Enough</div>
<div class="meta-line">Authors: Fikresilase Wondmeneh Abebayew</div>
<div class="meta-line">First: 2025-10-16T17:00:42+00:00 · Latest: 2025-10-16T17:00:42+00:00</div>
<div class="meta-line">Comments: 7 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14881v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14881v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs&#x27; stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity &quot;latent state&quot; representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent&#x27;s model of the
system remains verifiably grounded in the system&#x27;s reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs&#x27; stateless nature and inefficient context management.</div>
</details>
</div>
<div class="card">
<div class="title">Thinker: Learning to Think Fast and Slow</div>
<div class="meta-line">Authors: Stephen Chung, Wenyu Du, Jie Fu</div>
<div class="meta-line">First: 2025-05-27T12:22:46+00:00 · Latest: 2025-10-16T16:20:17+00:00</div>
<div class="meta-line">Comments: 23 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.21097v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.21097v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies show that the reasoning capabilities of Large Language Models
(LLMs) can be improved by applying Reinforcement Learning (RL) to
question-answering (QA) tasks in areas such as math and coding. With a long
context length, LLMs may learn to perform search, as indicated by the
self-correction behavior observed in DeepSeek R1. However, this search behavior
is often imprecise and lacks confidence, resulting in long, redundant responses
and highlighting deficiencies in intuition and verification. Inspired by the
Dual Process Theory in psychology, we introduce a simple modification to the QA
task that includes four stages: Fast Thinking, where the LLM must answer within
a strict token budget; Verification, where the model evaluates its initial
response; Slow Thinking, where it refines the initial response with more
deliberation; and Summarization, where it distills the refinement from the
previous stage into precise steps. Our proposed task improves average accuracy
from 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for
DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone
achieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial
inference efficiency gains. These findings suggest that intuition and
deliberative reasoning are distinct, complementary systems benefiting from
targeted training. Additionally, we have open-sourced both the trained models
and the source code.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding.</div>
</details>
</div>
<div class="card">
<div class="title">QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for   Vision-Language-Action Models</div>
<div class="meta-line">Authors: Yixuan Li, Yuhui Chen, Mingcai Zhou, Haoran Li</div>
<div class="meta-line">First: 2025-10-16T16:11:18+00:00 · Latest: 2025-10-16T16:11:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14836v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14836v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)
models to accomplish fine-grained manipulation tasks. However, existing
approaches often lack the ability to understand and reason over the essential
3D structures necessary for precise control. To address this limitation, we
propose QDepth-VLA, a general framework that augments VLA models with an
auxiliary depth prediction task. A dedicated depth expert is designed to
predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,
enabling the model to learn depth-aware representations that capture critical
geometric cues. Experimental results on the simulation benchmarks and
real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning
and competitive performance on manipulation tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Supervised Fine-Tuning or Contrastive Learning? Towards Better   Multimodal LLM Reranking</div>
<div class="meta-line">Authors: Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang</div>
<div class="meta-line">First: 2025-10-16T16:02:27+00:00 · Latest: 2025-10-16T16:02:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14824v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14824v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In information retrieval, training reranking models mainly focuses on two
types of objectives: metric learning (e.g. contrastive loss to increase the
predicted scores on relevant query-document pairs) and classification (binary
label prediction of relevance vs. irrelevance). For BERT-style encoders,
various studies have shown that contrastive learning (CL) can be more effective
than discriminative (classification) learning. However, for large language
models (LLMs), classification via supervised fine-tuning (SFT), which predicts
&#x27;&#x27;yes&#x27;&#x27; (resp. &#x27;&#x27;no&#x27;&#x27;) token for relevant (resp. irrelevant) pairs, appears
more promising as it aligns well with the generative nature of LLMs. This
divergence raises a central question: which objective is intrinsically better
suited to LLM-based reranking, and what mechanism underlies the difference? In
this work, we conduct a comprehensive comparison and analysis between CL and
SFT for reranking, taking the universal multimodal retrieval (UMR) as the
experimental playground. We first decompose the objectives into two components:
weight, which controls the magnitude of those updates, and direction, which
guides the model updates, then present a unified framework for understanding
their interactions. Through probing experiments, we find that SFT provides a
substantially stronger weighting scheme than CL, whereas the preferred scoring
direction shows no clear winner. Taken together, these results point to a
consistent advantage of SFT over CL for LLM reranking. To further validate our
findings, we conduct large-scale training with SFT and present new
state-of-the-art rerankers on the MRB benchmark. We also provide ablations on
SFT settings and expect our findings to benefit future research and
applications in this area.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g.</div>
</details>
</div>
<div class="card">
<div class="title">Agentic NL2SQL to Reduce Computational Costs</div>
<div class="meta-line">Authors: Dominik Jehle, Lennart Purucker, Frank Hutter</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-16T15:42:28+00:00 · Latest: 2025-10-16T15:42:28+00:00</div>
<div class="meta-line">Comments: Accepted at the NeurIPS 2025 Workshop on Efficient Reasoning. 10
  pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14808v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14808v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL) has recently been empowered by large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">SimKO: Simple Pass@K Policy Optimization</div>
<div class="meta-line">Authors: Ruotian Peng, Yi Ren, Zhouliang Yu, Weiyang Liu, Yandong Wen</div>
<div class="meta-line">First: 2025-10-16T15:40:49+00:00 · Latest: 2025-10-16T15:40:49+00:00</div>
<div class="meta-line">Comments: Technical report (20 pages, 10 figures, project page:
  https://spherelab.ai/simko/)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14807v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spherelab.ai/simko/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K&gt;1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR&#x27;s exploration.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries</div>
<div class="meta-line">Authors: Divyat Mahajan, Sachin Goyal, Badr Youbi Idrissi, Mohammad Pezeshki, Ioannis Mitliagkas, David Lopez-Paz, Kartik Ahuja</div>
<div class="meta-line">First: 2025-10-16T14:52:52+00:00 · Latest: 2025-10-16T14:52:52+00:00</div>
<div class="meta-line">Comments: Preprint. Under Review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14751v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14751v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Next-token prediction (NTP) has driven the success of large language models
(LLMs), but it struggles with long-horizon reasoning, planning, and creative
writing, with these limitations largely attributed to teacher-forced training.
Multi-token prediction (MTP) partially mitigates these issues by predicting
several future tokens at once, but it mostly captures short-range dependencies
and offers limited improvement. We propose future summary prediction (FSP),
which trains an auxiliary head to predict a compact representation of the
long-term future, preserving information relevant for long-form generations. We
explore two variants of FSP: handcrafted summaries, for example, a bag of words
summary of the future of the sequence, and learned summaries, which use
embeddings produced by a reverse language model trained from right to left.
Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate
that FSP provides improvements over both NTP and MTP across math, reasoning,
and coding benchmarks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Next-token prediction (NTP) has driven the success of large language models (LLMs), but it struggles with long-horizon reasoning, planning, and creative writing, with these limitations largely attributed to teacher-forced training.</div>
</details>
</div>
<div class="card">
<div class="title">Moto: Latent Motion Token as the Bridging Language for Learning Robot   Manipulation from Videos</div>
<div class="meta-line">Authors: Yi Chen, Yuying Ge, Weiliang Tang, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, Xihui Liu</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2024-12-05T18:57:04+00:00 · Latest: 2025-10-16T14:13:23+00:00</div>
<div class="meta-line">Comments: ICCV 2025. Project page: https://chenyi99.github.io/moto/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.04445v4">Abs</a> · <a href="http://arxiv.org/pdf/2412.04445v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://chenyi99.github.io/moto/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent developments in Large Language Models pre-trained on extensive corpora
have shown significant success in various natural language processing tasks
with minimal fine-tuning. This success offers new promise for robotics, which
has long been constrained by the high cost of action-labeled data. We ask:
given the abundant video data containing interaction-related knowledge
available as a rich &quot;corpus&quot;, can a similar generative pre-training approach be
effectively applied to enhance robot learning? The key challenge is to identify
an effective representation for autoregressive pre-training that benefits robot
manipulation tasks. Inspired by the way humans learn new skills through
observing dynamic environments, we propose that effective robotic learning
should emphasize motion-related knowledge, which is closely tied to low-level
actions and is hardware-agnostic, facilitating the transfer of learned motions
to actual robot actions. To this end, we introduce Moto, which converts video
content into latent Motion Token sequences by a Latent Motion Tokenizer,
learning a bridging &quot;language&quot; of motion from videos in an unsupervised manner.
We pre-train Moto-GPT through motion token autoregression, enabling it to
capture diverse visual motion knowledge. After pre-training, Moto-GPT
demonstrates the promising ability to produce semantically interpretable motion
tokens, predict plausible motion trajectories, and assess trajectory
rationality through output likelihood. To transfer learned motion priors to
real robot actions, we implement a co-fine-tuning strategy that seamlessly
bridges latent motion token prediction and real robot control. Extensive
experiments show that the fine-tuned Moto-GPT exhibits superior robustness and
efficiency on robot manipulation benchmarks, underscoring its effectiveness in
transferring knowledge from video data to downstream visual manipulation tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning.</div>
</details>
</div>
<div class="card">
<div class="title">Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion   Transformer</div>
<div class="meta-line">Authors: Mohsen Ghafoorian, Denis Korzhenkov, Amirhossein Habibian</div>
<div class="meta-line">First: 2025-09-29T15:09:51+00:00 · Latest: 2025-10-16T13:51:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.24899v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.24899v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://qualcomm-ai-research.github.io/attention-surgery">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based video diffusion models (VDMs) deliver state-of-the-art
video generation quality but are constrained by the quadratic cost of
self-attention, making long sequences and high resolutions computationally
expensive. While linear attention offers sub-quadratic complexity, prior
attempts fail to match the expressiveness of softmax attention without costly
retraining. We introduce Attention Surgery, an efficient framework for
linearizing or hybridizing attention in pretrained VDMs without training from
scratch. Inspired by recent advances in language models, our method combines a
novel hybrid attention mechanism-mixing softmax and linear tokens-with a
lightweight distillation and fine-tuning pipeline requiring only a few
GPU-days. Additionally, we incorporate a cost-aware block-rate strategy to
balance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a
state-of-the-art DiT-based VDM, Attention Surgery achieves the first
competitive sub-quadratic attention video diffusion models, reducing attention
cost by up to 40\% in terms of FLOPs, while maintaining generation quality as
measured on the standard VBench and VBench-2.0 benchmarks. Project page is
available at: https://qualcomm-ai-research.github.io/attention-surgery.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformer-based video diffusion models (VDMs) deliver state-of-the-art video generation quality but are constrained by the quadratic cost of self-attention, making long sequences and high resolutions computationally expensive.</div>
</details>
</div>
<div class="card">
<div class="title">On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification</div>
<div class="meta-line">Authors: Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, Xu Yang</div>
<div class="meta-line">First: 2025-08-07T17:59:04+00:00 · Latest: 2025-10-16T13:40:55+00:00</div>
<div class="meta-line">Comments: 14 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.05629v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.05629v2">PDF</a> · <a href="https://github.com/yongliang-wu/DFT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a simple yet theoretically motivated improvement to Supervised
Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited
generalization compared to reinforcement learning (RL). Through mathematical
analysis, we reveal that standard SFT gradients implicitly encode a problematic
reward structure that may severely restrict the generalization capabilities of
model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing
gradient updates for each token by dynamically rescaling the objective function
with the probability of this token. Remarkably, this single-line code change
significantly outperforms standard SFT across multiple challenging benchmarks
and base models, demonstrating greatly improved generalization. Additionally,
our approach shows competitive results in offline RL settings, offering an
effective yet simpler alternative. This work bridges theoretical insight and
practical solutions, substantially advancing SFT performance. The code will be
available at https://github.com/yongliang-wu/DFT.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL).</div>
</details>
</div>
<div class="card">
<div class="title">EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through   an Encoder-Decoder Architecture</div>
<div class="meta-line">Authors: Wenfeng Feng, Hongxiang Wang, Jianlong Wang, Xin Zhang, Jingjing Zhao, Yueyue Liang, Xiang Chen, Duokui Han</div>
<div class="meta-line">First: 2025-04-09T09:51:41+00:00 · Latest: 2025-10-16T12:43:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.06738v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.06738v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel
architecture designed to mitigate the attention sink phenomenon observed in
Vision Transformer models. Attention sink occurs when an excessive amount of
attention is allocated to the [CLS] token, distorting the model&#x27;s ability to
effectively process image patches. To address this, we introduce a
layer-aligned encoder-decoder architecture, where the encoder utilizes
self-attention to process image patches, while the decoder uses cross-attention
to focus on the [CLS] token. Unlike traditional encoder-decoder framework,
where the decoder depends solely on high-level encoder representations, EDIT
allows the decoder to extract information starting from low-level features,
progressively refining the representation layer by layer. EDIT is naturally
interpretable demonstrated through sequential attention maps, illustrating the
refined, layer-by-layer focus on key image features. Experiments on ImageNet-1k
and ImageNet-21k, along with transfer learning tasks, show that EDIT achieves
consistent performance improvements over DeiT3 models. These results highlight
the effectiveness of EDIT&#x27;s design in addressing attention sink and improving
visual feature extraction.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel architecture designed to mitigate the attention sink phenomenon observed in Vision Transformer models.</div>
</details>
</div>
<div class="card">
<div class="title">Adapting Self-Supervised Representations as a Latent Space for Efficient   Generation</div>
<div class="meta-line">Authors: Ming Gui, Johannes Schusterbauer, Timy Phan, Felix Krause, Josh Susskind, Miguel Angel Bautista, Björn Ommer</div>
<div class="meta-line">First: 2025-10-16T12:43:03+00:00 · Latest: 2025-10-16T12:43:03+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/CompVis/RepTok</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14630v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14630v1">PDF</a> · <a href="https://github.com/CompVis/RepTok">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Representation Tokenizer (RepTok), a generative modeling
framework that represents an image using a single continuous latent token
obtained from self-supervised vision transformers. Building on a pre-trained
SSL encoder, we fine-tune only the semantic token embedding and pair it with a
generative decoder trained jointly using a standard flow matching objective.
This adaptation enriches the token with low-level, reconstruction-relevant
details, enabling faithful image reconstruction. To preserve the favorable
geometry of the original SSL space, we add a cosine-similarity loss that
regularizes the adapted token, ensuring the latent space remains smooth and
suitable for generation. Our single-token formulation resolves spatial
redundancies of 2D latent spaces and significantly reduces training costs.
Despite its simplicity and efficiency, RepTok achieves competitive results on
class-conditional ImageNet generation and naturally extends to text-to-image
synthesis, reaching competitive zero-shot performance on MS-COCO under
extremely limited training budgets. Our findings highlight the potential of
fine-tuned SSL representations as compact and effective latent spaces for
efficient generative modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce Representation Tokenizer (RepTok), a generative modeling framework that represents an image using a single continuous latent token obtained from self-supervised vision transformers.</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster   VLM Inference</div>
<div class="meta-line">Authors: Natan Bagrov, Eugene Khvedchenia, Borys Tymchenko, Shay Aharon, Lior Kadoch, Tomer Keren, Ofri Masad, Yonatan Geifman, Ran Zilberstein, Tuomas Rintamaki, Matthieu Le, Andrew Tao</div>
<div class="meta-line">First: 2025-10-16T12:34:38+00:00 · Latest: 2025-10-16T12:34:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14624v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14624v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have recently expanded from static image
understanding to video reasoning, but their scalability is fundamentally
limited by the quadratic cost of processing dense frame sequences. Long videos
often exceed the token budget of modern language models, leading to severe
context limitations and latency issues. We introduce Efficient Video Sampling
(EVS), a simple, plug-and-play method for reducing token redundancy in videos
by identifying and pruning temporally static patches -- spatial regions that
remain unchanged across consecutive frames. EVS preserves positional identity,
requires no architectural changes or retraining. We show that EVS substantially
reduces token count while maintaining semantic fidelity, enabling faster
inference and longer input sequences. Applied at inference time, EVS reduces
large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
accuracy loss. When combined with an uptraining phase using stochastic pruning
rates, EVS yields models that are robust to varying compression levels and
retain full performance under aggressive pruning. Extensive experiments
demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
unlocking scalable video-language understanding without sacrificing quality.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models (VLMs) have recently expanded from static image understanding to video reasoning, but their scalability is fundamentally limited by the quadratic cost of processing dense frame sequences.</div>
</details>
</div>
<div class="card">
<div class="title">3DOT: Texture Transfer for 3DGS Objects from a Single Reference Image</div>
<div class="meta-line">Authors: Xiao Cao, Beibei Lin, Bo Wang, Zhiyong Huang, Robby T. Tan</div>
<div class="meta-line">First: 2025-03-24T16:31:52+00:00 · Latest: 2025-10-16T12:29:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.18853v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.18853v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D texture swapping allows for the customization of 3D object textures,
enabling efficient and versatile visual transformations in 3D editing. While no
dedicated method exists, adapted 2D editing and text-driven 3D editing
approaches can serve this purpose. However, 2D editing requires frame-by-frame
manipulation, causing inconsistencies across views, while text-driven 3D
editing struggles to preserve texture characteristics from reference images. To
tackle these challenges, we introduce 3DSwapping, a 3D texture swapping method
that integrates: 1) progressive generation, 2) view-consistency gradient
guidance, and 3) prompt-tuned gradient guidance. To ensure view consistency,
our progressive generation process starts by editing a single reference image
and gradually propagates the edits to adjacent views. Our view-consistency
gradient guidance further reinforces consistency by conditioning the generation
model on feature differences between consistent and inconsistent outputs. To
preserve texture characteristics, we introduce prompt-tuning-based gradient
guidance, which learns a token that precisely captures the difference between
the reference image and the 3D object. This token then guides the editing
process, ensuring more consistent texture preservation across views. Overall,
3DSwapping integrates these novel strategies to achieve higher-fidelity texture
transfer while preserving structural coherence across multiple viewpoints.
Extensive qualitative and quantitative evaluations confirm that our three novel
components enable convincing and effective 2D texture swapping for 3D objects.
Code will be available upon acceptance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">3D texture swapping allows for the customization of 3D object textures, enabling efficient and versatile visual transformations in 3D editing.</div>
</details>
</div>
<div class="card">
<div class="title">STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored   Encoding</div>
<div class="meta-line">Authors: Zhifei Chen, Tianshuo Xu, Leyi Wu, Luozhou Wang, Dongyu Yan, Zihan You, Wenting Luo, Guo Zhang, Yingcong Chen</div>
<div class="meta-line">First: 2025-10-16T11:50:38+00:00 · Latest: 2025-10-16T11:50:38+00:00</div>
<div class="meta-line">Comments: Code, model, and demos can be found at
  https://envision-research.github.io/STANCE/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14588v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14588v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://envision-research.github.io/STANCE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation has recently made striking visual progress, but maintaining
coherent object motion and interactions remains difficult. We trace two
practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)
often collapse to too few effective tokens after encoding, weakening guidance;
and (ii) optimizing for appearance and motion in a single head can favor
texture over temporal consistency. We present STANCE, an image-to-video
framework that addresses both issues with two simple components. First, we
introduce Instance Cues -- a pixel-aligned control signal that turns sparse,
user-editable hints into a dense 2.5D (camera-relative) motion field by
averaging per-instance flow and augmenting with monocular depth over the
instance mask. This reduces depth ambiguity compared to 2D arrow inputs while
remaining easy to use. Second, we preserve the salience of these cues in token
space with Dense RoPE, which tags a small set of motion tokens (anchored on the
first frame) with spatial-addressable rotary embeddings. Paired with joint RGB
\(+\) auxiliary-map prediction (segmentation or depth), our model anchors
structure while RGB handles appearance, stabilizing optimization and improving
temporal coherence without requiring per-frame trajectory scripts.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult.</div>
</details>
</div>
<div class="card">
<div class="title">State-Space Models for Tabular Prior-Data Fitted Networks</div>
<div class="meta-line">Authors: Felix Koch, Marcel Wever, Fabian Raisch, Benjamin Tischler</div>
<div class="meta-line">Venue: ICML</div>
<div class="meta-line">First: 2025-10-16T11:31:51+00:00 · Latest: 2025-10-16T11:31:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14573v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14573v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in foundation models for tabular data, such as TabPFN,
demonstrated that pretrained Transformer architectures can approximate Bayesian
inference with high predictive performance. However, Transformers suffer from
quadratic complexity with respect to sequence length, motivating the
exploration of more efficient sequence models. In this work, we investigate the
potential of using Hydra, a bidirectional linear-time structured state space
model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies
in SSM&#x27;s inherent sensitivity to the order of input tokens - an undesirable
property for tabular datasets where the row order is semantically meaningless.
We investigate to what extent a bidirectional approach can preserve efficiency
and enable symmetric context aggregation. Our experiments show that this
approach reduces the order-dependence, achieving predictive performance
competitive to the original TabPFN model.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advancements in foundation models for tabular data, such as TabPFN, demonstrated that pretrained Transformer architectures can approximate Bayesian inference with high predictive performance.</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning in Space via Grounding in the World</div>
<div class="meta-line">Authors: Yiming Chen, Zekun Qi, Wenyao Zhang, Xin Jin, Li Zhang, Peidong Liu</div>
<div class="meta-line">First: 2025-10-15T17:58:08+00:00 · Latest: 2025-10-16T11:18:03+00:00</div>
<div class="meta-line">Comments: 20 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13800v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.13800v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we claim that 3D visual grounding is the cornerstone of
spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to
explore the effective spatial representations that bridge the gap between them.
Existing 3D LLMs suffer from the absence of a unified 3D representation capable
of jointly capturing semantic and geometric information. This deficiency is
manifested either in poor performance on grounding or in an excessive reliance
on external modules, ultimately hindering the seamless integration of grounding
and spatial reasoning. To address this, we propose a simple yet effective
dual-path pooling mechanism that tightly aligns geometric features with both
semantic and positional cues, constructing a unified image patch-based 3D
representation that encapsulates all essential information without increasing
the number of input tokens. Leveraging this holistic representation,
GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely
without external modules while delivering performance comparable to
state-of-the-art models, establishing a unified and self-contained framework
for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we
introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is
meticulously curated to include both 3D bounding box annotations for objects
referenced in reasoning questions and step-by-step reasoning paths that
integrate grounding as a core component of the problem-solving process.
Extensive experiments demonstrate that GS-Reasoner achieves impressive results
on 3D visual grounding, which in turn significantly enhances its spatial
reasoning capabilities, leading to state-of-the-art performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them.</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Entropy-Balanced Policy Optimization</div>
<div class="meta-line">Authors: Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou</div>
<div class="meta-line">First: 2025-10-16T10:40:52+00:00 · Latest: 2025-10-16T10:40:52+00:00</div>
<div class="meta-line">Comments: Working in progress</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14545v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14545v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity&#x27;s Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity&#x27;s Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents.</div>
</details>
</div>
<div class="card">
<div class="title">JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context   Protocol</div>
<div class="meta-line">Authors: Emanuele Antonioni, Stefan Markovic, Anirudha Shankar, Jaime Bernardo, Lovro Markovic, Silvia Pareti, Benedetto Proietti</div>
<div class="meta-line">First: 2025-10-16T10:28:23+00:00 · Latest: 2025-10-16T10:28:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14537v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today&#x27;s
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user&#x27;s prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent&#x27;s ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AI systems are continually evolving and advancing, and user expectations are concurrently increasing, with a growing demand for interactions that go beyond simple text-based interaction with Large Language Models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image   Misalignment in Diffusion Models</div>
<div class="meta-line">Authors: Yunze Tong, Didi Zhu, Zijing Hu, Jinluan Yang, Ziyu Zhao</div>
<div class="meta-line">First: 2025-10-16T10:14:34+00:00 · Latest: 2025-10-16T10:14:34+00:00</div>
<div class="meta-line">Comments: Appendix will be appended soon</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14526v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14526v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In text-to-image generation, different initial noises induce distinct denoising paths with a pretrained Stable Diffusion (SD) model.</div>
</details>
</div>
<div class="card">
<div class="title">Detecting Token-Level Hallucinations Using Variance Signals: A   Reference-Free Approach</div>
<div class="meta-line">Authors: Keshav Kumar</div>
<div class="meta-line">First: 2025-07-05T19:20:59+00:00 · Latest: 2025-10-16T09:42:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.04137v3">Abs</a> · <a href="http://arxiv.org/pdf/2507.04137v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated impressive generative
capabilities across diverse tasks but remain susceptible to hallucinations,
confidently generated yet factually incorrect outputs. We introduce a
reference-free, token-level hallucination detection framework that leverages
the variance in token log-probabilities across multiple stochastic generations.
Unlike prior methods that require ground-truth references or sentence-level
verification, our approach is model-agnostic, interpretable, and suited for
real-time or post-hoc analysis. We evaluate our method on unanswerable question
prompts from the SQuAD v2 dataset and benchmark across three autoregressive
models of varying scales: GPT-Neo 125M, Falcon 1B, and Mistral 7B. Through both
quantitative metrics and visual diagnostics, we show that token-level variance
reliably highlights instability in model outputs and correlates with
hallucination patterns. Our framework is lightweight, reproducible, and
adaptable to multiple domains, offering a valuable diagnostic tool for
analyzing generative reliability in LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated impressive generative capabilities across diverse tasks but remain susceptible to hallucinations, confidently generated yet factually incorrect outputs.</div>
</details>
</div>
<div class="card">
<div class="title">MIO: A Foundation Model on Multimodal Tokens</div>
<div class="meta-line">Authors: Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang</div>
<div class="meta-line">Venue: EMNLP 2025 Oral</div>
<div class="meta-line">First: 2024-09-26T09:57:16+00:00 · Latest: 2025-10-16T08:18:03+00:00</div>
<div class="meta-line">Comments: EMNLP 2025 (Oral). Codes and models are available in
  https://github.com/MIO-Team/MIO</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2409.17692v4">Abs</a> · <a href="http://arxiv.org/pdf/2409.17692v4">PDF</a> · <a href="https://github.com/MIO-Team/MIO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner.</div>
</details>
</div>
<div class="card">
<div class="title">Low Power Vision Transformer Accelerator with Hardware-Aware Pruning and   Optimized Dataflow</div>
<div class="meta-line">Authors: Ching-Lin Hsiung, Tian-Sheuan Chang</div>
<div class="meta-line">First: 2025-10-16T07:44:42+00:00 · Latest: 2025-10-16T07:44:42+00:00</div>
<div class="meta-line">Comments: 10 pages; IEEE Transactions on Circuits and Systems I: Regular Papers</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14393v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14393v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current transformer accelerators primarily focus on optimizing self-attention
due to its quadratic complexity. However, this focus is less relevant for
vision transformers with short token lengths, where the Feed-Forward Network
(FFN) tends to be the dominant computational bottleneck. This paper presents a
low power Vision Transformer accelerator, optimized through algorithm-hardware
co-design. The model complexity is reduced using hardware-friendly dynamic
token pruning without introducing complex mechanisms. Sparsity is further
improved by replacing GELU with ReLU activations and employing dynamic FFN2
pruning, achieving a 61.5\% reduction in operations and a 59.3\% reduction in
FFN2 weights, with an accuracy loss of less than 2\%. The hardware adopts a
row-wise dataflow with output-oriented data access to eliminate data
transposition, and supports dynamic operations with minimal area overhead.
Implemented in TSMC&#x27;s 28nm CMOS technology, our design occupies 496.4K gates
and includes a 232KB SRAM buffer, achieving a peak throughput of 1024 GOPS at
1GHz, with an energy efficiency of 2.31 TOPS/W and an area efficiency of 858.61
GOPS/mm2.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current transformer accelerators primarily focus on optimizing self-attention due to its quadratic complexity.</div>
</details>
</div>
<div class="card">
<div class="title">FairBatching: Fairness-Aware Batch Formation for LLM Inference</div>
<div class="meta-line">Authors: Hongtao Lyu, Boyue Liu, Mingyu Wu, Haibo Chen</div>
<div class="meta-line">First: 2025-10-16T07:43:56+00:00 · Latest: 2025-10-16T07:43:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.14392v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.14392v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) inference systems face a fundamental tension
between minimizing Time-to-First-Token (TTFT) latency for new requests and
maintaining a high, steady token generation rate (low Time-Per-Output-Token, or
TPOT) for ongoing requests. Existing stall-free batching schedulers proposed by
Sarathi, while effective at preventing decode stalls, introduce significant
computational unfairness. They prioritize decode tasks excessively,
simultaneously leading to underutilized decode slack and unnecessary prefill
queuing delays, which collectively degrade the system&#x27;s overall quality of
service (QoS).
  This work identifies the root cause of this unfairness: the non-monotonic
nature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid
decode-prioritizing policy that fails to adapt to dynamic workload bursts. We
therefore propose FairBatching, a novel LLM inference scheduler that enforces
fair resource allocation between prefill and decode tasks. It features an
adaptive batch capacity determination mechanism, which dynamically adjusts the
computational budget to improve the GPU utilization without triggering SLO
violations. Its fair and dynamic batch formation algorithm breaks away from the
decode-prioritizing paradigm, allowing computation resources to be reclaimed
from bursting decode tasks to serve prefill surges, achieving global fairness.
Furthermore, FairBatching provides a novel load estimation method, enabling
more effective coordination with upper-level schedulers. Implemented and
evaluated on realistic traces, FairBatching significantly reduces TTFT tail
latency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall
20.0% improvement in single-node capacity and 54.3% improvement in
cluster-level capacity.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language model (LLM) inference systems face a fundamental tension between minimizing Time-to-First-Token (TTFT) latency for new requests and maintaining a high, steady token generation rate (low Time-Per-Output-Token, or TPOT) for ongoing requests.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
