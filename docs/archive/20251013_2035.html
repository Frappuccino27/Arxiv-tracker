<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-13 20:35</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251013_2035</div>
    <div class="row"><div class="card">
<div class="title">StreamingVLM: Real-Time Understanding for Infinite Video Streams</div>
<div class="meta-line">Authors: Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, Song Han</div>
<div class="meta-line">First: 2025-10-10T17:59:58+00:00 · Latest: 2025-10-10T17:59:58+00:00</div>
<div class="meta-line">Comments: The first two authors contributed equally to this work</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09608v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09608v1">PDF</a> · <a href="https://github.com/mit-han-lab/streaming-vlm">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) could power real-time assistants and autonomous
agents, but they face a critical challenge: understanding near-infinite video
streams without escalating latency and memory usage. Processing entire videos
with full attention leads to quadratic computational costs and poor performance
on long videos. Meanwhile, simple sliding window methods are also flawed, as
they either break coherence or suffer from high latency due to redundant
recomputation. In this paper, we introduce StreamingVLM, a model designed for
real-time, stable understanding of infinite visual input. Our approach is a
unified framework that aligns training with streaming inference. During
inference, we maintain a compact KV cache by reusing states of attention sinks,
a short window of recent vision tokens, and a long window of recent text
tokens. This streaming ability is instilled via a simple supervised fine-tuning
(SFT) strategy that applies full attention on short, overlapped video chunks,
which effectively mimics the inference-time attention pattern without training
on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a
new benchmark with videos averaging over two hours that requires dense,
per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM
achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time
performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy
also enhances general VQA abilities without any VQA-specific fine-tuning,
improving performance on LongVideoBench by +4.30 and OVOBench Realtime by
+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage.</div>
</details>
</div>
<div class="card">
<div class="title">VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action   Expert Distillation</div>
<div class="meta-line">Authors: Shaoqi Dong, Chaoyou Fu, Haihan Gao, Yi-Fan Zhang, Chi Yan, Chu Wu, Xiaoyu Liu, Yunhang Shen, Jing Huo, Deqiang Jiang, Haoyu Cao, Yang Gao, Xing Sun, Ran He, Caifeng Shan</div>
<div class="meta-line">First: 2025-10-10T17:59:56+00:00 · Latest: 2025-10-10T17:59:56+00:00</div>
<div class="meta-line">Comments: Homepage: https://ltbai.github.io/VITA-VLA/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09607v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ltbai.github.io/VITA-VLA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Action (VLA) models significantly advance robotic
manipulation by leveraging the strong perception capabilities of pretrained
vision-language models (VLMs). By integrating action modules into these
pretrained models, VLA methods exhibit improved generalization. However,
training them from scratch is costly. In this work, we propose a simple yet
effective distillation-based framework that equips VLMs with action-execution
capability by transferring knowledge from pretrained small action models. Our
architecture retains the original VLM structure, adding only an action token
and a state encoder to incorporate physical inputs. To distill action
knowledge, we adopt a two-stage training strategy. First, we perform
lightweight alignment by mapping VLM hidden states into the action space of the
small action model, enabling effective reuse of its pretrained action decoder
and avoiding expensive pretraining. Second, we selectively fine-tune the
language model, state encoder, and action modules, enabling the system to
integrate multimodal inputs with precise action generation. Specifically, the
action token provides the VLM with a direct handle for predicting future
actions, while the state encoder allows the model to incorporate robot dynamics
not captured by vision alone. This design yields substantial efficiency gains
over training large VLA models from scratch. Compared with previous
state-of-the-art methods, our method achieves 97.3% average success rate on
LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In
real-world experiments across five manipulation tasks, our method consistently
outperforms the teacher model, achieving 82.0% success rate (17% improvement),
which demonstrate that action distillation effectively enables VLMs to generate
precise actions while substantially reducing training costs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Action (VLA) models significantly advance robotic manipulation by leveraging the strong perception capabilities of pretrained vision-language models (VLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Detecting and Filtering Unsafe Training Data via Data Attribution with   Denoised Representation</div>
<div class="meta-line">Authors: Yijun Pan, Taiwei Shi, Jieyu Zhao, Jiaqi W. Ma</div>
<div class="meta-line">First: 2025-02-17T03:50:58+00:00 · Latest: 2025-10-10T17:54:29+00:00</div>
<div class="meta-line">Comments: 14 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.11411v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.11411v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are highly sensitive to even small amounts of
unsafe training data, making effective detection and filtering essential for
trustworthy model development. Current state-of-the-art (SOTA) detection
approaches primarily rely on moderation classifiers, which require significant
computation overhead for training and are limited to predefined taxonomies. In
this work, we explore data attribution approaches that measure the similarity
between individual training samples and a small set of unsafe target examples,
based on data representations such as hidden states or gradients. We identify a
key limitation in existing methods: unsafe target texts contain both critical
tokens that make them unsafe and neutral tokens (e.g., stop words or benign
facts) that are necessary to form fluent language, and the latter of which
makes the overall representations ``noisy&#x27;&#x27; for the purpose of detecting unsafe
training data. To address this challenge, we propose Denoised Representation
Attribution (DRA), a novel representation-based data attribution approach that
denoises training and target representations for unsafe data detection. Across
tasks of filtering jailbreaks and detecting gender bias, the proposed approach
leads to significant improvement for data attribution methods, outperforming
SOTA methods that are mostly based on moderation classifiers.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) are highly sensitive to even small amounts of unsafe training data, making effective detection and filtering essential for trustworthy model development.</div>
</details>
</div>
<div class="card">
<div class="title">STaTS: Structure-Aware Temporal Sequence Summarization via Statistical   Window Merging</div>
<div class="meta-line">Authors: Disharee Bhowmick, Ranjith Ramanathan, Sathyanarayanan N. Aakur</div>
<div class="meta-line">First: 2025-10-10T17:51:47+00:00 · Latest: 2025-10-10T17:51:47+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 4 tables. Under Review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09593v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09593v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series data often contain latent temporal structure, transitions between
locally stationary regimes, repeated motifs, and bursts of variability, that
are rarely leveraged in standard representation learning pipelines. Existing
models typically operate on raw or fixed-window sequences, treating all time
steps as equally informative, which leads to inefficiencies, poor robustness,
and limited scalability in long or noisy sequences. We propose STaTS, a
lightweight, unsupervised framework for Structure-Aware Temporal Summarization
that adaptively compresses both univariate and multivariate time series into
compact, information-preserving token sequences. STaTS detects change points
across multiple temporal resolutions using a BIC-based statistical divergence
criterion, then summarizes each segment using simple functions like the mean or
generative models such as GMMs. This process achieves up to 30x sequence
compression while retaining core temporal dynamics. STaTS operates as a
model-agnostic preprocessor and can be integrated with existing unsupervised
time series encoders without retraining. Extensive experiments on 150+
datasets, including classification tasks on the UCR-85, UCR-128, and UEA-30
archives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,
demonstrate that STaTS enables 85-90\% of the full-model performance while
offering dramatic reductions in computational cost. Moreover, STaTS improves
robustness under noise and preserves discriminative structure, outperforming
uniform and clustering-based compression baselines. These results position
STaTS as a principled, general-purpose solution for efficient, structure-aware
time series modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Time series data often contain latent temporal structure, transitions between locally stationary regimes, repeated motifs, and bursts of variability, that are rarely leveraged in standard representation learning pipelines.</div>
</details>
</div>
<div class="card">
<div class="title">Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical   Perspective</div>
<div class="meta-line">Authors: Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, Chandan K. Reddy</div>
<div class="meta-line">First: 2025-06-23T18:31:22+00:00 · Latest: 2025-10-10T17:14:10+00:00</div>
<div class="meta-line">Comments: 29 pages, 9 figures, 15 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.19028v5">Abs</a> · <a href="http://arxiv.org/pdf/2506.19028v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often generate responses with inherent biases,
undermining their reliability in real-world applications. Existing evaluation
methods often overlook biases in long-form responses and the intrinsic
variability of LLM outputs. To address these challenges, we propose FiSCo
(Fine-grained Semantic Comparison), a novel statistical framework to evaluate
group-level fairness in LLMs by detecting subtle semantic differences in
long-form responses across demographic groups. Unlike prior work focusing on
sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis
by operating at the claim level, leveraging entailment checks to assess the
consistency of meaning across responses. We decompose model outputs into
semantically distinct claims and apply statistical hypothesis testing to
compare inter- and intra-group similarities, enabling robust detection of
subtle biases. We formalize a new group counterfactual fairness definition and
validate FiSCo on both synthetic and human-annotated datasets spanning gender,
race, and age. Experiments show that FiSCo more reliably identifies nuanced
biases while reducing the impact of stochastic LLM variability, outperforming
various evaluation metrics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications.</div>
</details>
</div>
<div class="card">
<div class="title">SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models</div>
<div class="meta-line">Authors: Chengyu Wang, Paria Rashidinejad, DiJia Su, Song Jiang, Sid Wang, Siyan Zhao, Cai Zhou, Shannon Zejiang Shen, Feiyu Chen, Tommi Jaakkola, Yuandong Tian, Bo Liu</div>
<div class="meta-line">First: 2025-10-10T16:52:25+00:00 · Latest: 2025-10-10T16:52:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09541v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09541v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion large language models (dLLMs) are emerging as an efficient
alternative to autoregressive models due to their ability to decode multiple
tokens in parallel. However, aligning dLLMs with human preferences or
task-specific rewards via reinforcement learning (RL) is challenging because
their intractable log-likelihood precludes the direct application of standard
policy gradient methods. While prior work uses surrogates like the evidence
lower bound (ELBO), these one-sided approximations can introduce significant
policy gradient bias. To address this, we propose the Sandwiched Policy
Gradient (SPG) that leverages both an upper and a lower bound of the true
log-likelihood. Experiments show that SPG significantly outperforms baselines
based on ELBO or one-step estimation. Specifically, SPG improves the accuracy
over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,
18.4% in Countdown and 27.0% in Sudoku.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel.</div>
</details>
</div>
<div class="card">
<div class="title">COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens</div>
<div class="meta-line">Authors: Eugene Kwek, Wenpeng Yin</div>
<div class="meta-line">First: 2025-09-08T16:07:06+00:00 · Latest: 2025-10-10T16:50:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.06836v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.06836v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Making large language models (LLMs) more efficient in memory, latency, and
serving cost is crucial for edge deployment, interactive applications, and
sustainable inference at scale. Pruning is a promising technique, but existing
pruning methods are limited: width pruning often breaks the standard
transformer layout, requiring custom inference code, while depth pruning can
cause abrupt accuracy drops. Also, while many pruning approaches are effective
against LLMs, they struggle to maintain performance on small language models
(SLMs). In this work, we propose COMPACT, which jointly (i) prunes rare
vocabulary to shrink embedding/LM head layers and (ii) prunes FFN intermediate
channels using common-token-weighted activations, aligning importance with the
post-pruning token distribution. COMPACT inherits strengths of both depth and
width pruning, such as: deployment-friendliness (keeps a standard transformer
architecture), scale-adaptivity (trade off vocab. vs. FFN pruning), competitive
pruning times, and strong memory savings alongside throughput gains.
Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show
state-of-the-art downstream performance, with substantial reductions in
parameters, GPU memory, and latency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Making large language models (LLMs) more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale.</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Overthinking through Reasoning Shaping</div>
<div class="meta-line">Authors: Feifan Song, Shaohang Wei, Bofei Gao, Yejie Wang, Wen Luo, Wei Li, Linli Yao, Weimin Xiong, Liang Chen, Tianyu Liu, Houfeng Wang</div>
<div class="meta-line">First: 2025-10-10T16:49:03+00:00 · Latest: 2025-10-10T16:49:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09535v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier
Reward (RLVR) have shown great power in problem solving, yet they often cause
overthinking: excessive, meandering reasoning that inflates computational cost.
Prior designs of penalization in RLVR manage to reduce token consumption while
often harming model performance, which arises from the oversimplicity of
token-level supervision. In this paper, we argue that the granularity of
supervision plays a crucial role in balancing efficiency and accuracy, and
propose Group Relative Segment Penalization (GRSP), a step-level method to
regularize reasoning. Since preliminary analyses show that reasoning segments
are strongly correlated with token consumption and model performance, we design
a length-aware weighting mechanism across segment clusters. Extensive
experiments demonstrate that GRSP achieves superior token efficiency without
heavily compromising accuracy, especially the advantages with harder problems.
Moreover, GRSP stabilizes RL training and scales effectively across model
sizes.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier Reward (RLVR) have shown great power in problem solving, yet they often cause overthinking: excessive, meandering reasoning that inflates computational cost.</div>
</details>
</div>
<div class="card">
<div class="title">Towards Sequence Modeling Alignment between Tokenizer and Autoregressive   Model</div>
<div class="meta-line">Authors: Pingyu Wu, Kai Zhu, Yu Liu, Longxiang Tang, Jian Yang, Yansong Peng, Wei Zhai, Yang Cao, Zheng-Jun Zha</div>
<div class="meta-line">First: 2025-06-05T17:45:10+00:00 · Latest: 2025-10-10T16:20:43+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/ali-vilab/alitok</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.05289v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.05289v2">PDF</a> · <a href="https://github.com/ali-vilab/alitok">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive image generation aims to predict the next token based on
previous ones. However, this process is challenged by the bidirectional
dependencies inherent in conventional image tokenizations, which creates a
fundamental misalignment with the unidirectional nature of autoregressive
models. To resolve this, we introduce AliTok, a novel Aligned Tokenizer that
alters the dependency structure of the token sequence. AliTok employs a
bidirectional encoder constrained by a causal decoder, a design that compels
the encoder to produce a token sequence with both semantic richness and
forward-dependency. Furthermore, by incorporating prefix tokens and employing a
two-stage tokenizer training process to enhance reconstruction performance,
AliTok achieves high fidelity and predictability simultaneously. Building upon
AliTok, a standard decoder-only autoregressive model with just 177M parameters
achieves a gFID of 1.44 and an IS of 319.5 on the ImageNet-256 benchmark.
Scaling up to 662M parameters, our model reaches a gFID of 1.28, surpassing the
state-of-the-art diffusion method while achieving a 10x faster sampling speed.
The code and weights are available at https://github.com/ali-vilab/alitok.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive image generation aims to predict the next token based on previous ones.</div>
</details>
</div>
<div class="card">
<div class="title">Few-shot multi-token DreamBooth with LoRa for style-consistent character   generation</div>
<div class="meta-line">Authors: Ruben Pascual, Mikel Sesma-Sara, Aranzazu Jurio, Daniel Paternain, Mikel Galar</div>
<div class="meta-line">First: 2025-10-10T15:28:55+00:00 · Latest: 2025-10-10T15:28:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09475v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09475v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The audiovisual industry is undergoing a profound transformation as it is
integrating AI developments not only to automate routine tasks but also to
inspire new forms of art. This paper addresses the problem of producing a
virtually unlimited number of novel characters that preserve the artistic style
and shared visual traits of a small set of human-designed reference characters,
thus broadening creative possibilities in animation, gaming, and related
domains. Our solution builds upon DreamBooth, a well-established fine-tuning
technique for text-to-image diffusion models, and adapts it to tackle two core
challenges: capturing intricate character details beyond textual prompts and
the few-shot nature of the training data. To achieve this, we propose a
multi-token strategy, using clustering to assign separate tokens to individual
characters and their collective style, combined with LoRA-based
parameter-efficient fine-tuning. By removing the class-specific regularization
set and introducing random tokens and embeddings during generation, our
approach allows for unlimited character creation while preserving the learned
style. We evaluate our method on five small specialized datasets, comparing it
to relevant baselines using both quantitative metrics and a human evaluation
study. Our results demonstrate that our approach produces high-quality, diverse
characters while preserving the distinctive aesthetic features of the reference
characters, with human evaluation further reinforcing its effectiveness and
highlighting the potential of our method.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The audiovisual industry is undergoing a profound transformation as it is integrating AI developments not only to automate routine tasks but also to inspire new forms of art.</div>
</details>
</div>
<div class="card">
<div class="title">CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual   Optimization for On-Device Language Models</div>
<div class="meta-line">Authors: Dongqi Zheng, Wenjin Fu</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2025-09-29T22:07:20+00:00 · Latest: 2025-10-10T15:06:28+00:00</div>
<div class="meta-line">Comments: Accepted by 39th NeurIPS - Constrained Optimization for Machine
  Learning</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.03298v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.03298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce Constraint-Aware Federated Learning with Lagrangian Dual Optimization (CAFL-L), a principled extension of FedAvg that explicitly incorporates device-level resource constraints including energy, communication, memory, and thermal budgets.</div>
</details>
</div>
<div class="card">
<div class="title">ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning   Language Models</div>
<div class="meta-line">Authors: Dongqi Zheng</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2025-09-29T20:19:41+00:00 · Latest: 2025-10-10T15:04:19+00:00</div>
<div class="meta-line">Comments: Accepted by 39th NeurIPS - Foundations of Reasoning in Language
  Models</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.00071v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.00071v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable
capabilities in complex reasoning tasks, but suffer from significant
computational inefficiencies due to overthinking phenomena. Existing efficient
reasoning methods face the challenge of balancing reasoning quality with
inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression
(ARS)}, a novel training-free approach that dynamically suppresses redundant
reasoning steps while preserving accuracy through adaptive certainty
monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism
with progressive suppression thresholds, achieving superior efficiency compared
to static suppression methods. Our extensive evaluation across mathematical
reasoning benchmarks using multiple model architectures demonstrates that ARS
achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,
while maintaining or improving accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena.</div>
</details>
</div>
<div class="card">
<div class="title">On the Representations of Entities in Auto-regressive Large Language   Models</div>
<div class="meta-line">Authors: Victor Morand, Josiane Mothe, Benjamin Piwowarski</div>
<div class="meta-line">First: 2025-10-10T14:23:44+00:00 · Latest: 2025-10-10T14:23:44+00:00</div>
<div class="meta-line">Comments: Accepted at BlackBoxNLP@EMNLP2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09421v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09421v1">PDF</a> · <a href="https://github.com/VictorMorand/EntityRepresentations">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Named entities are fundamental building blocks of knowledge in text,
grounding factual information and structuring relationships within language.
Despite their importance, it remains unclear how Large Language Models (LLMs)
internally represent entities. Prior research has primarily examined explicit
relationships, but little is known about entity representations themselves. We
introduce entity mention reconstruction as a novel framework for studying how
LLMs encode and manipulate entities. We investigate whether entity mentions can
be generated from internal representations, how multi-token entities are
encoded beyond last-token embeddings, and whether these representations capture
relational knowledge. Our proposed method, leveraging _task vectors_, allows to
consistently generate multi-token mentions from various entity representations
derived from the LLMs hidden states. We thus introduce the _Entity Lens_,
extending the _logit-lens_ to predict multi-token mentions. Our results bring
new evidence that LLMs develop entity-specific mechanisms to represent and
manipulate any multi-token entities, including those unseen during training.
Our code is avalable at https://github.com/VictorMorand/EntityRepresentations .</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Named entities are fundamental building blocks of knowledge in text, grounding factual information and structuring relationships within language.</div>
</details>
</div>
<div class="card">
<div class="title">HoliTom: Holistic Token Merging for Fast Video Large Language Models</div>
<div class="meta-line">Authors: Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang</div>
<div class="meta-line">First: 2025-05-27T15:28:45+00:00 · Latest: 2025-10-10T13:28:40+00:00</div>
<div class="meta-line">Comments: code link: https://github.com/cokeshao/HoliTom</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.21334v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.21334v3">PDF</a> · <a href="https://github.com/cokeshao/HoliTom">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video large language models (video LLMs) excel at video comprehension but
face significant computational inefficiency due to redundant video tokens.
Existing token pruning methods offer solutions. However, approaches operating
within the LLM (inner-LLM pruning), such as FastV, incur intrinsic
computational overhead in shallow layers. In contrast, methods performing token
pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy
within individual frames or limited temporal windows, neglecting the crucial
global temporal dynamics and correlations across longer video sequences. This
leads to sub-optimal spatio-temporal reduction and does not leverage video
compressibility fully. Crucially, the synergistic potential and mutual
influence of combining these strategies remain unexplored. To further reduce
redundancy, we introduce HoliTom, a novel training-free holistic token merging
framework. HoliTom employs outer-LLM pruning through global redundancy-aware
temporal segmentation, followed by spatial-temporal merging to reduce visual
tokens by over 90%, significantly alleviating the LLM&#x27;s computational burden.
Complementing this, we introduce a robust inner-LLM token similarity-based
merging approach, designed for superior performance and compatibility with
outer-LLM pruning. Evaluations demonstrate our method&#x27;s promising
efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational
costs to 6.9% of FLOPs while maintaining 99.1% of the original performance.
Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a
1.32x acceleration in decoding throughput, highlighting the practical benefits
of our integrated pruning approach for efficient video LLMs inference.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens.</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting associative recall in modern recurrent models</div>
<div class="meta-line">Authors: Destiny Okpekpe, Antonio Orvieto</div>
<div class="meta-line">First: 2025-08-26T13:45:08+00:00 · Latest: 2025-10-10T13:13:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.19029v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.19029v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the advantageous subquadratic complexity of modern recurrent deep
learning models -- such as state-space models (SSMs) -- recent studies have
highlighted their potential shortcomings compared to transformers on reasoning
and memorization tasks. In this paper, we dive deeper into one of such
benchmarks: associative recall (AR), which has been shown to correlate well
with language modeling performance, and inspect in detail the effects of
scaling and optimization issues in recently proposed token mixing strategies.
We first demonstrate that, unlike standard transformers, the choice of learning
rate plays a critical role in the performance of modern recurrent models: an
issue that can severely affect reported performance in previous works and
suggests further research is needed to stabilize training. Next, we show that
recurrent and attention-based models exhibit contrasting benefits when scaling
in width as opposed to depth, with attention being notably unable to solve AR
when limited to a single layer. We then further inspect 1-layer transformers,
revealing that despite their poor performance, their training dynamics
surprisingly resemble the formation of induction heads, a phenomenon previously
observed only in their 2-layer counterparts. Finally, through architectural
ablations, we study how components affects Transformer and Mamba&#x27;s performance
and optimization stability.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite the advantageous subquadratic complexity of modern recurrent deep learning models -- such as state-space models (SSMs) -- recent studies have highlighted their potential shortcomings compared to transformers on reasoning and memorization tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments   with a Hierarchical Spatial-Cognition Long-Short Memory System</div>
<div class="meta-line">Authors: Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li</div>
<div class="meta-line">First: 2025-06-24T09:00:43+00:00 · Latest: 2025-10-10T13:08:39+00:00</div>
<div class="meta-line">Comments: The paper is currently under investigation regarding concerns of
  potential academic misconduct. While the investigation is ongoing, the
  authors have voluntarily requested to withdraw the manuscript</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.19433v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.19433v2">PDF</a> · <a href="https://github.com/tsinghua-fib-lab/Mem4Nav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-and-Language Navigation (VLN) in large-scale urban environments
requires embodied agents to ground linguistic instructions in complex scenes
and recall relevant experiences over extended time horizons. Prior modular
pipelines offer interpretability but lack unified memory, while end-to-end
(M)LLM agents excel at fusing vision and language yet remain constrained by
fixed context windows and implicit spatial reasoning. We introduce
\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system
that can augment any VLN backbone. Mem4Nav fuses a sparse octree for
fine-grained voxel indexing with a semantic topology graph for high-level
landmark connectivity, storing both in trainable memory tokens embedded via a
reversible Transformer. Long-term memory (LTM) compresses and retains
historical observations at both octree and graph nodes, while short-term memory
(STM) caches recent multimodal entries in relative coordinates for real-time
obstacle avoidance and local planning. At each step, STM retrieval sharply
prunes dynamic context, and, when deeper history is needed, LTM tokens are
decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and
Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based
LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13
pp gains in Task Completion, sufficient SPD reduction, and &gt;10 pp nDTW
improvement. Ablations confirm the indispensability of both the hierarchical
map and dual memory modules. Our codes are open-sourced via
https://github.com/tsinghua-fib-lab/Mem4Nav.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons.</div>
</details>
</div>
<div class="card">
<div class="title">Spotlight on Token Perception for Multimodal Reinforcement Learning</div>
<div class="meta-line">Authors: Siyuan Huang, Xiaoye Qu, Yafu Li, Yun Luo, Zefeng He, Daizong Liu, Yu Cheng</div>
<div class="meta-line">First: 2025-10-10T11:25:33+00:00 · Latest: 2025-10-10T11:25:33+00:00</div>
<div class="meta-line">Comments: 31 pages, 10 figures, project page:
  https://github.com/huaixuheqing/VPPO-RL</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09285v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09285v1">PDF</a> · <a href="https://github.com/huaixuheqing/VPPO-RL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the
reasoning capabilities of Large Vision-Language Models (LVLMs), most existing
methods in multimodal reasoning neglect the critical role of visual perception
within the RLVR optimization process. In this paper, we undertake a pioneering
exploration of multimodal RLVR through the novel perspective of token
perception, which measures the visual dependency of each generated token. With
a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key
insights: first, token perception in a rollout trajectory is sparsely
distributed, where only a small fraction of tokens have high visual dependency
for visually-grounded reasoning; second, different trajectories exhibit
significant divergence in their overall visual dependency. Based on these
observations, we propose Visually-Perceptive Policy Optimization (VPPO), a
novel policy gradient algorithm that explicitly leverages token perception to
refine the learning signal. Specifically, VPPO achieves this through a dual
mechanism: it reweights a trajectory&#x27;s advantage by its overall visual
dependency, and focuses policy updates exclusively on perceptually pivotal
tokens. On a comprehensive suite of eight perception and reasoning benchmarks,
VPPO demonstrates substantial gains over leading open-source RL-tuned models,
with its effectiveness consistently validated across 7B and 32B model scales.
Our findings not only establish a new token-level perceptual perspective for
analyzing multimodal RLVR but also present a novel and effective optimization
strategy to significantly enhance the multimodal reasoning capabilities of
LVLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capabilities of Large Vision-Language Models (LVLMs), most existing methods in multimodal reasoning neglect the critical role of visual perception within the RLVR optimization process.</div>
</details>
</div>
<div class="card">
<div class="title">MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel   Understanding</div>
<div class="meta-line">Authors: Ming Dai, Sen Yang, Boqiang Duan, Wankou Yang, Jingdong Wang</div>
<div class="meta-line">First: 2025-10-10T11:18:21+00:00 · Latest: 2025-10-10T11:18:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09274v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09274v1">PDF</a> · <a href="https://github.com/Dmmm1997/MomentSeg">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Referring Video Object Segmentation (RefVOS) seeks to segment target objects
in videos guided by natural language descriptions, demanding both temporal
reasoning and fine-grained visual comprehension. Existing sampling strategies
for LLM-based approaches typically rely on either handcrafted heuristics or
external keyframe models. The former often overlooks essential temporal cues,
while the latter increases system complexity. To address this, we propose a
unified framework that jointly optimizes Temporal Sentence Grounding (TSG) and
RefVOS, naturally incorporating key moment grounding capability. During
training, we introduce a novel TSG paradigm that employs a dedicated
\texttt{[FIND]} token for key moment identification through temporal token
similarity matching, thereby avoiding the need for external timestamp
encodings. For inference, we design a Moment-Centric Sampling (MCS) strategy
that densely samples informative moments while sparsely sampling non-essential
frames, preserving both motion details and global context. To further enhance
tracking stability, we develop Bidirectional Anchor-updated Propagation (BAP),
which leverages the most relevant moment as start point for high-quality mask
initialization and dynamically updates at sampled points to mitigate
accumulated errors. Code and model will be available at:
https://github.com/Dmmm1997/MomentSeg</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Referring Video Object Segmentation (RefVOS) seeks to segment target objects in videos guided by natural language descriptions, demanding both temporal reasoning and fine-grained visual comprehension.</div>
</details>
</div>
<div class="card">
<div class="title">GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger   Synthesis</div>
<div class="meta-line">Authors: Subrat Kishore Dutta, Yuelin Xu, Piyush Pant, Xiao Zhang</div>
<div class="meta-line">First: 2025-10-10T10:59:14+00:00 · Latest: 2025-10-10T10:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09260v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09260v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has shown that RLHF is highly susceptible to backdoor attacks,
poisoning schemes that inject malicious triggers in preference data. However,
existing methods often rely on static, rare-token-based triggers, limiting
their effectiveness in realistic scenarios. In this paper, we develop GREAT, a
novel framework for crafting generalizable backdoors in RLHF through
emotion-aware trigger synthesis. Specifically, GREAT targets harmful response
generation for a vulnerable user subgroup characterized by both semantically
violent requests and emotionally angry triggers. At the core of GREAT is a
trigger identification pipeline that operates in the latent embedding space,
leveraging principal component analysis and clustering techniques to identify
the most representative triggers. To enable this, we present Erinyes, a
high-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a
principled, hierarchical, and diversity-promoting approach. Experiments on
benchmark RLHF datasets demonstrate that GREAT significantly outperforms
baseline methods in attack success rates, especially for unseen trigger
scenarios, while largely preserving the response quality on benign inputs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent work has shown that RLHF is highly susceptible to backdoor attacks, poisoning schemes that inject malicious triggers in preference data.</div>
</details>
</div>
<div class="card">
<div class="title">3D Reconstruction from Transient Measurements with Time-Resolved   Transformer</div>
<div class="meta-line">Authors: Yue Li, Shida Sun, Yu Hong, Feihu Xu, Zhiwei Xiong</div>
<div class="meta-line">First: 2025-10-10T09:44:08+00:00 · Latest: 2025-10-10T09:44:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09205v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09205v1">PDF</a> · <a href="https://github.com/Depth2World/TRT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transient measurements, captured by the timeresolved systems, are widely
employed in photon-efficient reconstruction tasks, including line-of-sight
(LOS) and non-line-of-sight (NLOS) imaging. However, challenges persist in
their 3D reconstruction due to the low quantum efficiency of sensors and the
high noise levels, particularly for long-range or complex scenes. To boost the
3D reconstruction performance in photon-efficient imaging, we propose a generic
Time-Resolved Transformer (TRT) architecture. Different from existing
transformers designed for high-dimensional data, TRT has two elaborate
attention designs tailored for the spatio-temporal transient measurements.
Specifically, the spatio-temporal self-attention encoders explore both local
and global correlations within transient data by splitting or downsampling
input features into different scales. Then, the spatio-temporal cross attention
decoders integrate the local and global features in the token space, resulting
in deep features with high representation capabilities. Building on TRT, we
develop two task-specific embodiments: TRT-LOS for LOS imaging and TRT-NLOS for
NLOS imaging. Extensive experiments demonstrate that both embodiments
significantly outperform existing methods on synthetic data and real-world data
captured by different imaging systems. In addition, we contribute a
large-scale, high-resolution synthetic LOS dataset with various noise levels
and capture a set of real-world NLOS measurements using a custom-built imaging
system, enhancing the data diversity in this field. Code and datasets are
available at https://github.com/Depth2World/TRT.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transient measurements, captured by the timeresolved systems, are widely employed in photon-efficient reconstruction tasks, including line-of-sight (LOS) and non-line-of-sight (NLOS) imaging.</div>
</details>
</div>
<div class="card">
<div class="title">RPG: A Repository Planning Graph for Unified and Scalable Codebase   Generation</div>
<div class="meta-line">Authors: Jane Luo, Xin Zhang, Steven Liu, Jie Wu, Yiming Huang, Yangyu Huang, Chengyu Yin, Ying Xin, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qi Chen, Scarlett Li, Mao Yang</div>
<div class="meta-line">First: 2025-09-19T17:58:14+00:00 · Latest: 2025-10-10T09:35:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.16198v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.16198v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models excel at generating individual functions or single
files of code, yet generating complete repositories from scratch remains a
fundamental challenge. This capability is key to building coherent software
systems from high-level specifications and realizing the full potential of
automated code generation. The process requires planning at two levels:
deciding what features and modules to build (proposal stage) and defining their
implementation details (implementation stage). Current approaches rely on
natural language planning, which often produces unclear specifications,
misaligned components, and brittle designs due to its inherent ambiguity and
lack of structure. To address these limitations, we introduce the Repository
Planning Graph (RPG), a structured representation that encodes capabilities,
file structures, data flows, and functions in a unified graph. By replacing
free-form natural language with an explicit blueprint, RPG enables consistent
long-horizon planning for repository generation. Building on RPG, we develop
ZeroRepo, a graph-driven framework that operates in three stages:
proposal-level planning, implementation-level construction, and graph-guided
code generation with test validation. To evaluate, we construct RepoCraft, a
benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo
produces nearly 36K Code Lines and 445K Code Tokens, on average 3.9$\times$
larger than the strongest baseline (Claude Code), and 68$\times$ larger than
other baselines. It achieves 81.5% coverage and 69.7% test accuracy, improving
over Claude Code by 27.3 and 35.8 points. Further analysis shows that RPG
models complex dependencies, enables more sophisticated planning through
near-linear scaling, and improves agent understanding of repositories, thus
accelerating localization.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models excel at generating individual functions or single files of code, yet generating complete repositories from scratch remains a fundamental challenge.</div>
</details>
</div>
<div class="card">
<div class="title">Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal   Forgetting</div>
<div class="meta-line">Authors: Suming Qiu, Jing Li, Zhicheng Zhou, Junjie Huang, Linyuan Qiu, Zhijie Sun</div>
<div class="meta-line">First: 2025-10-10T08:55:32+00:00 · Latest: 2025-10-10T08:55:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09152v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09152v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) often face a trade-off in post-training:
improvements on specialized domains frequently come at the expense of general
capabilities. Existing solutions attempt to mitigate this tension via
regularization, selective parameter updates, or data-centric replay, but each
imposes significant costs in computation, data access, or adaptability. Recent
work has shown that training signals can be compressed to subsets of logits
without severe accuracy loss, suggesting a path toward efficient adaptation.
However, naive truncation destabilizes optimization and exacerbates forgetting.
  We introduce Logits Replay + MoClip, a two-stage framework that compresses
supervision in the logit space and stabilizes optimization at the update level.
In Stage 0, we record dynamic Top-K token subsets that cover a probability
threshold, always including the gold label. In Stage 1, we replay these compact
subsets to compute exact renormalized losses, avoiding full softmax computation
and implicitly regularizing. To ensure stability, we design MoClip, an
optimizer that caps gradient-momentum rotation and applies an arctan2-based
rescaling of updates. Empirically, our method improves domain performance on
Communication Technology (CT) and NL2SQL tasks while mitigating forgetting on
general benchmarks (MMLU, BBH, GPQA, MATH), and reduces training cost by over
40%. Together, these contributions offer a scalable, architecture-agnostic path
for domain adaptation of LLMs without sacrificing generalization.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) often face a trade-off in post-training: improvements on specialized domains frequently come at the expense of general capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">Neural Codecs as Biosignal Tokenizers</div>
<div class="meta-line">Authors: Kleanthis Avramidis, Tiantian Feng, Woojae Jeong, Jihwan Lee, Wenhui Cui, Richard M Leahy, Shrikanth Narayanan</div>
<div class="meta-line">First: 2025-10-10T07:47:20+00:00 · Latest: 2025-10-10T07:47:20+00:00</div>
<div class="meta-line">Comments: 25 pages, 7 figures, 10 tables, currently under peer review</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09095v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09095v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neurophysiological recordings such as electroencephalography (EEG) offer
accessible and minimally invasive means of estimating physiological activity
for applications in healthcare, diagnostic screening, and even immersive
entertainment. However, these recordings yield high-dimensional, noisy
time-series data that typically require extensive pre-processing and
handcrafted feature extraction to reveal meaningful information. Recently,
there has been a surge of interest in applying representation learning
techniques from large pre-trained (foundation) models to effectively decode and
interpret biosignals. We discuss the challenges posed for incorporating such
methods and introduce BioCodec, an alternative representation learning
framework inspired by neural codecs to capture low-level signal characteristics
in the form of discrete tokens. Pre-trained on thousands of EEG hours, BioCodec
shows efficacy across multiple downstream tasks, ranging from clinical
diagnostic tasks and sleep physiology to decoding speech and motor imagery,
particularly in low-resource settings. Additionally, we provide a qualitative
analysis of codebook usage and estimate the spatial coherence of codebook
embeddings from EEG connectivity. Notably, we also document the suitability of
our method to other biosignal data, i.e., electromyographic (EMG) signals.
Overall, the proposed approach provides a versatile solution for biosignal
tokenization that performs competitively with state-of-the-art models. The
source code and model checkpoints are shared.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Neurophysiological recordings such as electroencephalography (EEG) offer accessible and minimally invasive means of estimating physiological activity for applications in healthcare, diagnostic screening, and even immersive entertainment.</div>
</details>
</div>
<div class="card">
<div class="title">FLToP CTC: Frame-Level Token Pruning via Relative Threshold for   Efficient and Memory-Saving Decoding on Diverse Platforms</div>
<div class="meta-line">Authors: Atul Shree, Harshith Jupuru</div>
<div class="meta-line">First: 2025-10-10T07:32:54+00:00 · Latest: 2025-10-10T07:32:54+00:00</div>
<div class="meta-line">Comments: 5 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.09085v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.09085v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CTC-based ASR systems face computational and memory bottlenecks in
resource-limited environments. Traditional CTC decoders, requiring up to 90% of
processing time in systems (e.g., wav2vec2-large on L4 GPUs), face
inefficiencies due to exhaustive token-level operations. This paper introduces
Frame Level Token Pruning for Connectionist Temporal Classification (FLToP
CTC), a novel decoding algorithm that employs frame-level token pruning guided
by a relative threshold probability. By dynamically eliminating low-probability
tokens per frame, FLToP CTC reduces compute and memory demands while
maintaining negligible WER degradation. On LibriSpeech, FLToP CTC achieves a
10.5x runtime speedup and 2.78x memory reduction versus standard CTC decoders.
Its simplicity enables seamless integration into CTC decoders across platforms
(CPUs, GPUs, etc.). FLToP CTC addresses CTC bottlenecks, offering scalability
for resource-limited environments and realtime applications, enhancing speech
recognition accessibility and efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CTC-based ASR systems face computational and memory bottlenecks in resource-limited environments.</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Generative Recommendation with Continuous Tokens</div>
<div class="meta-line">Authors: Haohao Qu, Shanru Lin, Yujuan Ding, Yiqi Wang, Wenqi Fan</div>
<div class="meta-line">Venue: WWW 2026</div>
<div class="meta-line">First: 2025-04-16T12:01:03+00:00 · Latest: 2025-10-10T07:27:13+00:00</div>
<div class="meta-line">Comments: Submitted to WWW 2026. Our code and data will be made publicly
  available after acceptance</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.12007v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.12007v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in generative artificial intelligence, particularly large
language models (LLMs), have opened new opportunities for enhancing recommender
systems (RecSys). Most existing LLM-based RecSys approaches operate in a
discrete space, using vector-quantized tokenizers to align with the inherent
discrete nature of language models. However, these quantization methods often
result in lossy tokenization and suboptimal learning, primarily due to
inaccurate gradient propagation caused by the non-differentiable argmin
operation in standard vector quantization. Inspired by the emerging trend of
embracing continuous tokens in language models, we propose ContRec, a novel
framework that seamlessly integrates continuous tokens into LLM-based RecSys.
Specifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which
encodes users/items with continuous tokens; and a Dispersive Diffusion module,
which captures implicit user preference. The tokenizer is trained with a
continuous Variational Auto-Encoder (VAE) objective, where three effective
techniques are adopted to avoid representation collapse. By conditioning on the
previously generated tokens of the LLM backbone during user modeling, the
Dispersive Diffusion module performs a conditional diffusion process with a
novel Dispersive Loss, enabling high-quality user preference generation through
next-token diffusion. Finally, ContRec leverages both the textual reasoning
output from the LLM and the latent representations produced by the diffusion
model for Top-K item retrieval, thereby delivering comprehensive recommendation
results. Extensive experiments on four datasets demonstrate that \ourname{}
consistently outperforms both traditional and SOTA LLM-based recommender
systems. Our results highlight the potential of continuous tokenization and
generative modeling for advancing the next generation of recommender systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in generative artificial intelligence, particularly large language models (LLMs), have opened new opportunities for enhancing recommender systems (RecSys).</div>
</details>
</div>
<div class="card">
<div class="title">PEAR: Phase Entropy Aware Reward for Efficient Reasoning</div>
<div class="meta-line">Authors: Chen Huang, Wei Lu, Wenxuan Zhang</div>
<div class="meta-line">First: 2025-10-09T10:04:31+00:00 · Latest: 2025-10-10T07:08:11+00:00</div>
<div class="meta-line">Comments: 15 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.08026v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.08026v2">PDF</a> · <a href="https://github.com/iNLP-Lab/PEAR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Reasoning Models (LRMs) have achieved impressive performance on complex
reasoning tasks by generating detailed chain-of-thought (CoT) explanations.
However, these responses are often excessively long, containing redundant
reasoning steps that inflate inference cost and reduce usability. Controlling
the length of generated reasoning without sacrificing accuracy remains an open
challenge. Through a systematic empirical analysis, we reveal a consistent
positive correlation between model entropy and response length at different
reasoning stages across diverse LRMs: the thinking phase exhibits higher
entropy, reflecting exploratory behavior of longer responses, while the final
answer phase shows lower entropy, indicating a more deterministic solution.
This observation suggests that entropy at different reasoning stages can serve
as a control knob for balancing conciseness and performance. Based on this
insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward
mechanism that incorporating phase-dependent entropy into the reward design.
Instead of treating all tokens uniformly, PEAR penalize excessive entropy
during the thinking phase and allowing moderate exploration at the final answer
phase, which encourages models to generate concise reasoning traces that retain
sufficient flexibility to solve the task correctly. This enables adaptive
control of response length without relying on explicit length targets or rigid
truncation rules. Extensive experiments across four benchmarks demonstrate that
PEAR consistently reduces response length while sustaining competitive accuracy
across model scales. In addition, PEAR demonstrates strong out-of-distribution
(OOD) robustness beyond the training distribution. Our code is available at:
https://github.com/iNLP-Lab/PEAR.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations.</div>
</details>
</div>
<div class="card">
<div class="title">TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive   Generation</div>
<div class="meta-line">Authors: Zhekai Chen, Ruihang Chu, Yukang Chen, Shiwei Zhang, Yujie Wei, Yingya Zhang, Xihui Liu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-07-24T16:04:55+00:00 · Latest: 2025-10-10T06:44:48+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.18537v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.18537v2">PDF</a> · <a href="https://github.com/ali-vilab/TTS-VAR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling visual generation models is essential for real-world content
creation, yet requires substantial training and computational expenses.
Alternatively, test-time scaling has garnered growing attention due to resource
efficiency and promising performance. In this work, we present TTS-VAR, the
first general test-time scaling framework for visual auto-regressive (VAR)
models, modeling the generation process as a path searching problem. To
dynamically balance computational efficiency with exploration capacity, we
first introduce an adaptive descending batch size schedule throughout the
causal generation process. Besides, inspired by VAR&#x27;s hierarchical
coarse-to-fine multi-scale generation, our framework integrates two key
components: (i) At coarse scales, we observe that generated tokens are hard for
evaluation, possibly leading to erroneous acceptance of inferior samples or
rejection of superior samples. Noticing that the coarse scales contain
sufficient structural information, we propose clustering-based diversity
search. It preserves structural variety through semantic feature clustering,
enabling later selection on samples with higher potential. (ii) In fine scales,
resampling-based potential selection prioritizes promising candidates using
potential scores, which are defined as reward functions incorporating
multi-scale generation history. Experiments on the powerful VAR model Infinity
show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights
reveal that early-stage structural features effectively influence final
quality, and resampling efficacy varies across generation scales. Code is
available at https://github.com/ali-vilab/TTS-VAR.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses.</div>
</details>
</div>
<div class="card">
<div class="title">Improving Image Captioning Descriptiveness by Ranking and LLM-based   Fusion</div>
<div class="meta-line">Authors: Luigi Celona, Simone Bianco, Marco Donzella, Paolo Napoletano</div>
<div class="meta-line">First: 2023-06-20T15:13:02+00:00 · Latest: 2025-10-10T06:42:26+00:00</div>
<div class="meta-line">Comments: This manuscript has been accepted for publication in Springer Neural
  Computing and Applications</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2306.11593v3">Abs</a> · <a href="http://arxiv.org/pdf/2306.11593v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-The-Art (SoTA) image captioning models are often trained on the
MicroSoft Common Objects in Context (MS-COCO) dataset, which contains
human-annotated captions with an average length of approximately ten tokens.
Although effective for general scene understanding, these short captions often
fail to capture complex scenes and convey detailed information. Moreover,
captioning models tend to exhibit bias towards the ``average&#x27;&#x27; caption, which
captures only the more general aspects, thus overlooking finer details. In this
paper, we present a novel approach to generate richer and more informative
image captions by combining the captions generated from different SoTA
captioning models. Our proposed method requires no additional model training:
given an image, it leverages pre-trained models from the literature to generate
the initial captions, and then ranks them using a newly introduced
image-text-based metric, which we name BLIPScore. Subsequently, the top two
captions are fused using a Large Language Model (LLM) to produce the final,
more detailed description. Experimental results on the MS-COCO and Flickr30k
test sets demonstrate the effectiveness of our approach in terms of
caption-image alignment and hallucination reduction according to the ALOHa,
CAPTURE, and Polos metrics. A subjective study lends additional support to
these results, suggesting that the captions produced by our model are generally
perceived as more consistent with human judgment. By combining the strengths of
diverse SoTA models, our method enhances the quality and appeal of image
captions, bridging the gap between automated systems and the rich and
informative nature of human-generated descriptions. This advance enables the
generation of more suitable captions for the training of both vision-language
and captioning models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">State-of-The-Art (SoTA) image captioning models are often trained on the MicroSoft Common Objects in Context (MS-COCO) dataset, which contains human-annotated captions with an average length of approximately ten tokens.</div>
</details>
</div>
<div class="card">
<div class="title">Post-training quantization of vision encoders needs prefixing registers</div>
<div class="meta-line">Authors: Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee</div>
<div class="meta-line">First: 2025-10-06T07:27:46+00:00 · Latest: 2025-10-10T06:33:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.04547v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.04547v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control.</div>
</details>
</div>
<div class="card">
<div class="title">On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised   Fine-Tuning and Reinforcement Learning via Dynamic Weighting</div>
<div class="meta-line">Authors: Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, Jingren Zhou</div>
<div class="meta-line">First: 2025-08-15T11:20:03+00:00 · Latest: 2025-10-10T06:33:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.11408v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.11408v2">PDF</a> · <a href="https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established response
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data&#x27;s influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from the expert,
which promotes on-policy exploration and mitigates disruption from off-policy
data. We conduct extensive experiments on mathematical reasoning problems and
practical tool-use tasks, providing empirical evidence that CHORD achieves a
stable and efficient learning process. By effectively harmonizing off-policy
expert data with on-policy exploration, CHORD demonstrates significant
improvements over baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms for refining the capabilities and aligning the behavior of Large Language Models (LLMs).</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
