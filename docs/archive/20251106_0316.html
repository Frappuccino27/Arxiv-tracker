<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-06 03:16</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251106_0316</div>
    <div class="row"><div class="card">
<div class="title">Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities</div>
<div class="meta-line">Authors: Amanda Bertsch, Adithya Pratapa, Teruko Mitamura, Graham Neubig, Matthew R. Gormley</div>
<div class="meta-line">First: 2025-11-04T18:42:12+00:00 · Latest: 2025-11-04T18:42:12+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02817v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02817v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted.</div>
</details>
</div>
<div class="card">
<div class="title">LAWCAT: Efficient Distillation from Quadratic to Linear Attention with   Convolution across Tokens for Long Context Modeling</div>
<div class="meta-line">Authors: Zeyu Liu, Souvik Kundu, Lianghao Jiang, Anni Li, Srikanth Ronanki, Sravan Bodapati, Gourav Datta, Peter A. Beerel</div>
<div class="meta-line">First: 2025-09-22T22:43:44+00:00 · Latest: 2025-11-04T18:01:01+00:00</div>
<div class="meta-line">Comments: 17 pages, 8 figures. EMNLP2025 Findings</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.18467v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.18467v2">PDF</a> · <a href="https://github.com/zeyuliu1037/LAWCAT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&amp;2\&amp;3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&amp;QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources. Code is released at:
https://github.com/zeyuliu1037/LAWCAT</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Although transformer architectures have achieved state-of-the-art performance across diverse domains, their quadratic computational complexity with respect to sequence length remains a significant bottleneck, particularly for latency-sensitive long-context applications.</div>
</details>
</div>
<div class="card">
<div class="title">A Practical Investigation of Spatially-Controlled Image Generation with   Transformers</div>
<div class="meta-line">Authors: Guoxuan Xia, Harleen Hanspal, Petru-Daniel Tudosiu, Shifeng Zhang, Sarah Parisot</div>
<div class="meta-line">First: 2025-07-21T15:33:49+00:00 · Latest: 2025-11-04T17:54:35+00:00</div>
<div class="meta-line">Comments: TMLR https://openreview.net/forum?id=loT6xhgLYK</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.15724v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.15724v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
&quot;forgetting&quot; and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Enabling image generation models to be spatially controlled is an important area of research, empowering users to better generate images according to their own fine-grained specifications via e.g.</div>
</details>
</div>
<div class="card">
<div class="title">Agentic World Modeling for 6G: Near-Real-Time Generative State-Space   Reasoning</div>
<div class="meta-line">Authors: Farhad Rezazadeh, Hatim Chergui, Merouane Debbah, Houbing Song, Dusit Niyato, Lingjia Liu</div>
<div class="meta-line">First: 2025-11-04T17:22:22+00:00 · Latest: 2025-11-04T17:22:22+00:00</div>
<div class="meta-line">Comments: 13 Pages, 3 Figures, 4 Tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02748v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02748v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We argue that sixth-generation (6G) intelligence is not fluent token
prediction but the capacity to imagine and choose -- to simulate future
scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe
open radio access network (O-RAN) near-real-time (Near-RT) control via
counterfactual dynamics and a world modeling (WM) paradigm that learns an
action-conditioned generative state space. This enables quantitative &quot;what-if&quot;
forecasting beyond large language models (LLMs) as the primary modeling
primitive. Actions such as physical resource blocks (PRBs) are treated as
first-class control inputs in a causal world model, and both aleatoric and
epistemic uncertainty are modeled for prediction and what-if analysis. An
agentic, model predictive control (MPC)-based cross-entropy method (CEM)
planner operates over short horizons, using prior-mean rollouts within
data-driven PRB bounds to maximize a deterministic reward. The model couples
multi-scale structured state-space mixtures (MS3M) with a compact stochastic
latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories
and predicting next-step KPIs under hypothetical PRB sequences. On realistic
O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with
32% fewer parameters and similar latency, and achieves 35-80% lower root mean
squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster
inference, enabling rare-event simulation and offline policy screening.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We argue that sixth-generation (6G) intelligence is not fluent token prediction but the capacity to imagine and choose -- to simulate future scenarios, weigh trade-offs, and act with calibrated uncertainty.</div>
</details>
</div>
<div class="card">
<div class="title">Tokens, the oft-overlooked appetizer: Large language models, the   distributional hypothesis, and meaning</div>
<div class="meta-line">Authors: Julia Witte Zimmerman, Denis Hudon, Kathryn Cramer, Alejandro J. Ruiz, Calla Beauregard, Ashley Fehr, Mikaela Irene Fudolig, Bradford Demarest, Yoshi Meke Bird, Milo Z. Trujillo, Christopher M. Danforth, Peter Sheridan Dodds</div>
<div class="meta-line">First: 2024-12-14T18:18:52+00:00 · Latest: 2025-11-04T16:46:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.10924v8">Abs</a> · <a href="http://arxiv.org/pdf/2412.10924v8">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenization is a necessary component within the current architecture of many
language mod-els, including the transformer-based large language models (LLMs)
of Generative AI, yet its impact on the model&#x27;s cognition is often overlooked.
We argue that LLMs demonstrate that the Distributional Hypothesis (DH) is
sufficient for reasonably human-like language performance (particularly with
respect to inferential lexical competence), and that the emergence of
human-meaningful linguistic units among tokens and current structural
constraints motivate changes to existing, linguistically-agnostic tokenization
techniques, particularly with respect to their roles as (1) vehicles for
conveying salient distributional patterns from human language to the model and
as (2) semantic primitives. We explore tokenizations from a BPE tokenizer;
extant model vocabularies obtained from Hugging Face and tiktoken; and the
information in exemplar token vectors as they move through the layers of a
RoBERTa (large) model. Besides creating suboptimal semantic building blocks and
obscuring the model&#x27;s access to the necessary distributional patterns, we
describe how tokens and pretraining can act as a backdoor for bias and other
unwanted content, which current alignment practices may not remediate.
Additionally, we relay evidence that the tokenization algorithm&#x27;s objective
function impacts the LLM&#x27;s cognition, despite being arguably meaningfully
insulated from the main system intelligence. Finally, we discuss implications
for architectural choices, meaning construction, the primacy of language for
thought, and LLM cognition. [First uploaded to arXiv in December, 2024.]</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Tokenization is a necessary component within the current architecture of many language mod-els, including the transformer-based large language models (LLMs) of Generative AI, yet its impact on the model&#x27;s cognition is often overlooked.</div>
</details>
</div>
<div class="card">
<div class="title">Linear-Time Demonstration Selection for In-Context Learning via Gradient   Estimation</div>
<div class="meta-line">Authors: Ziniu Zhang, Zhenshuo Zhang, Dongyue Li, Lu Wang, Jennifer Dy, Hongyang R. Zhang</div>
<div class="meta-line">Venue: EMNLP</div>
<div class="meta-line">First: 2025-08-27T15:59:47+00:00 · Latest: 2025-11-04T16:44:29+00:00</div>
<div class="meta-line">Comments: 19 pages. EMNLP&#x27;25</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.19999v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.19999v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
${1}\%$ error across six datasets. This allows us to scale up subset selection
that would otherwise run full inference by up to ${37.7}\times$ on models with
up to $34$ billion parameters, and outperform existing selection methods based
on input embeddings by ${11}\%$ on average.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces an algorithm to select demonstration examples for in-context learning of a query set.</div>
</details>
</div>
<div class="card">
<div class="title">Repetitions are not all alike: distinct mechanisms sustain repetition in   language models</div>
<div class="meta-line">Authors: Matéo Mahaut, Francesca Franzon</div>
<div class="meta-line">First: 2025-04-01T18:16:11+00:00 · Latest: 2025-11-04T16:26:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.01100v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.01100v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) can sometimes degrade into repetitive loops,
persistently generating identical word sequences. Because repetition is rare in
natural human language, its frequent occurrence across diverse tasks and
contexts in LLMs remains puzzling. Here we investigate whether behaviorally
similar repetition patterns arise from distinct underlying mechanisms and how
these mechanisms develop during model training. We contrast two conditions:
repetitions elicited by natural text prompts with those induced by in-context
learning (ICL) setups that explicitly require copying behavior. Our analyses
reveal that ICL-induced repetition relies on a dedicated network of attention
heads that progressively specialize over training, whereas naturally occurring
repetition emerges early and lacks a defined circuitry. Attention inspection
further shows that natural repetition focuses disproportionately on
low-information tokens, suggesting a fallback behavior when relevant context
cannot be retrieved. These results indicate that superficially similar
repetition behaviors originate from qualitatively different internal processes,
reflecting distinct modes of failure and adaptation in language models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) can sometimes degrade into repetitive loops, persistently generating identical word sequences.</div>
</details>
</div>
<div class="card">
<div class="title">DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across   the Web3 Domain</div>
<div class="meta-line">Authors: Enhao Huang, Pengyu Sun, Zixin Lin, Alex Chen, Joey Ouyang, Haobo Wang, Kaichun Hu, James Yi, Frank Li, Zhiyu Zhang, Tianxiang Xu, Gang Zhao, Ziang Ling, Lowes Yang</div>
<div class="meta-line">First: 2025-04-18T16:40:39+00:00 · Latest: 2025-11-04T16:26:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.16116v3">Abs</a> · <a href="http://arxiv.org/pdf/2504.16116v3">PDF</a> · <a href="https://huggingface.co/datasets/DMindAI/DMind_Benchmark">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have achieved impressive performance in diverse
natural language processing tasks, but specialized domains such as Web3 present
new challenges and require more tailored evaluation. Despite the significant
user base and capital flows in Web3, encompassing smart contracts,
decentralized finance (DeFi), non-fungible tokens (NFTs), decentralized
autonomous organizations (DAOs), on-chain governance, and novel
token-economics, no comprehensive benchmark has systematically assessed LLM
performance in this domain. To address this gap, we introduce the DMind
Benchmark, a holistic Web3-oriented evaluation suite covering nine critical
subfields: fundamental blockchain concepts, blockchain infrastructure, smart
contract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and
security vulnerabilities. Beyond multiple-choice questions, DMind Benchmark
features domain-specific tasks such as contract debugging and on-chain numeric
reasoning, mirroring real-world scenarios. We evaluated 26 models, including
ChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable
performance gaps in specialized areas like token economics and
security-critical contract analysis. While some models excel in blockchain
infrastructure tasks, advanced subfields remain challenging. Our benchmark
dataset and evaluation pipeline are open-sourced on
https://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in
Hugging Face&#x27;s trending dataset charts within a week of release.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) have achieved impressive performance in diverse natural language processing tasks, but specialized domains such as Web3 present new challenges and require more tailored evaluation.</div>
</details>
</div>
<div class="card">
<div class="title">Curriculum Design for Trajectory-Constrained Agent: Compressing   Chain-of-Thought Tokens in LLMs</div>
<div class="meta-line">Authors: Georgios Tzannetos, Parameswaran Kamalaruban, Adish Singla</div>
<div class="meta-line">Venue: NeurIPS</div>
<div class="meta-line">First: 2025-11-04T16:14:56+00:00 · Latest: 2025-11-04T16:14:56+00:00</div>
<div class="meta-line">Comments: NeurIPS&#x27;25 paper</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02690v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training agents to operate under strict constraints during deployment, such
as limited resource budgets or stringent safety requirements, presents
significant challenges, especially when these constraints render the task
complex. In this work, we propose a curriculum learning strategy that gradually
tightens constraints during training, enabling the agent to incrementally
master the deployment requirements. Inspired by self-paced learning techniques
in unconstrained reinforcement learning (RL), our approach facilitates a
smoother transition to challenging environments by initially training on
simplified versions of the constraints and progressively introducing the full
deployment conditions. We provide a theoretical analysis using an RL agent in a
binary-tree Markov Decision Process (MDP) to demonstrate that our curriculum
strategy can accelerate training relative to a baseline approach that imposes
the trajectory constraints from the outset. Moreover, we empirically validate
the effectiveness and generality of our method across both RL and large
language model (LLM) agents in diverse settings, including a binary-tree MDP, a
multi-task navigation domain, and a math reasoning task with two benchmarks.
These results highlight the potential of curriculum design in enhancing the
efficiency and performance of agents operating under complex trajectory
constraints during deployment. Moreover, when applied to LLMs, our strategy
enables compression of output chain-of-thought tokens, achieving a substantial
inference speedup on consumer hardware, demonstrating its effectiveness for
resource-constrained deployment.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Training agents to operate under strict constraints during deployment, such as limited resource budgets or stringent safety requirements, presents significant challenges, especially when these constraints render the task complex.</div>
</details>
</div>
<div class="card">
<div class="title">GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to   8K Resolution</div>
<div class="meta-line">Authors: Fengxiang Wang, Mingshuo Chen, Yueying Li, Di Wang, Haotian Wang, Zonghao Guo, Zefan Wang, Boqi Shan, Long Lan, Yulin Wang, Hongzhen Wang, Wenjing Yang, Bo Du, Jing Zhang</div>
<div class="meta-line">First: 2025-05-27T16:05:03+00:00 · Latest: 2025-11-04T15:32:06+00:00</div>
<div class="meta-line">Comments: NeurlPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.21375v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.21375v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data
for Earth observation but pose challenges for existing multimodal foundation
models due to two key bottlenecks: (1) limited availability of UHR training
data, and (2) token explosion caused by the large image size. To address data
scarcity, we introduce SuperRS-VQA (avg. 8,376$\times$8,376) and HighRS-VQA
(avg. 2,000$\times$1,912), the highest-resolution vision-language datasets in
RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,
our pilot studies reveal significant redundancy in RS images: crucial
information is concentrated in a small subset of object-centric tokens, while
pruning background tokens (e.g., ocean or forest) can even improve performance.
Motivated by these findings, we propose two strategies: Background Token
Pruning and Anchored Token Selection, to reduce the memory footprint while
preserving key semantics.Integrating these techniques, we introduce
GeoLLaVA-8K, the first RS-focused multimodal large language model capable of
handling inputs up to 8K$\times$8K resolution, built on the LLaVA framework.
Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art
on the XLRS-Bench.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size.</div>
</details>
</div>
<div class="card">
<div class="title">Differentiable Hierarchical Visual Tokenization</div>
<div class="meta-line">Authors: Marius Aasan, Martine Hjelkrem-Tan, Nico Catalano, Changkyu Choi, Adín Ramírez Rivera</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-11-04T15:18:29+00:00 · Latest: 2025-11-04T15:18:29+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02652v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02652v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Transformers rely on fixed patch tokens that ignore the spatial and
semantic structure of images. In this work, we introduce an end-to-end
differentiable tokenizer that adapts to image content with pixel-level
granularity while remaining backward-compatible with existing architectures for
retrofitting pretrained models. Our method uses hierarchical model selection
with information criteria to provide competitive performance in both
image-level classification and dense-prediction tasks, and even supports
out-of-the-box raster-to-vector conversion.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision Transformers rely on fixed patch tokens that ignore the spatial and semantic structure of images.</div>
</details>
</div>
<div class="card">
<div class="title">Can Visual Input Be Compressed? A Visual Token Compression Benchmark for   Large Multimodal Models</div>
<div class="meta-line">Authors: Tianfan Peng, Yuntao Du, Pengzhou Ji, Shijie Dong, Kailin Jiang, Mingchuan Ma, Yijun Tian, Jinhe Bi, Qian Li, Wei Du, Feng Xiao, Lizhen Cui</div>
<div class="meta-line">First: 2025-11-04T15:17:06+00:00 · Latest: 2025-11-04T15:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02650v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02650v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large multimodal models (LMMs) often suffer from severe inference
inefficiency due to the large number of visual tokens introduced by image
encoders. While recent token compression methods, such as pruning and merging,
have shown promise in reducing redundancy, their evaluation remains fragmented
and inconsistent. In this work, we present UniPruneBench, a unified and
extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench
provides standardized protocols across six ability dimensions and ten datasets,
covering ten representative compression algorithms and three families of LMMs
(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates
system-level metrics such as runtime and prefilling latency to provide a
holistic view. Our experiments uncover several key findings: (1) random pruning
is a surprisingly strong baseline, (2) no single method consistently
outperforms others across scenarios, (3) pruning sensitivity varies
significantly across tasks, with OCR being most vulnerable, and (4) pruning
ratio is the dominant factor governing performance degradation. We believe
UniPruneBench will serve as a reliable foundation for future research on
efficient multimodal modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders.</div>
</details>
</div>
<div class="card">
<div class="title">Federated Attention: A Distributed Paradigm for Collaborative LLM   Inference over Edge Networks</div>
<div class="meta-line">Authors: Xiumei Deng, Zehui Xiong, Binbin Chen, Dong In Kim, Merouane Debbah, H. Vincent Poor</div>
<div class="meta-line">First: 2025-11-04T15:14:58+00:00 · Latest: 2025-11-04T15:14:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02647v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02647v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn&#x27;s potential to deliver
scalability and efficiency in real-world edge deployments.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios.</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action   Model</div>
<div class="meta-line">Authors: John Won, Kyungmin Lee, Huiwon Jang, Dongyoung Kim, Jinwoo Shin</div>
<div class="meta-line">First: 2025-10-31T16:32:12+00:00 · Latest: 2025-11-04T14:46:28+00:00</div>
<div class="meta-line">Comments: 20 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.27607v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.27607v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, augmenting vision-language-action models (VLAs) with world-models
has shown promise in robotic policy learning. However, it remains challenging
to jointly predict next-state observations and action sequences because of the
inherent difference between the two modalities. To address this, we propose
DUal-STream diffusion (DUST), a world-model augmented VLA framework that
handles the modality conflict and enhances the performance of VLAs across
diverse tasks. Specifically, we propose a multimodal diffusion transformer
architecture that explicitly maintains separate modality streams while enabling
cross-modal knowledge sharing. In addition, we propose training techniques such
as independent noise perturbations for each modality and a decoupled flow
matching loss, which enables the model to learn the joint distribution in a
bidirectional manner while avoiding the need for a unified latent space.
Furthermore, based on the decoupled training framework, we introduce a sampling
method where we sample action and vision tokens asynchronously at different
rates, which shows improvement through inference-time scaling. Through
experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up
to 6% gains over a standard VLA baseline and implicit world-modeling methods,
with our inference-time scaling approach providing an additional 2-5% gain on
success rate. On real-world tasks with the Franka Research 3, DUST outperforms
baselines in success rate by 13%, confirming its effectiveness beyond
simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale
pretraining with action-free videos from BridgeV2, where DUST leads to
significant gain when transferred to the RoboCasa benchmark.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning.</div>
</details>
</div>
<div class="card">
<div class="title">UniChange: Unifying Change Detection with Multimodal Large Language   Model</div>
<div class="meta-line">Authors: Xu Zhang, Danyang Li, Xiaohang Dong, Tianhao Wu, Hualong Yu, Jianye Wang, Qicheng Li, Xiang Li</div>
<div class="meta-line">First: 2025-11-04T14:31:06+00:00 · Latest: 2025-11-04T14:31:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02607v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02607v1">PDF</a> · <a href="https://github.com/Erxucomeon/UniChange">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Change detection (CD) is a fundamental task for monitoring and analyzing land
cover dynamics. While recent high performance models and high quality datasets
have significantly advanced the field, a critical limitation persists. Current
models typically acquire limited knowledge from single-type annotated data and
cannot concurrently leverage diverse binary change detection (BCD) and semantic
change detection (SCD) datasets. This constraint leads to poor generalization
and limited versatility. The recent advancements in Multimodal Large Language
Models (MLLMs) introduce new possibilities for a unified CD framework. We
leverage the language priors and unification capabilities of MLLMs to develop
UniChange, the first MLLM-based unified change detection model. UniChange
integrates generative language abilities with specialized CD functionalities.
Our model successfully unifies both BCD and SCD tasks through the introduction
of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange
utilizes text prompts to guide the identification of change categories,
eliminating the reliance on predefined classification heads. This design allows
UniChange to effectively acquire knowledge from multi-source datasets, even
when their class definitions conflict. Experiments on four public benchmarks
(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,
achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,
surpassing all previous methods. The code is available at
https://github.com/Erxucomeon/UniChange.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics.</div>
</details>
</div>
<div class="card">
<div class="title">Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations   to Decode Student Behaviour</div>
<div class="meta-line">Authors: Max Norris, Kobi Gal, Sahan Bulathwela</div>
<div class="meta-line">First: 2025-11-04T14:20:56+00:00 · Latest: 2025-11-04T14:20:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.02599v1">Abs</a> · <a href="http://arxiv.org/pdf/2511.02599v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modelling student knowledge is a key challenge when leveraging AI in education, with major implications for personalised learning.</div>
</details>
</div>
<div class="card">
<div class="title">From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient   PDE Neural Operators</div>
<div class="meta-line">Authors: Lei Liu, Zhongyi Yu, Hong Wang, Huanshuo Dong, Haiyang Xin, Hongwei Zhao, Bin Li</div>
<div class="meta-line">First: 2025-10-27T03:58:09+00:00 · Latest: 2025-11-04T14:03:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.00032v2">Abs</a> · <a href="http://arxiv.org/pdf/2511.00032v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In recent years, Neural Operators(NO) have gradually emerged as a popular approach for solving Partial Differential Equations (PDEs).</div>
</details>
</div>
<div class="card">
<div class="title">ABS: Enforcing Constraint Satisfaction On Generated Sequences Via   Automata-Guided Beam Search</div>
<div class="meta-line">Authors: Vincenzo Collura, Karim Tit, Laura Bussi, Eleonora Giunchiglia, Maxime Cordy</div>
<div class="meta-line">First: 2025-06-11T13:14:01+00:00 · Latest: 2025-11-04T13:52:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.09701v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.09701v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sequence generation and prediction form a cornerstone of modern machine
learning, with applications spanning natural language processing, program
synthesis, and time-series forecasting. These tasks are typically modeled in an
autoregressive fashion, where each token is generated conditional on the
preceding ones, and beam search is commonly used to balance exploration and
fluency during decoding. While deep learning models and Large Language Models
(LLMs) excel at capturing statistical patterns in this setting, they remain
ill-equipped to guarantee compliance with formal constraints. In this paper, we
introduce ABS: a general and model-agnostic inference-time algorithm that
guarantees compliance with any constraint that can be compiled into a
Deterministic Finite Automaton (DFA), without requiring retraining. ABS
leverages the DFA to guide a constrained variant of beam search: at each
decoding step, transitions leading to violations are masked, while remaining
paths are dynamically re-ranked according to both the model&#x27;s probabilities and
the automaton&#x27;s acceptance structure. We formally prove that the resulting
sequences are guaranteed to satisfy the given constraints, and we empirically
demonstrate that ABS also improves output quality. We validate our approach on
three distinct tasks: constrained image-stream classification, controlled text
generation, and text infilling. In all settings, ABS achieves perfect
constraint satisfaction, while outperforming or matching state-of-the-art
baselines on standard quality metrics and efficiency.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Sequence generation and prediction form a cornerstone of modern machine learning, with applications spanning natural language processing, program synthesis, and time-series forecasting.</div>
</details>
</div>
<div class="card">
<div class="title">RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing</div>
<div class="meta-line">Authors: Fengxiang Wang, Yulin Wang, Mingshuo Chen, Haiyan Zhao, Yangang Sun, Shuo Wang, Hongzhen Wang, Di Wang, Long Lan, Wenjing Yang, Jing Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-03-13T14:09:18+00:00 · Latest: 2025-11-04T13:36:22+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.10392v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.10392v2">PDF</a> · <a href="https://github.com/MiliLab/RoMA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in self-supervised learning for Vision Transformers (ViTs)
have fueled breakthroughs in remote sensing (RS) foundation models. However,
the quadratic complexity of self-attention poses a significant barrier to
scalability, particularly for large models and high-resolution images. While
the linear-complexity Mamba architecture offers a promising alternative,
existing RS applications of Mamba remain limited to supervised tasks on small,
domain-specific datasets. To address these challenges, we propose RoMA, a
framework that enables scalable self-supervised pretraining of Mamba-based RS
foundation models using large-scale, diverse, unlabeled data. RoMA enhances
scalability for high-resolution images through a tailored auto-regressive
learning strategy, incorporating two key innovations: 1) a rotation-aware
pretraining mechanism combining adaptive cropping with angular embeddings to
handle sparsely distributed objects with arbitrary orientations, and 2)
multi-scale token prediction objectives that address the extreme variations in
object scales inherent to RS imagery. Systematic empirical studies validate
that Mamba adheres to RS data and parameter scaling laws, with performance
scaling reliably as model and data size increase. Furthermore, experiments
across scene classification, object detection, and semantic segmentation tasks
demonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based
counterparts in both accuracy and computational efficiency. The source code and
pretrained models will be released at https://github.com/MiliLab/RoMA.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in self-supervised learning for Vision Transformers (ViTs) have fueled breakthroughs in remote sensing (RS) foundation models.</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Generative Recommendation with Continuous Tokens</div>
<div class="meta-line">Authors: Haohao Qu, Shanru Lin, Yujuan Ding, Yiqi Wang, Wenqi Fan</div>
<div class="meta-line">First: 2025-04-16T12:01:03+00:00 · Latest: 2025-11-04T12:25:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.12007v3">Abs</a> · <a href="http://arxiv.org/pdf/2504.12007v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in generative artificial intelligence, particularly large
language models (LLMs), have opened new opportunities for enhancing recommender
systems (RecSys). Most existing LLM-based RecSys approaches operate in a
discrete space, using vector-quantized tokenizers to align with the inherent
discrete nature of language models. However, these quantization methods often
result in lossy tokenization and suboptimal learning, primarily due to
inaccurate gradient propagation caused by the non-differentiable argmin
operation in standard vector quantization. Inspired by the emerging trend of
embracing continuous tokens in language models, we propose ContRec, a novel
framework that seamlessly integrates continuous tokens into LLM-based RecSys.
Specifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which
encodes users/items with continuous tokens; and a Dispersive Diffusion module,
which captures implicit user preference. The tokenizer is trained with a
continuous Variational Auto-Encoder (VAE) objective, where three effective
techniques are adopted to avoid representation collapse. By conditioning on the
previously generated tokens of the LLM backbone during user modeling, the
Dispersive Diffusion module performs a conditional diffusion process with a
novel Dispersive Loss, enabling high-quality user preference generation through
next-token diffusion. Finally, ContRec leverages both the textual reasoning
output from the LLM and the latent representations produced by the diffusion
model for Top-K item retrieval, thereby delivering comprehensive recommendation
results. Extensive experiments on four datasets demonstrate that ContRec
consistently outperforms both traditional and SOTA LLM-based recommender
systems. Our results highlight the potential of continuous tokenization and
generative modeling for advancing the next generation of recommender systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in generative artificial intelligence, particularly large language models (LLMs), have opened new opportunities for enhancing recommender systems (RecSys).</div>
</details>
</div>
<div class="card">
<div class="title">Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning</div>
<div class="meta-line">Authors: Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao</div>
<div class="meta-line">Venue: NeurIPS 2025 spotlight</div>
<div class="meta-line">First: 2025-02-04T23:26:10+00:00 · Latest: 2025-11-04T12:04:06+00:00</div>
<div class="meta-line">Comments: To appear on NeurIPS 2025 (spotlight)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.02770v5">Abs</a> · <a href="http://arxiv.org/pdf/2502.02770v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging attention sparsity to accelerate long-context large language
models (LLMs) has been a hot research topic. However, current algorithms such
as sparse attention or key-value (KV) cache compression tend to use a fixed
budget, which presents a significant challenge during deployment because it
fails to account for the dynamic nature of real-world scenarios, where the
optimal balance between accuracy and efficiency can vary greatly. In this
paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse
attention can surprisingly achieve adaptive budgeting. Based on this, we
propose Twilight, a framework to bring adaptive sparsity to any existing sparse
attention algorithm without sacrificing their accuracy. Empirical results show
that Twilight can adaptively prune at most 98% of redundant tokens, leading to
$15.4\times$ acceleration in self-attention operations and $3.9\times$
acceleration in end-to-end per token latency in long context LLM decoding.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic.</div>
</details>
</div>
<div class="card">
<div class="title">Pay for The Second-Best Service: A Game-Theoretic Approach Against   Dishonest LLM Providers</div>
<div class="meta-line">Authors: Yuhan Cao, Yu Wang, Sitong Liu, Miao Li, Yixin Tao, Tianxing He</div>
<div class="meta-line">First: 2025-11-02T08:18:20+00:00 · Latest: 2025-11-04T11:48:22+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2511.00847v2">Abs</a> · <a href="http://arxiv.org/pdf/2511.00847v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread adoption of Large Language Models (LLMs) through Application
Programming Interfaces (APIs) induces a critical vulnerability: the potential
for dishonest manipulation by service providers. This manipulation can manifest
in various forms, such as secretly substituting a proclaimed high-performance
model with a low-cost alternative, or inflating responses with meaningless
tokens to increase billing. This work tackles the issue through the lens of
algorithmic game theory and mechanism design. We are the first to propose a
formal economic model for a realistic user-provider ecosystem, where a user can
iteratively delegate $T$ queries to multiple model providers, and providers can
engage in a range of strategic behaviors. As our central contribution, we prove
that for a continuous strategy space and any $\epsilon\in(0,\frac12)$, there
exists an approximate incentive-compatible mechanism with an additive
approximation ratio of $O(T^{1-\epsilon}\log T)$, and a guaranteed quasi-linear
second-best user utility. We also prove an impossibility result, stating that
no mechanism can guarantee an expected user utility that is asymptotically
better than our mechanism. Furthermore, we demonstrate the effectiveness of our
mechanism in simulation experiments with real-world API settings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The widespread adoption of Large Language Models (LLMs) through Application Programming Interfaces (APIs) induces a critical vulnerability: the potential for dishonest manipulation by service providers.</div>
</details>
</div>
<div class="card">
<div class="title">Improving Uncertainty Estimation through Semantically Diverse Language   Generation</div>
<div class="meta-line">Authors: Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, Sepp Hochreiter</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2024-06-06T17:53:34+00:00 · Latest: 2025-11-04T10:59:26+00:00</div>
<div class="meta-line">Comments: ICLR 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2406.04306v2">Abs</a> · <a href="http://arxiv.org/pdf/2406.04306v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) can suffer from hallucinations when generating
text. These hallucinations impede various applications in society and industry
by making LLMs untrustworthy. Current LLMs generate text in an autoregressive
fashion by predicting and appending text tokens. When an LLM is uncertain about
the semantic meaning of the next tokens to generate, it is likely to start
hallucinating. Thus, it has been suggested that predictive uncertainty is one
of the main causes of hallucinations. We introduce Semantically Diverse
Language Generation (SDLG) to quantify predictive uncertainty in LLMs. SDLG
steers the LLM to generate semantically diverse yet likely alternatives for an
initially generated text. This approach provides a precise measure of aleatoric
semantic uncertainty, detecting whether the initial text is likely to be
hallucinated. Experiments on question-answering tasks demonstrate that SDLG
consistently outperforms existing methods while being the most computationally
efficient, setting a new standard for uncertainty estimation in LLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) can suffer from hallucinations when generating text.</div>
</details>
</div>
<div class="card">
<div class="title">Tongyi DeepResearch Technical Report</div>
<div class="meta-line">Authors: Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Minpeng Liao, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang</div>
<div class="meta-line">First: 2025-10-28T17:53:02+00:00 · Latest: 2025-11-04T10:23:45+00:00</div>
<div class="meta-line">Comments: https://tongyi-agent.github.io/blog</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24701v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.24701v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tongyi-agent.github.io/blog">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Tongyi DeepResearch, an agentic large language model, which is
specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is
developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent
interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total
parameters, with only 3.3 billion activated per token, achieves
state-of-the-art performance across a range of agentic deep research
benchmarks, including Humanity&#x27;s Last Exam, BrowseComp, BrowseComp-ZH,
WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We
open-source the model, framework, and complete solutions to empower the
community.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Dense Backpropagation Improves Training for Sparse Mixture-of-Experts</div>
<div class="meta-line">Authors: Ashwinee Panda, Vatsal Baherwani, Zain Sarwar, Benjamin Therien, Sambit Sahu, Tom Goldstein, Supriyo Chakraborty</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-04-16T19:55:36+00:00 · Latest: 2025-11-04T10:16:54+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.12463v3">Abs</a> · <a href="http://arxiv.org/pdf/2504.12463v3">PDF</a> · <a href="https://github.com/vatsal0/default-moe">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mixture of Experts (MoE) pretraining is more scalable than dense Transformer
pretraining, because MoEs learn to route inputs to a sparse set of their
feedforward parameters. However, this means that MoEs only receive a sparse
backward update, leading to training instability and suboptimal performance. We
present a lightweight approximation method that gives the MoE router a dense
gradient update while continuing to sparsely activate its parameters. Our
method, which we refer to as Default MoE, substitutes missing expert
activations with default outputs consisting of an exponential moving average of
expert outputs previously seen over the course of training. This allows the
router to receive signals from every expert for each token, leading to
significant improvements in training performance. Our Default MoE outperforms
standard TopK routing in a variety of settings without requiring significant
computational overhead. Code: https://github.com/vatsal0/default-moe.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Mixture of Experts (MoE) pretraining is more scalable than dense Transformer pretraining, because MoEs learn to route inputs to a sparse set of their feedforward parameters.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251105_0315.html">20251105_0315</a>
<a href="archive/20251104_0314.html">20251104_0314</a>
<a href="archive/20251103_0313.html">20251103_0313</a>
<a href="archive/20251102_0313.html">20251102_0313</a>
<a href="archive/20251101_0314.html">20251101_0314</a>
<a href="archive/20251031_0314.html">20251031_0314</a>
<a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
