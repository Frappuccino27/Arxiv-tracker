<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-17 03:12</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251017_0312</div>
    <div class="row"><div class="card">
<div class="title">Reasoning in Space via Grounding in the World</div>
<div class="meta-line">Authors: Yiming Chen, Zekun Qi, Wenyao Zhang, Xin Jin, Li Zhang, Peidong Liu</div>
<div class="meta-line">First: 2025-10-15T17:58:08+00:00 · Latest: 2025-10-15T17:58:08+00:00</div>
<div class="meta-line">Comments: 20 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13800v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13800v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we claim that 3D visual grounding is the cornerstone of
spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to
explore the effective spatial representations that bridge the gap between them.
Existing 3D LLMs suffer from the absence of a unified 3D representation capable
of jointly capturing semantic and geometric information. This deficiency is
manifested either in poor performance on grounding or in an excessive reliance
on external modules, ultimately hindering the seamless integration of grounding
and spatial reasoning. To address this, we propose a simple yet effective
dual-path pooling mechanism that tightly aligns geometric features with both
semantic and positional cues, constructing a unified image patch-based 3D
representation that encapsulates all essential information without increasing
the number of input tokens. Leveraging this holistic representation,
GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely
without external modules while delivering performance comparable to
state-of-the-art models, establishing a unified and self-contained framework
for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we
introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is
meticulously curated to include both 3D bounding box annotations for objects
referenced in reasoning questions and step-by-step reasoning paths that
integrate grounding as a core component of the problem-solving process.
Extensive experiments demonstrate that GS-Reasoner achieves impressive results
on 3D visual grounding, which in turn significantly enhances its spatial
reasoning capabilities, leading to state-of-the-art performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them.</div>
</details>
</div>
<div class="card">
<div class="title">T3former: Temporal Graph Classification with Topological Machine   Learning</div>
<div class="meta-line">Authors: Md. Joshem Uddin, Soham Changani, Baris Coskunuzer</div>
<div class="meta-line">First: 2025-10-15T17:46:32+00:00 · Latest: 2025-10-15T17:46:32+00:00</div>
<div class="meta-line">Comments: 14 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13789v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporal graph classification plays a critical role in applications such as
cybersecurity, brain connectivity analysis, social dynamics, and traffic
monitoring. Despite its significance, this problem remains underexplored
compared to temporal link prediction or node forecasting. Existing methods
often rely on snapshot-based or recurrent architectures that either lose
fine-grained temporal information or struggle with long-range dependencies.
Moreover, local message-passing approaches suffer from oversmoothing and
oversquashing, limiting their ability to capture complex temporal structures.
  We introduce T3former, a novel Topological Temporal Transformer that
leverages sliding-window topological and spectral descriptors as first-class
tokens, integrated via a specialized Descriptor-Attention mechanism. This
design preserves temporal fidelity, enhances robustness, and enables principled
cross-modal fusion without rigid discretization. T3former achieves
state-of-the-art performance across multiple benchmarks, including dynamic
social networks, brain functional connectivity datasets, and traffic networks.
It also offers theoretical guarantees of stability under temporal and
structural perturbations. Our results highlight the power of combining
topological and spectral insights for advancing the frontier of temporal graph
learning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Temporal graph classification plays a critical role in applications such as cybersecurity, brain connectivity analysis, social dynamics, and traffic monitoring.</div>
</details>
</div>
<div class="card">
<div class="title">Teaching Models to Understand (but not Generate) High-risk Data</div>
<div class="meta-line">Authors: Ryan Wang, Matthew Finlayson, Luca Soldaini, Swabha Swayamdipta, Robin Jia</div>
<div class="meta-line">First: 2025-05-05T22:24:06+00:00 · Latest: 2025-10-15T17:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.03052v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.03052v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language model developers typically filter out high-risk content -- such as
toxic or copyrighted text -- from their pre-training data to prevent models
from generating similar outputs. However, removing such data altogether limits
models&#x27; ability to recognize and appropriately respond to harmful or sensitive
content. In this paper, we introduce Selective Loss to Understand but Not
Generate (SLUNG), a pre-training paradigm through which models learn to
understand high-risk data without learning to generate it. Instead of uniformly
applying the next-token prediction loss, SLUNG selectively avoids incentivizing
the generation of high-risk tokens while ensuring they remain within the
model&#x27;s context window. As the model learns to predict low-risk tokens that
follow high-risk ones, it is forced to understand the high-risk content.
Through our experiments, we show that SLUNG consistently improves models&#x27;
understanding of high-risk data (e.g., ability to recognize toxic content)
without increasing its generation (e.g., toxicity of model responses). Overall,
our SLUNG paradigm enables models to benefit from high-risk text that would
otherwise be filtered out.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs.</div>
</details>
</div>
<div class="card">
<div class="title">ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel   Vision Transformers for Improved Cross-Channel Learning</div>
<div class="meta-line">Authors: Chau Pham, Juan C. Caicedo, Bryan A. Plummer</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-03-25T03:45:59+00:00 · Latest: 2025-10-15T17:17:25+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.19331v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.19331v2">PDF</a> · <a href="https://github.com/chaudatascience/cha_mae_vit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prior work using Masked Autoencoders (MAEs) typically relies on random patch
masking based on the assumption that images have significant redundancies
across different channels, allowing for the reconstruction of masked content
using cross-channel correlations. However, this assumption does not hold in
Multi-Channel Imaging (MCI), where channels may provide complementary
information with minimal feature overlap. Thus, these MAEs primarily learn
local structures within individual channels from patch reconstruction, failing
to fully leverage cross-channel interactions and limiting their MCI
effectiveness. In this paper, we present ChA-MAEViT, an MAE-based method that
enhances feature learning across MCI channels via four key strategies: (1)
dynamic channel-patch masking, which compels the model to reconstruct missing
channels in addition to masked patches, thereby enhancing cross-channel
dependencies and improving robustness to varying channel configurations; (2)
memory tokens, which serve as long-term memory aids to promote information
sharing across channels, addressing the challenges of reconstructing
structurally diverse channels; (3) hybrid token fusion module, which merges
fine-grained patch tokens with a global class token to capture richer
representations; and (4) Channel-Aware Decoder, a lightweight decoder utilizes
channel tokens to effectively reconstruct image patches. Experiments on
satellite and microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, show that
ChA-MAEViT significantly outperforms state-of-the-art MCI-ViTs by 3.0-21.5%,
highlighting the importance of cross-channel interactions in MCI. Our code is
publicly available at https://github.com/chaudatascience/cha_mae_vit.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Prior work using Masked Autoencoders (MAEs) typically relies on random patch masking based on the assumption that images have significant redundancies across different channels, allowing for the reconstruction of masked content using cross-channel correlations.</div>
</details>
</div>
<div class="card">
<div class="title">FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI   Model Access</div>
<div class="meta-line">Authors: Aditya Tanikanti, Benoit Côté, Yanfei Guo, Le Chen, Nickolaus Saint, Ryan Chard, Ken Raffenetti, Rajeev Thakur, Thomas Uram, Ian Foster, Michael E. Papka, Venkatram Vishwanath</div>
<div class="meta-line">First: 2025-10-15T16:28:34+00:00 · Latest: 2025-10-15T16:28:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13724v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13724v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
&quot;hot&quot; nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present the Federated Inference Resource Scheduling Toolkit (FIRST), a framework enabling Inference-as-a-Service across distributed High-Performance Computing (HPC) clusters.</div>
</details>
</div>
<div class="card">
<div class="title">On Pretraining for Project-Level Code Completion</div>
<div class="meta-line">Authors: Maksim Sapronov, Evgeniy Glukhov</div>
<div class="meta-line">First: 2025-10-15T15:55:19+00:00 · Latest: 2025-10-15T15:55:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13697v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Repository-level pretraining is commonly used to enable large language models for code to leverage codebase-wide context.</div>
</details>
</div>
<div class="card">
<div class="title">I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic   Representations in LLaMA 3.2</div>
<div class="meta-line">Authors: Oliver McLaughlin, Arjun Khurana, Jack Merullo</div>
<div class="meta-line">First: 2025-08-04T15:36:51+00:00 · Latest: 2025-10-15T15:52:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.02527v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.02527v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models demonstrate proficiency on phonetic tasks, such as
rhyming, without explicit phonetic or auditory grounding. In this work, we
investigate how \verb|Llama-3.2-1B-Instruct| represents token-level phonetic
information. Our results suggest that Llama uses a rich internal model of
phonemes to complete phonetic tasks. We provide evidence for high-level
organization of phoneme representations in its latent space. In doing so, we
also identify a ``phoneme mover head&quot; which promotes phonetic information
during rhyming tasks. We visualize the output space of this head and find that,
while notable differences exist, Llama learns a model of vowels similar to the
standard IPA vowel chart for humans, despite receiving no direct supervision to
do so.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models demonstrate proficiency on phonetic tasks, such as rhyming, without explicit phonetic or auditory grounding.</div>
</details>
</div>
<div class="card">
<div class="title">Information-Theoretic Reward Modeling for Stable RLHF: Detecting and   Mitigating Reward Hacking</div>
<div class="meta-line">Authors: Yuchun Miao, Liang Ding, Sen Zhang, Rong Bao, Lefei Zhang, Dacheng Tao</div>
<div class="meta-line">First: 2025-10-15T15:51:59+00:00 · Latest: 2025-10-15T15:51:59+00:00</div>
<div class="meta-line">Comments: 46 pages, 36 figures, submitted to IEEE Transactions on Pattern
  Analysis and Machine Intelligence</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13694v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13694v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the success of Reinforcement Learning from Human Feedback (RLHF) in
aligning language models with human values, reward hacking-or reward
over-optimization-remains a major challenge. We identify two key obstacles to
its mitigation: (1) reward misgeneralization in reward modeling, where reward
models overfit to spurious, preference-irrelevant features; and (2) the lack of
suitable regularization during RL optimization, as existing token-level
constraints often over-restrict the policy space. To address these issues, we
propose InfoRM, an information-theoretic reward modeling framework based on the
Information Bottleneck (IB) principle, which filters out preference-irrelevant
information to alleviate reward misgeneralization. We further observe that
reward-hacked responses manifest as pronounced outliers in InfoRM&#x27;s IB latent
space, measured by Mahalanobis distance from the SFT-induced distribution.
Motivated by this, we introduce IBL, a distribution-level regularization that
penalizes such deviations, effectively expanding the optimization landscape
while maintaining alignment. We prove that IBL is theoretically equivalent to
the pessimistic RL objective within the IB latent space. Finally, we present
Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying
reward hacking severity, enabling principled hyperparameter tuning and online
mitigation such as early stopping. Extensive experiments across diverse LLMs
and datasets confirm the generality of our findings, the effectiveness of
InfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively
advancing the state of RLHF.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite the success of Reinforcement Learning from Human Feedback (RLHF) in aligning language models with human values, reward hacking-or reward over-optimization-remains a major challenge.</div>
</details>
</div>
<div class="card">
<div class="title">CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas</div>
<div class="meta-line">Authors: Zian Li, Muhan Zhang</div>
<div class="meta-line">First: 2025-10-15T15:29:09+00:00 · Latest: 2025-10-15T15:29:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13669v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13669v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked autoregressive models (MAR) have recently emerged as a powerful
paradigm for image and video generation, combining the flexibility of masked
modeling with the potential of continuous tokenizer. However, video MAR models
suffer from two major limitations: the slow-start problem, caused by the lack
of a structured global prior at early sampling stages, and error accumulation
across the autoregression in both spatial and temporal dimensions. In this
work, we propose CanvasMAR, a novel video MAR model that mitigates these issues
by introducing a canvas mechanism--a blurred, global prediction of the next
frame, used as the starting point for masked generation. The canvas provides
global structure early in sampling, enabling faster and more coherent frame
synthesis. Furthermore, we introduce compositional classifier-free guidance
that jointly enlarges spatial (canvas) and temporal conditioning, and employ
noise-based canvas augmentation to enhance robustness. Experiments on the BAIR
and Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality
videos with fewer autoregressive steps. Our approach achieves remarkable
performance among autoregressive models on Kinetics-600 dataset and rivals
diffusion-based methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Masked autoregressive models (MAR) have recently emerged as a powerful paradigm for image and video generation, combining the flexibility of masked modeling with the potential of continuous tokenizer.</div>
</details>
</div>
<div class="card">
<div class="title">FLARE: Fast Low-rank Attention Routing Engine</div>
<div class="meta-line">Authors: Vedant Puri, Aditya Joglekar, Kevin Ferguson, Yu-hsuan Chen, Yongjie Jessica Zhang, Levent Burak Kara</div>
<div class="meta-line">First: 2025-08-18T03:00:55+00:00 · Latest: 2025-10-15T15:13:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.12594v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.12594v2">PDF</a> · <a href="https://github.com/vpuri3/FLARE.py">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The quadratic complexity of self-attention limits its applicability and
scalability on large unstructured meshes. We introduce Fast Low-rank Attention
Routing Engine (FLARE), a linear complexity self-attention mechanism that
routes attention through fixed-length latent sequences. Each attention head
performs global communication among $N$ tokens by projecting the input sequence
onto a fixed length latent sequence of $M \ll N$ tokens using learnable query
tokens. By routing attention through a bottleneck sequence, FLARE learns a
low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only
scales to unprecedented problem sizes, but also delivers superior accuracy
compared to state-of-the-art neural PDE surrogates across diverse benchmarks.
We also release a new additive manufacturing dataset to spur further research.
Our code is available at https://github.com/vpuri3/FLARE.py.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The quadratic complexity of self-attention limits its applicability and scalability on large unstructured meshes.</div>
</details>
</div>
<div class="card">
<div class="title">NOSA: Native and Offloadable Sparse Attention</div>
<div class="meta-line">Authors: Yuxiang Huang, Chaojun Xiao, Xu Han, Zhiyuan Liu</div>
<div class="meta-line">First: 2025-10-15T14:33:16+00:00 · Latest: 2025-10-15T14:33:16+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13602v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Trainable sparse attention has emerged as a promising solution to address the
decoding efficiency bottleneck of LLMs in long-context processing,
significantly saving memory accesses while minimally impacting task
performance. However, existing sparse attention methods leave a crucial
limitation unresolved: the size of the key-value (KV) cache remains unreduced,
which constrains on-GPU batch sizes and throttles decoding throughput,
especially in large-scale batched inference. In this paper, we show that
trainable sparse attention naturally exhibits strong locality in token
selection across adjacent decoding steps, thereby enabling KV cache offloading
without altering the underlying attention computation. However, the inherent
locality remains insufficient to achieve efficient offloading, as the transfer
of selected KV pairs between the CPU and GPU continues to dominate the overall
decoding cost. Building on this insight, we present NOSA, a trainable sparse
attention framework designed to natively support KV cache offloading. NOSA
introduces explicit locality constraints by decomposing token selection into
query-aware and query-agnostic components, thereby reducing KV transfers while
preserving the same attention computation as used during training. We pretrain
a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that
it preserves near-lossless performance while achieving up to a 2.3x improvement
in decoding throughput compared with the vanilla trainable sparse attention
baseline (InfLLM-V2).</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs in long-context processing, significantly saving memory accesses while minimally impacting task performance.</div>
</details>
</div>
<div class="card">
<div class="title">EEGChaT: A Transformer-Based Modular Channel Selector for SEEG Analysis</div>
<div class="meta-line">Authors: Chen Wang, Yansen Wang, Dongqi Han, Zilong Wang, Dongsheng Li</div>
<div class="meta-line">First: 2025-10-15T14:22:07+00:00 · Latest: 2025-10-15T14:22:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13592v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13592v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Analyzing stereoelectroencephalography (SEEG) signals is critical for
brain-computer interface (BCI) applications and neuroscience research, yet
poses significant challenges due to the large number of input channels and
their heterogeneous relevance. Traditional channel selection methods struggle
to scale or provide meaningful interpretability for SEEG data. In this work, we
propose EEGChaT, a novel Transformer-based channel selection module designed to
automatically identify the most task-relevant channels in SEEG recordings.
EEGChaT introduces Channel Aggregation Tokens (CATs) to aggregate information
across channels, and leverages an improved Attention Rollout technique to
compute interpretable, quantitative channel importance scores. We evaluate
EEGChaT on the DuIN dataset, demonstrating that integrating EEGChaT with
existing classification models consistently improves decoding accuracy,
achieving up to 17\% absolute gains. Furthermore, the channel weights produced
by EEGChaT show substantial overlap with manually selected channels, supporting
the interpretability of the approach. Our results suggest that EEGChaT is an
effective and generalizable solution for channel selection in high-dimensional
SEEG analysis, offering both enhanced performance and insights into neural
signal relevance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Analyzing stereoelectroencephalography (SEEG) signals is critical for brain-computer interface (BCI) applications and neuroscience research, yet poses significant challenges due to the large number of input channels and their heterogeneous relevance.</div>
</details>
</div>
<div class="card">
<div class="title">Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm   Enables Fine-Grained Policy Optimization</div>
<div class="meta-line">Authors: Yang Li, Zhichen Dong, Yuhan Sun, Weixun Wang, Shaopan Xiong, Yijia Luo, Jiashun Liu, Han Lu, Jiamang Wang, Wenbo Su, Bo Zheng, Junchi Yan</div>
<div class="meta-line">First: 2025-10-15T13:49:51+00:00 · Latest: 2025-10-15T13:49:51+00:00</div>
<div class="meta-line">Comments: 23 pages, 8 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13554v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13554v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reasoning pattern of Large language models (LLMs) remains opaque, and
Reinforcement learning (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish attention heads
between locally and globally focused information processing and reveal that
locally focused heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while globally focused heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) Windowed Average Attention Distance, which measures the extent of backward
attention within a clipped window; 2) Future Attention Influence, which
quantifies a token&#x27;s global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform targeted credit assignment to critical nodes (preplan
tokens, anchor tokens, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model&#x27;s intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The reasoning pattern of Large language models (LLMs) remains opaque, and Reinforcement learning (RL) typically applies uniform credit across an entire generation, blurring the distinction between pivotal and routine steps.</div>
</details>
</div>
<div class="card">
<div class="title">Tandem Training for Language Models</div>
<div class="meta-line">Authors: Robert West, Ashton Anderson, Ece Kamar, Eric Horvitz</div>
<div class="meta-line">First: 2025-10-15T13:48:16+00:00 · Latest: 2025-10-15T13:48:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13551v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13551v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As language models continue to rapidly improve, we can expect their actions
and reasoning to become difficult or impossible for weaker agents and humans to
follow, undermining interpretability and oversight. With an eye on long-term
futures, we pursue methods that encourage models to produce solutions that
remain intelligible to weaker collaborators. We formalize intelligibility as
handoff robustness: a strong model&#x27;s solution is intelligible to a weaker model
if randomly handing off control to the weaker model along the solution path
does not cause failure. Building on this criterion, we introduce tandem
training for language models, a reinforcement learning (RL) paradigm in which
rollout tokens are intermittently and randomly sampled from a frozen weak model
rather than the strong model being trained. Because rollouts succeed only when
the strong model&#x27;s actions and reasoning process can be continued by the weak
model -- when the two can co-construct a successful solution -- optimizing
standard RL objectives with tandem training implicitly incentivizes both
correctness and intelligibility. In the GSM8K math reasoning task, tandem
training reliably teaches models to abandon jargon and adapt their language to
weaker partners while keeping task accuracy high. Our results demonstrate a
promising route to building AI systems that remain auditable by weaker agents,
with implications for human--AI collaboration and multi-agent communication.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight.</div>
</details>
</div>
<div class="card">
<div class="title">Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen   Visual Unicode Representations</div>
<div class="meta-line">Authors: A. Bochkov</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research (TMLR), 2025</div>
<div class="meta-line">First: 2025-07-07T11:17:32+00:00 · Latest: 2025-10-15T13:46:44+00:00</div>
<div class="meta-line">Comments: Published in Transactions on Machine Learning Research (10/2025).
  OpenReview: https://openreview.net/forum?id=Odh8IynO1o</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.04886v4">Abs</a> · <a href="http://arxiv.org/pdf/2507.04886v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding the locus of semantic representation in large language models
(LLMs) is crucial for interpretability and architectural innovation. The
dominant paradigm posits that trainable input embeddings serve as foundational
&quot;meaning vectors.&quot; This paper challenges that view. We construct Transformer
models where the embedding layer is entirely frozen, with vectors derived not
from data, but from the visual structure of Unicode glyphs. These non-semantic,
precomputed visual embeddings are fixed throughout training. Our method is
compatible with any tokenizer, including a novel Unicode-centric tokenizer we
introduce to ensure universal text coverage. Despite the absence of trainable,
semantically initialized embeddings, our models converge, generate coherent
text, and, critically, outperform architecturally identical models with
trainable embeddings on the MMLU reasoning benchmark. We attribute this to
&quot;representational interference&quot; in conventional models, where the embedding
layer is burdened with learning both structural and semantic features. Our
results indicate that high-level semantics are not inherent to input embeddings
but are an emergent property of the Transformer&#x27;s compositional architecture
and data scale. This reframes the role of embeddings from meaning containers to
structural primitives. We release all code and models to foster further
research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation.</div>
</details>
</div>
<div class="card">
<div class="title">MedDINOv3: How to adapt vision foundation models for medical image   segmentation?</div>
<div class="meta-line">Authors: Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang</div>
<div class="meta-line">First: 2025-09-02T14:44:43+00:00 · Latest: 2025-10-15T13:42:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.02379v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.02379v3">PDF</a> · <a href="https://github.com/ricklisz/MedDINOv3">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate segmentation of organs and tumors in CT and MRI scans is essential
for diagnosis, treatment planning, and disease monitoring. While deep learning
has advanced automated segmentation, most models remain task-specific, lacking
generalizability across modalities and institutions. Vision foundation models
(FMs) pretrained on billion-scale natural images offer powerful and
transferable representations. However, adapting them to medical imaging faces
two key challenges: (1) the ViT backbone of most foundation models still
underperform specialized CNNs on medical image segmentation, and (2) the large
domain gap between natural and medical images limits transferability. We
introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to
medical segmentation. We first revisit plain ViTs and design a simple and
effective architecture with multi-scale token aggregation. Then, we perform
domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT
slices, using a multi-stage DINOv3 recipe to learn robust dense features.
MedDINOv3 matches or exceeds state-of-the-art performance across four
segmentation benchmarks, demonstrating the potential of vision foundation
models as unified backbones for medical image segmentation. The code is
available at https://github.com/ricklisz/MedDINOv3.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring.</div>
</details>
</div>
<div class="card">
<div class="title">CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy   Optimization in Reinforcement Learning</div>
<div class="meta-line">Authors: Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou</div>
<div class="meta-line">First: 2025-09-25T03:22:04+00:00 · Latest: 2025-10-15T13:41:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.20712v4">Abs</a> · <a href="http://arxiv.org/pdf/2509.20712v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}oordinating \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Confidence as a Reward: Transforming LLMs into Reward Models</div>
<div class="meta-line">Authors: He Du, Bowen Li, Chengxing Xie, Chang Gao, Kai Chen, Dacheng Tao</div>
<div class="meta-line">First: 2025-10-15T12:51:47+00:00 · Latest: 2025-10-15T12:51:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13501v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13501v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models can significantly enhance the reasoning capabilities of large
language models (LLMs), but they typically require extensive curated data and
costly training. To mitigate these challenges, training-free approaches such as
LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate
responses, achieving promising results. Recent works have also indicated that
model confidence can serve effectively as a reward metric, distinguishing
between chain-of-thought (CoT) and non-CoT paths. However, the concept of using
confidence as a reward has not been comprehensively studied. In this work, we
systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful
training-free method that utilizes token-level confidence in the model&#x27;s final
answers as a proxy for reward, especially suitable for close-ended tasks.
Through extensive experiments on mathematical reasoning tasks, we demonstrate
that CRew outperforms existing training-free reward approaches on the MATH500
and RewardMATH benchmarks, and even surpasses most trained reward models. We
further identify a strong correlation between CRew scores and the actual
reasoning performance of the model. Additionally, we find that CRew can
effectively filter high-quality training data. Building upon these insights, we
propose CRew-DPO, a training strategy that constructs preference data from
confidence scores combined with correctness signals. Finetuning with CRew-DPO
further enhances the model&#x27;s judging capabilities and consistently outperforms
existing self-training methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reward models can significantly enhance the reasoning capabilities of large language models (LLMs), but they typically require extensive curated data and costly training.</div>
</details>
</div>
<div class="card">
<div class="title">Tahakom LLM guidelines and receipts: from pre-training data to an Arabic   LLM</div>
<div class="meta-line">Authors: Areej AlOtaibi, Lina Alyahya, Raghad Alshabanah, Shahad Alfawzan, Shuruq Alarefei, Reem Alsabti, Nouf Alsubaie, Abdulaziz Alhuzaymi, Lujain Alkhelb, Majd Alsayari, Waad Alahmed, Omar Talabay, Jalal Alowibdi, Salem Alelyani, Adel Bibi</div>
<div class="meta-line">First: 2025-10-15T12:27:34+00:00 · Latest: 2025-10-15T12:27:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13481v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13481v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have significantly advanced the field of natural
language processing, enhancing capabilities in both language understanding and
generation across diverse domains. However, developing LLMs for Arabic presents
unique challenges. This paper explores these challenges by focusing on critical
aspects such as data curation, tokenizer design, and evaluation. We detail our
approach to the collection and filtration of Arabic pre-training datasets,
assess the impact of various tokenizer designs on model performance, and
examine the limitations of existing Arabic evaluation frameworks, for which we
propose a systematic corrective methodology. To promote transparency and
facilitate collaborative development, we share our data and methodologies,
contributing to the advancement of language modeling, particularly for the
Arabic language.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) have significantly advanced the field of natural language processing, enhancing capabilities in both language understanding and generation across diverse domains.</div>
</details>
</div>
<div class="card">
<div class="title">Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like   Specialization</div>
<div class="meta-line">Authors: Badr AlKhamissi, C. Nicolò De Sabbata, Greta Tuckute, Zeming Chen, Martin Schrimpf, Antoine Bosselut</div>
<div class="meta-line">First: 2025-06-16T10:21:54+00:00 · Latest: 2025-10-15T12:04:23+00:00</div>
<div class="meta-line">Comments: Preprint. Project Page at https://cognitive-reasoners.epfl.ch</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.13331v2">Abs</a> · <a href="http://arxiv.org/pdf/2506.13331v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human cognitive behavior arises from the interaction of specialized brain
networks dedicated to distinct functions, such as language, logic, and social
reasoning. Inspired by this organization, we propose Mixture of Cognitive
Reasoners (MiCRo): a modular, transformer-based architecture post-trained with
a curriculum that induces functional specialization across experts. Concretely,
we partition the layers of a pretrained language model into four expert modules
aligned with well-studied cognitive networks in the human brain. MiCRo offers
three key advantages over standard language models. (1) The specialized experts
are interpretable and causally meaningful -- ablating a module causes
substantial drops on benchmarks requiring its specialized domain. (2) MiCRo&#x27;s
behavior can be dynamically steered at inference time by routing tokens to
particular experts (e.g., favoring social over logical reasoning), enabling
fine-grained control over outputs. (3) MiCRo outperforms or matches comparable
baselines on both machine-learning reasoning benchmarks (e.g., GSM8K, BBH) and
alignment to human behavior (CogBench), while maintaining interpretability.
Taken together, cognitively grounded functional specialization yields models
that are both more human-like and more human-interpretable.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Human cognitive behavior arises from the interaction of specialized brain networks dedicated to distinct functions, such as language, logic, and social reasoning.</div>
</details>
</div>
<div class="card">
<div class="title">F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs</div>
<div class="meta-line">Authors: Jude Haris, José Cano</div>
<div class="meta-line">First: 2025-10-15T10:56:37+00:00 · Latest: 2025-10-15T10:56:37+00:00</div>
<div class="meta-line">Comments: Accepted to Workshop on New Approaches for Addressing the Computing
  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13401v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have become increasingly prominent for daily
tasks, from improving sound-totext translation to generating additional frames
for the latest video games. With the help of LLM inference frameworks, such as
llama.cpp, which support optimizations such as KV-caching and quantization, it
is now easier than ever to deploy LLMs on edge devices. Quantization is
fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp
utilizes block floating point (BFP) quantization to drastically reduce the bit
width of weights and input tensors, the memory footprint, and the computational
power required to run LLMs. LLMs are typically quantized with mixed BFP
quantization across the model layers to reduce the loss of model accuracy due
to quantization. Therefore, to efficiently accelerate across the layers of
BFP-quantized LLMs, specialized accelerators need to support different BFP
variants without reconfiguration. To address this issue, we propose a Flexible
Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically
switch between two BFP quantization variants and perform matrix multiplication
(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD
Kria board, reduces inference time by 1.4x on average over the Arm NEON-based
CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per
second (~3.9 words per second).</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) have become increasingly prominent for daily tasks, from improving sound-totext translation to generating additional frames for the latest video games.</div>
</details>
</div>
<div class="card">
<div class="title">Hints of Prompt: Enhancing Visual Representation for Multimodal LLMs in   Autonomous Driving</div>
<div class="meta-line">Authors: Hao Zhou, Zhanning Gao, Zhili Chen, Maosheng Ye, Qifeng Chen, Tongyi Cao, Honggang Qi</div>
<div class="meta-line">First: 2024-11-20T06:58:33+00:00 · Latest: 2025-10-15T09:13:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2411.13076v2">Abs</a> · <a href="http://arxiv.org/pdf/2411.13076v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In light of the dynamic nature of autonomous driving environments and
stringent safety requirements, general MLLMs combined with CLIP alone often
struggle to accurately represent driving-specific scenarios, particularly in
complex interactions and long-tail cases. To address this, we propose the Hints
of Prompt (HoP) framework, which introduces three key enhancements: Affinity
hint to emphasize instance-level structure by strengthening token-wise
connections, Semantic hint to incorporate high-level information relevant to
driving-specific cases, such as complex interactions among vehicles and traffic
signs, and Question hint to align visual features with the query context,
focusing on question-relevant regions. These hints are fused through a Hint
Fusion module, enriching visual representations by capturing driving-related
representations with limited domain data, ensuring faster adaptation to driving
scenarios. Extensive experiments confirm the effectiveness of the HoP
framework, showing that it significantly outperforms previous state-of-the-art
methods in all key metrics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In light of the dynamic nature of autonomous driving environments and stringent safety requirements, general MLLMs combined with CLIP alone often struggle to accurately represent driving-specific scenarios, particularly in complex interactions and long-tail cases.</div>
</details>
</div>
<div class="card">
<div class="title">Self-Augmented Visual Contrastive Decoding</div>
<div class="meta-line">Authors: Eun Woo Im, Muhammad Kashif Ali, Vivek Gupta</div>
<div class="meta-line">First: 2025-10-15T09:03:34+00:00 · Latest: 2025-10-15T09:03:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13315v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13315v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal
capabilities, but they inherit the tendency to hallucinate from their
underlying language models. While visual contrastive decoding has been proposed
to mitigate this issue, existing methods often apply generic visual
augmentations that disregard the specific context provided by the text query,
limiting their effectiveness. This study introduces a novel training-free
decoding strategy that addresses these limitations, featuring two key
contributions. First, a self-augmentation prompting strategy that leverages the
intrinsic knowledge of the model to dynamically align semantics between the
query and the visual augmentation. Second, an adaptive thresholding algorithm
that adaptively adjusts next token candidate size based on the output sparsity,
utilizing full information from the logit distribution. Extensive experiments
across four LVLMs and seven benchmarks demonstrate that the proposed decoding
significantly enhances factual consistency compared to state-of-the-art
decoding methods. This work highlights the importance of integrating
query-dependent augmentation and entropy-aware decoding for improving effective
generation of LVLMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal capabilities, but they inherit the tendency to hallucinate from their underlying language models.</div>
</details>
</div>
<div class="card">
<div class="title">SafeGuider: Robust and Practical Content Safety Control for   Text-to-Image Models</div>
<div class="meta-line">Authors: Peigui Qi, Kunsheng Tang, Wenbo Zhou, Weiming Zhang, Nenghai Yu, Tianwei Zhang, Qing Guo, Jie Zhang</div>
<div class="meta-line">First: 2025-10-05T10:24:48+00:00 · Latest: 2025-10-15T08:53:37+00:00</div>
<div class="meta-line">Comments: Accepted by ACM CCS 2025, Code is available at [this https
  URL](https://github.com/pgqihere/safeguider)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.05173v3">Abs</a> · <a href="http://arxiv.org/pdf/2510.05173v3">PDF</a> · <a href="https://github.com/pgqihere/safeguider">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image models have shown remarkable capabilities in generating
high-quality images from natural language descriptions. However, these models
are highly vulnerable to adversarial prompts, which can bypass safety measures
and produce harmful content. Despite various defensive strategies, achieving
robustness against attacks while maintaining practical utility in real-world
applications remains a significant challenge. To address this issue, we first
conduct an empirical study of the text encoder in the Stable Diffusion (SD)
model, which is a widely used and representative text-to-image model. Our
findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting
distinct distributional patterns between benign and adversarial prompts in its
embedding space. Building on this insight, we introduce SafeGuider, a two-step
framework designed for robust safety control without compromising generation
quality. SafeGuider combines an embedding-level recognition model with a
safety-aware feature erasure beam search algorithm. This integration enables
the framework to maintain high-quality image generation for benign prompts
while ensuring robust defense against both in-domain and out-of-domain attacks.
SafeGuider demonstrates exceptional effectiveness in minimizing attack success
rates, achieving a maximum rate of only 5.48\% across various attack scenarios.
Moreover, instead of refusing to generate or producing black images for unsafe
prompts, SafeGuider generates safe and meaningful images, enhancing its
practical utility. In addition, SafeGuider is not limited to the SD model and
can be effectively applied to other text-to-image models, such as the Flux
model, demonstrating its versatility and adaptability across different
architectures. We hope that SafeGuider can shed some light on the practical
deployment of secure text-to-image systems.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions.</div>
</details>
</div>
<div class="card">
<div class="title">Detecting Distillation Data from Reasoning Models</div>
<div class="meta-line">Authors: Hengxiang Zhang, Hyeong Kyu Choi, Sharon Li, Hongxin Wei</div>
<div class="meta-line">First: 2025-10-06T14:37:02+00:00 · Latest: 2025-10-15T08:23:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.04850v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.04850v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens&#x27;
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reasoning distillation has emerged as an efficient and powerful paradigm for enhancing the reasoning capabilities of large language models.</div>
</details>
</div>
<div class="card">
<div class="title">What &quot;Not&quot; to Detect: Negation-Aware VLMs via Structured Reasoning and   Token Merging</div>
<div class="meta-line">Authors: Inha Kang, Youngsun Lim, Seonho Lee, Jiho Choi, Junsuk Choe, Hyunjung Shim</div>
<div class="meta-line">First: 2025-10-15T07:36:38+00:00 · Latest: 2025-10-15T07:36:38+00:00</div>
<div class="meta-line">Comments: 38 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13232v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art vision-language models (VLMs) suffer from a critical failure
in understanding negation, often referred to as affirmative bias. This
limitation is particularly severe in described object detection (DOD) tasks. To
address this, we propose two primary contributions: (1) a new dataset pipeline
and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a
dataset constructed with a systematic chain-of-thought (CoT) and VQA-based
pipeline to generate high-quality, instance-grounded negation data. Second, we
propose NegToMe, a novel text token merging module that directly tackles the
architectural cause of affirmative bias. NegToMe fundamentally addresses the
structural loss of negation cues in tokenization, grouping them with attributes
into coherent semantic phrases. It maintains correct polarity at the input
level, enabling robust negation understanding even with limited data. For
instance, to prevent a model from treating the fragmented tokens &quot;not&quot; and
&quot;girl&quot; as simply &quot;girl&quot;, NegToMe binds them into a single token whose meaning
is correctly distinguished from that of &quot;girl&quot; alone. This module is integrated
with a parameter-efficient and strategic LoRA fine-tuning approach. Our method
significantly improves performance on challenging negation benchmarks with a
lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval
and demonstrating generalization to SoTA VLMs. This work marks a crucial step
forward in addressing negation understanding for real-world detection
applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">State-of-the-art vision-language models (VLMs) suffer from a critical failure in understanding negation, often referred to as affirmative bias.</div>
</details>
</div>
<div class="card">
<div class="title">Prompt-based Adaptation in Large-scale Vision Models: A Survey</div>
<div class="meta-line">Authors: Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han</div>
<div class="meta-line">First: 2025-10-15T07:14:50+00:00 · Latest: 2025-10-15T07:14:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.13219v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.13219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have
recently emerged as lightweight and effective alternatives to full fine-tuning
for adapting large-scale vision models within the ``pretrain-then-finetune&#x27;&#x27;
paradigm. However, despite rapid progress, their conceptual boundaries remain
blurred, as VP and VPT are frequently used interchangeably in current research,
reflecting a lack of systematic distinction between these techniques and their
respective applications. In this survey, we revisit the designs of VP and VPT
from first principles, and conceptualize them within a unified framework termed
Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing
methods into learnable, generative, and non-learnable prompts, and further
organizes them by injection granularity -- pixel-level and token-level. Beyond
the core methodologies, we examine PA&#x27;s integrations across diverse domains,
including medical imaging, 3D point clouds, and vision-language tasks, as well
as its role in test-time adaptation and trustworthy AI. We also summarize
current benchmarks and identify key challenges and future directions. To the
best of our knowledge, we are the first comprehensive survey dedicated to PA&#x27;s
methodologies and applications in light of their distinct characteristics. Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have recently emerged as lightweight and effective alternatives to full fine-tuning for adapting large-scale vision models within the ``pretrain-then-finetune&#x27;&#x27; paradigm.</div>
</details>
</div>
<div class="card">
<div class="title">Universal Speech Token Learning via Low-Bitrate Neural Codec and   Pretrained Representations</div>
<div class="meta-line">Authors: Xue Jiang, Xiulian Peng, Yuan Zhang, Yan Lu</div>
<div class="meta-line">First: 2025-03-15T12:50:43+00:00 · Latest: 2025-10-15T06:52:30+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Journal of Selected Topics in Signal
  Processing(JSTSP)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.12115v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.12115v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current large speech language models are mainly based on semantic tokens from
discretization of self-supervised learned representations and acoustic tokens
from a neural codec, following a semantic-modeling and acoustic-synthesis
paradigm. However, semantic tokens discard paralinguistic attributes of
speakers that is important for natural spoken communication, while prompt-based
acoustic synthesis from semantic tokens has limits in recovering paralinguistic
details and suffers from robustness issues, especially when there are domain
gaps between the prompt and the target. This paper unifies two types of tokens
and proposes the UniCodec, a universal speech token learning that encapsulates
all semantics of speech, including linguistic and paralinguistic information,
into a compact and semantically-disentangled unified token. Such a unified
token can not only benefit speech language models in understanding with
paralinguistic hints but also help speech generation with high-quality output.
A low-bitrate neural codec is leveraged to learn such disentangled discrete
representations at global and local scales, with knowledge distilled from
self-supervised learned features. Extensive evaluations on multilingual
datasets demonstrate its effectiveness in generating natural, expressive and
long-term consistent output quality with paralinguistic attributes well
preserved in several speech processing tasks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm.</div>
</details>
</div>
<div class="card">
<div class="title">Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of   Alzheimer&#x27;s Disease</div>
<div class="meta-line">Authors: Fangqi Cheng, Surajit Ray, Xiaochen Yang</div>
<div class="meta-line">First: 2025-09-09T11:36:21+00:00 · Latest: 2025-10-15T05:06:34+00:00</div>
<div class="meta-line">Comments: Accepted at MICAD 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.07613v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.07613v3">PDF</a> · <a href="https://github.com/CFQ666312/DEFT-VLM-AD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical vision-language models (Med-VLMs) have shown impressive results in
tasks such as report generation and visual question answering, but they still
face several limitations. Most notably, they underutilize patient metadata and
lack integration of clinical diagnostic knowledge. Moreover, most existing
models are typically trained from scratch or fine-tuned on large-scale 2D
image-text pairs, requiring extensive computational resources, and their
effectiveness on 3D medical imaging is often limited due to the absence of
structural information. To address these gaps, we propose a data-efficient
fine-tuning pipeline to adapt 3D CT-based Med-VLMs for 3D MRI and demonstrate
its application in Alzheimer&#x27;s disease (AD) diagnosis. Our system introduces
two key innovations. First, we convert structured metadata into synthetic
reports, enriching textual input for improved image-text alignment. Second, we
add an auxiliary token trained to predict the mini-mental state examination
(MMSE) score, a widely used clinical measure of cognitive function that
correlates with AD severity. This provides additional supervision for
fine-tuning. Applying lightweight prompt tuning to both image and text
modalities, our approach achieves state-of-the-art performance on ADNI with
only 1,504 training MRIs, outperforming methods trained on 27,161 MRIs, and
shows strong zero-shot generalization on OASIS-2 and AIBL. Code is available at
https://github.com/CFQ666312/DEFT-VLM-AD.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Medical vision-language models (Med-VLMs) have shown impressive results in tasks such as report generation and visual question answering, but they still face several limitations.</div>
</details>
</div>
<div class="card">
<div class="title">BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals</div>
<div class="meta-line">Authors: Qinfan Xiao, Ziyun Cui, Chi Zhang, Siqi Chen, Wen Wu, Andrew Thwaites, Alexandra Woolgar, Bowen Zhou, Chao Zhang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-18T14:07:14+00:00 · Latest: 2025-10-15T04:51:35+00:00</div>
<div class="meta-line">Comments: Accepted by the 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.18185v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.18185v3">PDF</a> · <a href="https://github.com/OpenTSLab/BrainOmni">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural
activity non-invasively by capturing electromagnetic fields generated by
dendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit
distinct signal patterns, further complicated by variations in sensor
configurations across modalities and recording devices. Existing approaches
typically rely on separate, modality- and dataset-specific models, which limits
the performance and cross-domain scalability. This paper proposes BrainOmni,
the first brain foundation model that generalises across heterogeneous EEG and
MEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the
first tokenizer that quantises spatiotemporal brain activity into discrete
representations. Central to BrainTokenizer is a novel Sensor Encoder that
encodes sensor properties such as spatial layout, orientation, and type,
enabling compatibility across devices and modalities. Building upon the
discrete representations, BrainOmni learns unified semantic embeddings of brain
signals by self-supervised pretraining. To the best of our knowledge, it is the
first foundation model to support both EEG and MEG signals, as well as the
first to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG
and 656 hours of MEG data are curated and standardised from publicly available
sources for pretraining. Experiments show that BrainOmni outperforms both
existing foundation models and state-of-the-art task-specific models on a range
of downstream tasks. It also demonstrates strong generalisation to unseen EEG
and MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training
yields consistent improvements across both modalities. Code and checkpoints are
publicly available at https://github.com/OpenTSLab/BrainOmni.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural activity non-invasively by capturing electromagnetic fields generated by dendritic currents.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
