<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-11-05 03:15</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251105_0315</div>
    <div class="row"><div class="card">
<div class="title">SimKey: A Semantically Aware Key Module for Watermarking Language Models</div>
<div class="meta-line">Authors: Shingo Kodama, Haya Diwan, Lucas Rosenblatt, R. Teal Witter, Niv Cohen</div>
<div class="meta-line">First: 2025-10-11T20:07:54+00:00 · Latest: 2025-11-03T18:20:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.12828v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.12828v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid spread of text generated by large language models (LLMs) makes it
increasingly difficult to distinguish authentic human writing from machine
output. Watermarking offers a promising solution: model owners can embed an
imperceptible signal into generated text, marking its origin. Most leading
approaches seed an LLM&#x27;s next-token sampling with a pseudo-random key that can
later be recovered to identify the text as machine-generated, while only
minimally altering the model&#x27;s output distribution. However, these methods
suffer from two related issues: (i) watermarks are brittle to simple
surface-level edits such as paraphrasing or reordering; and (ii) adversaries
can append unrelated, potentially harmful text that inherits the watermark,
risking reputational damage to model owners. To address these issues, we
introduce SimKey, a semantic key module that strengthens watermark robustness
by tying key generation to the meaning of prior context. SimKey uses
locality-sensitive hashing over semantic embeddings to ensure that paraphrased
text yields the same watermark key, while unrelated or semantically shifted
text produces a different one. Integrated with state-of-the-art watermarking
schemes, SimKey improves watermark robustness to paraphrasing and translation
while preventing harmful content from false attribution, establishing
semantic-aware keying as a practical and extensible watermarking direction.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid spread of text generated by large language models (LLMs) makes it increasingly difficult to distinguish authentic human writing from machine output.</div>
</details>
</div>
<div class="card">
<div class="title">ShortV: Efficient Multimodal Large Language Models by Freezing Visual   Tokens in Ineffective Layers</div>
<div class="meta-line">Authors: Qianhao Yuan, Qingyu Zhang, Yanjiang Liu, Jiawei Chen, Yaojie Lu, Hongyu Lin, Jia Zheng, Xianpei Han, Le Sun</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-04-01T07:47:55+00:00 · Latest: 2025-11-03T17:23:02+00:00</div>
<div class="meta-line">Comments: Published as a conference paper at ICCV 2025. Project page:
  https://github.com/icip-cas/ShortV</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.00502v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.00502v2">PDF</a> · <a href="https://github.com/icip-cas/ShortV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) suffer from high computational costs
due to their massive size and the large number of visual tokens. In this paper,
we investigate layer-wise redundancy in MLLMs by introducing a novel metric,
Layer Contribution (LC), which quantifies the impact of a layer&#x27;s
transformations on visual and text tokens, respectively. The calculation of LC
involves measuring the divergence in model output that results from removing
the layer&#x27;s transformations on the specified tokens. Our pilot experiment
reveals that many layers of MLLMs exhibit minimal contribution during the
processing of visual tokens. Motivated by this observation, we propose ShortV,
a training-free method that leverages LC to identify ineffective layers, and
freezes visual token updates in these layers. Experiments show that ShortV can
freeze visual token in approximately 60\% of the MLLM layers, thereby
dramatically reducing computational costs related to updating visual tokens.
For example, it achieves a 50\% reduction in FLOPs on LLaVA-NeXT-13B while
maintaining superior performance. The code will be publicly available at
https://github.com/icip-cas/ShortV</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens.</div>
</details>
</div>
<div class="card">
<div class="title">Task-Oriented Multimodal Token Transmission in Resource-Constrained   Multiuser Networks</div>
<div class="meta-line">Authors: Junhe Zhang, Wanli Ni, Pengwei Wang, Dongyu Wang</div>
<div class="meta-line">First: 2025-05-06T14:17:05+00:00 · Latest: 2025-11-03T13:36:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.07841v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.07841v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the emergence of large model-based agents, widely adopted
transformer-based architectures inevitably produce excessively long token
embeddings for transmission, which may result in high bandwidth overhead,
increased power consumption and latency. In this letter, we propose a
task-oriented multimodal token transmission scheme for efficient multimodal
information fusion and utilization. To improve the efficiency of token
transmission, we design a two-stage training algotithm, including cross-modal
alignment and task-oriented fine-tuning, for large model-based token
communication. Meanwhile, token compression is performed using a sliding window
pooling operation to save communication resources. To balance the trade-off
between latency and model performance caused by compression, we formulate a
weighted-sum optimization problem over latency and validation loss. We jointly
optimizes bandwidth, power allocation, and token length across users by using
an alternating optimization method. Simulation results demonstrate that the
proposed algorithm outperforms the baseline under different bandwidth and power
budgets. Moreover, the two-stage training algorithm achieves higher accuracy
across various signal-to-noise ratios than the method without cross-modal
alignment.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the emergence of large model-based agents, widely adopted transformer-based architectures inevitably produce excessively long token embeddings for transmission, which may result in high bandwidth overhead, increased power consumption and latency.</div>
</details>
</div>
<div class="card">
<div class="title">GroupSHAP-Guided Integration of Financial News Keywords and Technical   Indicators for Stock Price Prediction</div>
<div class="meta-line">Authors: Minjoo Kim, Jinwoong Kim, Sangjin Park</div>
<div class="meta-line">Venue: ICAIF 2025 Workshop</div>
<div class="meta-line">First: 2025-10-27T08:33:18+00:00 · Latest: 2025-11-03T13:06:41+00:00</div>
<div class="meta-line">Comments: 6 pages</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.23112v3">Abs</a> · <a href="http://arxiv.org/pdf/2510.23112v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in finance-specific language models such as FinBERT have
enabled the quantification of public sentiment into index-based measures, yet
compressing diverse linguistic signals into single metrics overlooks contextual
nuances and limits interpretability. To address this limitation, explainable AI
techniques, particularly SHAP (SHapley Additive Explanations), have been
employed to identify influential features. However, SHAP&#x27;s computational cost
grows exponentially with input features, making it impractical for large-scale
text-based financial data. This study introduces a GRU-based forecasting
framework enhanced with GroupSHAP, which quantifies contributions of
semantically related keyword groups rather than individual tokens,
substantially reducing computational burden while preserving interpretability.
We employed FinBERT to embed news articles from 2015 to 2024, clustered them
into coherent semantic groups, and applied GroupSHAP to measure each group&#x27;s
contribution to stock price movements. The resulting group-level SHAP variables
across multiple topics were used as input features for the prediction model.
Empirical results from one-day-ahead forecasting of the S&amp;P 500 index
throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE
and a 40.5% reduction in RMSE compared with benchmark models without the
GroupSHAP mechanism. This research presents the first application of GroupSHAP
in news-driven financial forecasting, showing that grouped sentiment
representations simultaneously enhance interpretability and predictive
performance.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability.</div>
</details>
</div>
<div class="card">
<div class="title">New Encoders for German Trained from Scratch: Comparing ModernGBERT with   Converted LLM2Vec Models</div>
<div class="meta-line">Authors: Julia Wunderle, Anton Ehrmanntraut, Jan Pfister, Fotis Jannidis, Andreas Hotho</div>
<div class="meta-line">First: 2025-05-19T14:07:20+00:00 · Latest: 2025-11-03T12:45:10+00:00</div>
<div class="meta-line">Comments: under review @LREC</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.13136v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.13136v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Encoders remain essential for efficient German NLP and NLU scenarios despite
the rise of decoder-only LLMs. This work studies two routes to high-quality
German encoders under identical data and training constraints: 1) training from
scratch and 2) converting decoders via LLM2Vec. We introduce two resources:
ModernGBERT (134M, 1B), fully transparent German encoders in the ModernBERT
style, and LL\&quot;aMmleinVec (120M, 1B, 7B), decoder-to-encoder conversions
trained with masked next-token prediction, both undergoing a context extension
to 8.192 tokens.
  Across SuperGLEBer, ModernGBERT 1B sets a new state of the art (avg 0.808),
surpassing GBERT Large (+4%) and the seven-times larger converted 7B model
(0.787). On German MTEB after supervised fine-tuning, ModernGBERT 1B (0.551)
approaches the converted 7B model (0.557).
  We release all models, checkpoints, datasets, and full training records, and
introduce an encoder-adapted QA-NIAH evaluation. All in all, our results
provide actionable guidance: when parameter efficiency and latency matter,
from-scratch encoders dominate. When a pre-trained decoder exists and compute
is a limited, conversion offers an effective alternative. ModernGBERT and
LL\&quot;aMmleinVec, including all code, data and intermediary checkpoints are
published under a research-only RAIL license.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Encoders remain essential for efficient German NLP and NLU scenarios despite the rise of decoder-only LLMs.</div>
</details>
</div>
<div class="card">
<div class="title">Diversity-Aware Policy Optimization for Large Language Model Reasoning</div>
<div class="meta-line">Authors: Jian Yao, Ran Cheng, Xingyu Wu, Jibin Wu, Kay Chen Tan</div>
<div class="meta-line">First: 2025-05-29T13:27:44+00:00 · Latest: 2025-11-03T12:40:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.23433v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.23433v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM&#x27;s reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek R1, which has inspired a surge of research into data quality and reinforcement learning (RL) algorithms.</div>
</details>
</div>
<div class="card">
<div class="title">Contextual Tokenization for Graph Inverted Indices</div>
<div class="meta-line">Authors: Pritish Chakraborty, Indradyumna Roy, Soumen Chakrabarti, Abir De</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-26T01:38:16+00:00 · Latest: 2025-11-03T10:11:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22479v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.22479v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieving graphs from a large corpus, that contain a subgraph isomorphic to
a given query graph, is a core operation in many real-world applications. While
recent multi-vector graph representations and scores based on set alignment and
containment can provide accurate subgraph isomorphism tests, their use in
retrieval remains limited by their need to score corpus graphs exhaustively. We
introduce CORGII (Contextual Representation of Graphs for Inverted Indexing), a
graph indexing framework in which, starting with a contextual dense graph
representation, a differentiable discretization module computes sparse binary
codes over a learned latent vocabulary. This text document-like representation
allows us to leverage classic, highly optimized inverted indices, while
supporting soft (vector) set containment scores. Pushing this paradigm further,
we replace the classical, fixed impact weight of a `token&#x27; on a graph (such as
TFIDF or BM25) with a data-driven, trainable impact weight. Finally, we explore
token expansion to support multi-probing the index for smoother
accuracy-efficiency tradeoffs. To our knowledge, CORGII is the first indexer of
dense graph representations using discrete tokens mapping to efficient inverted
lists. Extensive experiments show that CORGII provides better trade-offs
between accuracy and efficiency, compared to several baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Retrieving graphs from a large corpus, that contain a subgraph isomorphic to a given query graph, is a core operation in many real-world applications.</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Effective Tokens with Video Anomaly in Large Language Models</div>
<div class="meta-line">Authors: Yingxian Chen, Jiahui Liu, Ruidi Fan, Yanwei Li, Chirui Chang, Shizhen Zhao, Wilton W. T. Fok, Xiaojuan Qi, Yik-Chung Wu</div>
<div class="meta-line">First: 2025-08-08T14:30:05+00:00 · Latest: 2025-11-03T06:40:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.06350v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.06350v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Understanding abnormal events in videos is a vital and challenging task that has garnered significant attention in a wide range of applications.</div>
</details>
</div>
<div class="card">
<div class="title">Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion   Models</div>
<div class="meta-line">Authors: Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng</div>
<div class="meta-line">First: 2025-10-21T09:30:45+00:00 · Latest: 2025-11-03T03:48:04+00:00</div>
<div class="meta-line">Comments: v2 note: Corrected numerical values in Table 2 and Figure 4 due to a
  minor calculation error in v1. The overall conclusions remain unchanged. Code
  and models available at: https://github.com/tianciB/VFM-VAE</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.18457v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.18457v2">PDF</a> · <a href="https://github.com/tianciB/VFM-VAE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance of Latent Diffusion Models (LDMs) is critically dependent on
the quality of their visual tokenizer. While recent works have explored
incorporating Vision Foundation Models (VFMs) via distillation, we identify a
fundamental flaw in this approach: it inevitably weakens the robustness of
alignment with the original VFM, causing the aligned latents to deviate
semantically under distribution shifts. In this paper, we bypass distillation
by proposing a more direct approach: Vision Foundation Model Variational
Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM&#x27;s
semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE
decoder with Multi-Scale Latent Fusion and Progressive Resolution
Reconstruction blocks, enabling high-quality reconstruction from spatially
coarse VFM features. Furthermore, we provide a comprehensive analysis of
representation dynamics during diffusion training, introducing the proposed
SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows
us to develop a joint tokenizer-diffusion alignment strategy that dramatically
accelerates convergence. Our innovations in tokenizer design and training
strategy lead to superior performance and efficiency: our system reaches a gFID
(w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).
With continued training to 640 epochs, it further attains a gFID (w/o CFG) of
1.62, establishing direct VFM integration as a superior paradigm for LDMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The performance of Latent Diffusion Models (LDMs) is critically dependent on the quality of their visual tokenizer.</div>
</details>
</div>
<div class="card">
<div class="title">SmartFreeEdit: Mask-Free Spatial-Aware Image Editing with Complex   Instruction Understanding</div>
<div class="meta-line">Authors: Qianqian Sun, Jixiang Luo, Dell Zhang, Xuelong Li</div>
<div class="meta-line">First: 2025-04-17T07:17:49+00:00 · Latest: 2025-11-03T03:26:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.12704v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.12704v2">PDF</a> · <a href="https://github.com/smileformylove/SmartFreeEdit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in image editing have utilized large-scale multimodal
models to enable intuitive, natural instruction-driven interactions. However,
conventional methods still face significant challenges, particularly in spatial
reasoning, precise region segmentation, and maintaining semantic consistency,
especially in complex scenes. To overcome these challenges, we introduce
SmartFreeEdit, a novel end-to-end framework that integrates a multimodal large
language model (MLLM) with a hypergraph-enhanced inpainting architecture,
enabling precise, mask-free image editing guided exclusively by natural
language instructions. The key innovations of SmartFreeEdit include:(1)the
introduction of region aware tokens and a mask embedding paradigm that enhance
the spatial understanding of complex scenes;(2) a reasoning segmentation
pipeline designed to optimize the generation of editing masks based on natural
language instructions;and (3) a hypergraph-augmented inpainting module that
ensures the preservation of both structural integrity and semantic coherence
during complex edits, overcoming the limitations of local-based image
generation. Extensive experiments on the Reason-Edit benchmark demonstrate that
SmartFreeEdit surpasses current state-of-the-art methods across multiple
evaluation metrics, including segmentation accuracy, instruction adherence, and
visual quality preservation, while addressing the issue of local information
focus and improving global consistency in the edited image. Our project will be
available at https://github.com/smileformylove/SmartFreeEdit.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advancements in image editing have utilized large-scale multimodal models to enable intuitive, natural instruction-driven interactions.</div>
</details>
</div>
<div class="card">
<div class="title">Mapping Overlaps in Benchmarks through Perplexity in the Wild</div>
<div class="meta-line">Authors: Siyang Wu, Honglin Bao, Sida Li, Ari Holtzman, James A. Evans</div>
<div class="meta-line">First: 2025-09-27T20:23:13+00:00 · Latest: 2025-11-03T02:18:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.23488v3">Abs</a> · <a href="http://arxiv.org/pdf/2509.23488v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop signatures of capacity familiarity to characterize large language
model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures
probe the capacity required for benchmark performance. We formally define them
as a set of salient tokens drawn from in-the-wild, naturally authored corpora,
where LLM token perplexity, reflecting more or less pre-training exposure,
becomes highly predictive of LLM benchmark performance. Through a large-scale
meta-evaluation, we extract benchmark signatures via stepwise forward selection
with linear regressions across 32 LLMs and 88 benchmarks spanning diverse
knowledge, coding, logic, instruction following, math, language, reasoning, and
world modeling. Our analysis situates signatures in relation to both the
semantic similarity of benchmark questions and the correlation of model
performance. While performance overlaps are universally high and semantic
overlaps remain confined to a narrow mid-range, benchmark signatures prove
highly informative in capturing variation, overlap, and divergence. We observe
overlap in knowledge and reasoning subtasks, whereas multilingual and cultural
benchmarks exhibit less similarity, even compared to cross-task overlap.
Notably, performance-level results are strongly influenced by
benchmark-orthogonal factors such as question format, highlighting limitations
in LLM generalization, the conflation of performance with ability, and issues
inherent in current mainstream benchmark agreement studies. Benchmark
signatures, however, remain robust to such effects. Ultimately, we identify
cross-functional overlaps across logic, math, language, instruction following,
and world modeling, with coding emerging as the least overlapping domain.
Together, these findings provide mechanistic insights into benchmark validity
and LLM sensitivities, and sketch the underlying landscape of interconnected
LLM capabilities.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We develop signatures of capacity familiarity to characterize large language model (LLM) benchmarks and their meaningful overlaps.</div>
</details>
</div>
<div class="card">
<div class="title">OneCast: Structured Decomposition and Modular Generation for   Cross-Domain Time Series Forecasting</div>
<div class="meta-line">Authors: Tingyue Pan, Mingyue Cheng, Shilong Zhang, Zhiding Liu, Xiaoyu Tao, Yucong Luo, Jintao Zhang, Qi Liu</div>
<div class="meta-line">First: 2025-10-28T03:23:53+00:00 · Latest: 2025-11-03T01:49:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.24028v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.24028v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Cross-domain time series forecasting is a valuable task in various web applications.</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering Representation Bias for Investment Decisions in Open-Source   Large Language Models</div>
<div class="meta-line">Authors: Fabrizio Dimino, Krati Saxena, Bhaskarjit Sarmah, Stefano Pasquali</div>
<div class="meta-line">First: 2025-10-07T09:10:13+00:00 · Latest: 2025-11-03T01:00:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.05702v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.05702v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are increasingly adopted in financial applications to
support investment workflows. However, prior studies have seldom examined how
these models reflect biases related to firm size, sector, or financial
characteristics, which can significantly impact decision-making. This paper
addresses this gap by focusing on representation bias in open-source Qwen
models. We propose a balanced round-robin prompting method over approximately
150 U.S. equities, applying constrained decoding and token-logit aggregation to
derive firm-level confidence scores across financial contexts. Using
statistical tests and variance analysis, we find that firm size and valuation
consistently increase model confidence, while risk factors tend to decrease it.
Confidence varies significantly across sectors, with the Technology sector
showing the greatest variability. When models are prompted for specific
financial categories, their confidence rankings best align with fundamental
data, moderately with technical signals, and least with growth indicators.
These results highlight representation bias in Qwen models and motivate
sector-aware calibration and category-conditioned evaluation protocols for safe
and fair financial LLM deployment.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models are increasingly adopted in financial applications to support investment workflows.</div>
</details>
</div>
<div class="card">
<div class="title">Debiasing LLMs by Masking Unfairness-Driving Attention Heads</div>
<div class="meta-line">Authors: Tingxu Han, Wei Song, Ziqi Ding, Ziming Li, Chunrong Fang, Yuekang Li, Dongfang Liu, Zhenyu Chen, Zhenting Wang</div>
<div class="meta-line">First: 2025-10-11T09:48:31+00:00 · Latest: 2025-11-02T16:45:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.10142v3">Abs</a> · <a href="http://arxiv.org/pdf/2510.10142v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) increasingly mediate decisions in domains where
unfair treatment of demographic groups is unacceptable. Existing work probes
when biased outputs appear, but gives little insight into the mechanisms that
generate them, leaving existing mitigations largely fragile. In this paper, we
conduct a systematic investigation LLM unfairness and propose DiffHeads, a
lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)
prompting to Chain-of-Thought (CoT) prompting across eight representative open-
and closed-source LLMs. DA will trigger the nature bias part of LLM and improve
measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.
Next, we define a token-to-head contribution score that traces each token&#x27;s
influence back to individual attention heads. This reveals a small cluster of
bias heads that activate under DA but stay largely dormant with CoT, providing
the first causal link between prompting strategy and bias emergence. Finally,
building on this insight, we propose DiffHeads that identifies bias heads
through differential activation analysis between DA and CoT, and selectively
masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under
DA and CoT, respectively, without harming model utility.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) increasingly mediate decisions in domains where unfair treatment of demographic groups is unacceptable.</div>
</details>
</div>
<div class="card">
<div class="title">Exploring the limits of strong membership inference attacks on large   language models</div>
<div class="meta-line">Authors: Jamie Hayes, Ilia Shumailov, Christopher A. Choquette-Choo, Matthew Jagielski, George Kaissis, Milad Nasr, Sahra Ghalebikesabi, Meenatchi Sundaram Mutu Selva Annamalai, Niloofar Mireshghallah, Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye, Katherine Lee, Franziska Boenisch, Adam Dziedzic, A. Feder Cooper</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-24T16:23:43+00:00 · Latest: 2025-11-02T16:45:19+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.18773v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.18773v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art membership inference attacks (MIAs) typically require
training many reference models, making it difficult to scale these attacks to
large pre-trained language models (LLMs). As a result, prior research has
either relied on weaker attacks that avoid training references (e.g.,
fine-tuning attacks), or on stronger attacks applied to small models and
datasets. However, weaker attacks have been shown to be brittle and insights
from strong attacks in simplified settings do not translate to today&#x27;s LLMs.
These challenges prompt an important question: are the limitations observed in
prior work due to attack design choices, or are MIAs fundamentally ineffective
on LLMs? We address this question by scaling LiRA--one of the strongest
MIAs--to GPT-2 architectures ranging from 10M to 1B parameters, training
references on over 20B tokens from the C4 dataset. Our results advance the
understanding of MIAs on LLMs in four key ways. While (1) strong MIAs can
succeed on pre-trained LLMs, (2) their effectiveness, remains limited (e.g.,
AUC&lt;0.7) in practical settings. (3) Even when strong MIAs achieve
better-than-random AUC, aggregate metrics can conceal substantial per-sample
MIA decision instability: due to training randomness, many decisions are so
unstable that they are statistically indistinguishable from a coin flip.
Finally, (4) the relationship between MIA success and related LLM privacy
metrics is not as straightforward as prior work has suggested.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Token Choice for Code Watermarking: An RL Approach</div>
<div class="meta-line">Authors: Zhimeng Guo, Huaisheng Zhu, Siyuan Xu, Hangfan Zhang, Teng Xiao, Minhao Cheng</div>
<div class="meta-line">First: 2025-08-16T06:11:29+00:00 · Latest: 2025-11-02T15:47:22+00:00</div>
<div class="meta-line">Comments: 18 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.11925v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.11925v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Protecting intellectual property on LLM-generated code necessitates effective
watermarking systems that can operate within code&#x27;s highly structured,
syntactically constrained nature. In this work, we introduce CodeTracer, an
innovative adaptive code watermarking framework underpinned by a novel
reinforcement learning training paradigm. At its core, CodeTracer features a
policy-driven approach that utilizes a parameterized model to intelligently
bias token choices during next-token prediction. This strategy ensures that
embedded watermarks maintain code functionality while exhibiting subtle yet
statistically detectable deviations from typical token distributions. To
facilitate policy learning, we devise a comprehensive reward system that
seamlessly integrates execution feedback with watermark embedding signals,
balancing process-level and outcome-level rewards. Additionally, we employ
Gumbel Top-k reparameterization to enable gradient-based optimization of
discrete watermarking decisions. Extensive comparative evaluations demonstrate
CodeTracer&#x27;s significant superiority over state-of-the-art baselines in both
watermark detectability and the preservation of generated code&#x27;s functionality.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Protecting intellectual property on LLM-generated code necessitates effective watermarking systems that can operate within code&#x27;s highly structured, syntactically constrained nature.</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating Attention Sinks and Massive Activations in Audio-Visual   Speech Recognition with LLMs</div>
<div class="meta-line">Authors: Anand, Umberto Cappellazzo, Stavros Petridis, Maja Pantic</div>
<div class="meta-line">First: 2025-10-26T09:44:20+00:00 · Latest: 2025-11-02T11:33:56+00:00</div>
<div class="meta-line">Comments: The code is available at
  https://github.com/umbertocappellazzo/Llama-AVSR</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.22603v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.22603v2">PDF</a> · <a href="https://github.com/umbertocappellazzo/Llama-AVSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have recently advanced auditory speech
recognition (ASR), visual speech recognition (VSR), and audio-visual speech
recognition (AVSR). However, understanding of their internal dynamics under
fine-tuning remains limited. In natural language processing, recent work has
revealed attention sinks, tokens that attract disproportionately high
attention, and associated massive activations in which some features of sink
tokens exhibit huge activation in LLMs. In this work, we are the first to study
these phenomena in multimodal speech recognition. Through a detailed analysis
of audio-visual LLMs, we identify attention sinks and massive activations not
only at the BOS token but also at intermediate low-semantic tokens across ASR,
VSR, and AVSR. We show that massive activations originate in the MLP layers and
correspond to fixed feature indices across all sink tokens. We further show
that intermediate sink tokens exhibit high cosine similarity to the BOS token,
thereby amplifying attention and activation. Building on these insights, we
introduce a simple decorrelation loss that reduces cosine similarity between
BOS and other tokens, effectively mitigating intermediate sinks and massive
activations. Furthermore, our method improves word error rate (WER) under high
audio-visual feature downsampling while remaining stable at lower downsampling
rates.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) have recently advanced auditory speech recognition (ASR), visual speech recognition (VSR), and audio-visual speech recognition (AVSR).</div>
</details>
</div>
<div class="card">
<div class="title">Medical Hallucinations in Foundation Models and Their Impact on   Healthcare</div>
<div class="meta-line">Authors: Yubin Kim, Hyewon Jeong, Shan Chen, Shuyue Stella Li, Chanwoo Park, Mingyu Lu, Kumail Alhamoud, Jimin Mun, Cristina Grau, Minseok Jung, Rodrigo Gameiro, Lizhou Fan, Eugene Park, Tristan Lin, Joonsik Yoon, Wonjin Yoon, Maarten Sap, Yulia Tsvetkov, Paul Liang, Xuhai Xu, Xin Liu, Chunjong Park, Hyeonhoon Lee, Hae Won Park, Daniel McDuff, Samir Tulebaev, Cynthia Breazeal</div>
<div class="meta-line">First: 2025-02-26T02:30:44+00:00 · Latest: 2025-11-02T11:26:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.05777v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.05777v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucinations in foundation models arise from autoregressive training
objectives that prioritize token-likelihood optimization over epistemic
accuracy, fostering overconfidence and poorly calibrated uncertainty. We define
medical hallucination as any model-generated output that is factually
incorrect, logically inconsistent, or unsupported by authoritative clinical
evidence in ways that could alter clinical decisions. We evaluated 11
foundation models (7 general-purpose, 4 medical-specialized) across seven
medical hallucination tasks spanning medical reasoning and biomedical
information retrieval. General-purpose models achieved significantly higher
proportions of hallucination-free responses than medical-specialized models
(median: 76.6% vs 51.3%, difference = 25.2%, 95% CI: 18.7-31.3%, Mann-Whitney U
= 27.0, p = 0.012, rank-biserial r = -0.64). Top-performing models such as
Gemini-2.5 Pro exceeded 97% accuracy when augmented with chain-of-thought
prompting (base: 87.6%), while medical-specialized models like MedGemma ranged
from 28.6-61.9% despite explicit training on medical corpora. Chain-of-thought
reasoning significantly reduced hallucinations in 86.4% of tested comparisons
after FDR correction (q &lt; 0.05), demonstrating that explicit reasoning traces
enable self-verification and error detection. Physician audits confirmed that
64-72% of residual hallucinations stemmed from causal or temporal reasoning
failures rather than knowledge gaps. A global survey of clinicians (n = 70)
validated real-world impact: 91.8% had encountered medical hallucinations, and
84.7% considered them capable of causing patient harm. The underperformance of
medical-specialized models despite domain training indicates that safety
emerges from sophisticated reasoning capabilities and broad knowledge
integration developed during large-scale pre-training, not from narrow
optimization.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Hallucinations in foundation models arise from autoregressive training objectives that prioritize token-likelihood optimization over epistemic accuracy, fostering overconfidence and poorly calibrated uncertainty.</div>
</details>
</div>
<div class="card">
<div class="title">REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning</div>
<div class="meta-line">Authors: Sungho Jeon, Xinyue Ma, Kwang In Kim, Myeongjae Jeon</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2024-06-07T09:17:33+00:00 · Latest: 2025-11-02T08:57:42+00:00</div>
<div class="meta-line">Comments: accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2406.04772v5">Abs</a> · <a href="http://arxiv.org/pdf/2406.04772v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent rehearsal-free continual learning (CL) methods guided by prompts
achieve strong performance on vision tasks with non-stationary data but remain
resource-intensive, hindering real-world edge deployment. We introduce
resource-efficient prompting (REP), which improves the computational and memory
efficiency of prompt-based rehearsal-free continual learning methods while
minimizing accuracy trade-offs. Our approach employs swift prompt selection to
refine input data using a carefully provisioned model and introduces adaptive
token merging (AToM) and adaptive layer dropping (ALD) for efficient prompt
updates. AToM and ALD selectively skip data and model layers while preserving
task-specific features during the learning of new tasks. Extensive experiments
on multiple image classification datasets demonstrate REP&#x27;s superior resource
efficiency over state-of-the-art rehearsal-free CL methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent rehearsal-free continual learning (CL) methods guided by prompts achieve strong performance on vision tasks with non-stationary data but remain resource-intensive, hindering real-world edge deployment.</div>
</details>
</div>
<div class="card">
<div class="title">Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term   Memory Access</div>
<div class="meta-line">Authors: Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-04-23T15:15:06+00:00 · Latest: 2025-11-02T04:55:05+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.16795v2">Abs</a> · <a href="http://arxiv.org/pdf/2504.16795v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A key advantage of Recurrent Neural Networks (RNNs) over Transformers is
their linear computational and space complexity enables faster training and
inference for long sequences. However, RNNs are fundamentally unable to
randomly access historical context, and simply integrating attention mechanisms
may undermine their efficiency advantages. To overcome this limitation, we
propose Hierarchical Sparse Attention (HSA), a novel attention mechanism that
enhances RNNs with long-range random access flexibility while preserving their
merits in efficiency and length generalization. HSA divides inputs into chunks,
selects the top-$k$ chunks and hierarchically aggregates information. The core
innovation lies in learning token-to-chunk relevance based on fine-grained
token-level information inside each chunk. This approach enhances the precision
of chunk selection across both in-domain and out-of-domain context lengths. To
make HSA efficient, we further introduce a hardware-aligned kernel design. By
combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy
in passkey retrieval across 64 million contexts despite pre-training on only
4K-length contexts, and significant improvements on various downstream tasks,
with nearly constant memory footprint. These results show RAMba&#x27;s huge
potential in long-context modeling.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Autoregression: An Empirical Study of Diffusion Large Language   Models for Code Generation</div>
<div class="meta-line">Authors: Chengze Li, Yitong Zhang, Jia Li, Liyi Cai, Ge Li</div>
<div class="meta-line">First: 2025-09-14T12:51:06+00:00 · Latest: 2025-11-02T02:15:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.11252v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.11252v2">PDF</a> · <a href="https://github.com/zhangyitonggg/dllm4code">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLMs have become the mainstream approaches to code generation.</div>
</details>
</div>
<div class="card">
<div class="title">Free Draft-and-Verification: Toward Lossless Parallel Decoding for   Diffusion Large Language Models</div>
<div class="meta-line">Authors: Shutong Wu, Jiawei Zhang</div>
<div class="meta-line">First: 2025-09-30T21:28:04+00:00 · Latest: 2025-11-01T23:45:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.00294v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.00294v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of
language modeling beyond autoregressive next-token prediction. Thanks to their
bidirectional attention mechanism, DLLMs are more capable of capturing the
connection of context, and thus show unique advantages in challenges like the
famous &quot;reversal curse&quot; or learning under data-constrained scenarios. In
addition, taking advantage of their inherent modeling foundations, DLLMs have
the great potential of efficient inference with parallel decoding algorithms,
which enable multi-token prediction per step. However, the high generation
quality often requires the number of decoding steps equal to the sequence
length, which performs a one-token-per-step decoding, and existing parallel
decoding algorithms, which yield suboptimal decoding paths, bring inference
speedup at the cost of non-negligible performance degradation. To overcome this
challenge, we introduce Free Draft-and-Verification (FreeDave), a novel fast
decoding algorithm tailored for DLLMs that achieves lossless parallel decoding
without any model modification or extra modules. Specifically, we propose an
algorithm of parallel-decoded candidate generation and verification, which is
theoretically guaranteed to use the fewest model forward calls to reproduce the
same sequence generated by static decoding when enough computation and memory
budget is provided. By extensive evaluations on math reasoning and code
generation benchmarks across different DLLMs, FreeDave is proven to boost the
inference throughput up to $3.78\times$ without performance degradation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction.</div>
</details>
</div>
<div class="card">
<div class="title">Tricks and Plug-ins for Gradient Boosting with Transformers</div>
<div class="meta-line">Authors: Biyi Fang, Truong Vo, Jean Utke, Diego Klabjan</div>
<div class="meta-line">First: 2025-08-04T21:54:16+00:00 · Latest: 2025-11-01T21:05:07+00:00</div>
<div class="meta-line">Comments: Update the title of the pdf file only. The old version has a
  different title to the arxiv abstract. arXiv admin note: substantial text
  overlap with arXiv:2203.00761</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.02924v4">Abs</a> · <a href="http://arxiv.org/pdf/2508.02924v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer architectures dominate modern NLP but often demand heavy
computational resources and intricate hyperparameter tuning. To mitigate these
challenges, we propose a novel framework, BoostTransformer, that augments
transformers with boosting principles through subgrid token selection and
importance-weighted sampling. Our method incorporates a least square boosting
objective directly into the transformer pipeline, enabling more efficient
training and improved performance. Across multiple fine-grained text
classification benchmarks, BoostTransformer demonstrates both faster
convergence and higher accuracy, surpassing standard transformers while
minimizing architectural search overhead.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformer architectures dominate modern NLP but often demand heavy computational resources and intricate hyperparameter tuning.</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning by Superposition: A Theoretical Perspective on Chain of   Continuous Thought</div>
<div class="meta-line">Authors: Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-18T18:36:53+00:00 · Latest: 2025-11-01T18:48:10+00:00</div>
<div class="meta-line">Comments: 26 pages, 7 figures, NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.12514v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.12514v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated remarkable performance in many
applications, including challenging reasoning problems via chain-of-thoughts
(CoTs) techniques that generate ``thinking tokens&#x27;&#x27; before answering the
questions. While existing theoretical works demonstrate that CoTs with discrete
tokens boost the capability of LLMs, recent work on continuous CoTs lacks a
theoretical understanding of why it outperforms discrete counterparts in
various reasoning tasks such as directed graph reachability, a fundamental
graph reasoning problem that includes many practical domain applications as
special cases. In this paper, we prove that a two-layer transformer with $D$
steps of continuous CoTs can solve the directed graph reachability problem,
where $D$ is the diameter of the graph, while the best known result of
constant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps
where $n$ is the number of vertices ($D&lt;n$). In our construction, each
continuous thought vector is a superposition state that encodes multiple search
frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while
discrete CoTs must choose a single path sampled from the superposition state,
which leads to sequential search that requires many more steps and may be
trapped into local solutions. We also performed extensive experiments to verify
that our theoretical construction aligns well with the empirical solution
obtained via training dynamics. Notably, encoding of multiple search frontiers
as a superposition state automatically emerges in training continuous CoTs,
without explicit supervision to guide the model to explore multiple paths
simultaneously.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated remarkable performance in many applications, including challenging reasoning problems via chain-of-thoughts (CoTs) techniques that generate ``thinking tokens&#x27;&#x27; before answering the questions.</div>
</details>
</div>
<div class="card">
<div class="title">REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders</div>
<div class="meta-line">Authors: Savya Khosla, Sethuraman TV, Barnett Lee, Alexander Schwing, Derek Hoiem</div>
<div class="meta-line">First: 2025-05-23T17:59:33+00:00 · Latest: 2025-11-01T16:47:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.18153v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.18153v2">PDF</a> · <a href="https://github.com/savya08/REN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the Region Encoder Network (REN), a fast and effective model for
generating region-based image representations using point prompts. Recent
methods combine class-agnostic segmenters (e.g., SAM) with patch-based image
encoders (e.g., DINO) to produce compact and effective region representations,
but they suffer from high computational cost due to the segmentation step. REN
bypasses this bottleneck using a lightweight module that directly generates
region tokens, enabling 60x faster token generation with 35x less memory, while
also improving token quality. It uses a few cross-attention blocks that take
point prompts as queries and features from a patch-based image encoder as keys
and values to produce region tokens that correspond to the prompted objects. We
train REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that
it can be extended to other encoders without dedicated training. We evaluate
REN on semantic segmentation and retrieval tasks, where it consistently
outperforms the original encoders in both performance and compactness, and
matches or exceeds SAM-based region methods while being significantly faster.
Notably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D
benchmark and outperforms proprietary LMMs on Visual Haystacks&#x27; single-needle
challenge. Code and models are available at: https://github.com/savya08/REN.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce the Region Encoder Network (REN), a fast and effective model for generating region-based image representations using point prompts.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251104_0314.html">20251104_0314</a>
<a href="archive/20251103_0313.html">20251103_0313</a>
<a href="archive/20251102_0313.html">20251102_0313</a>
<a href="archive/20251101_0314.html">20251101_0314</a>
<a href="archive/20251031_0314.html">20251031_0314</a>
<a href="archive/20251030_0317.html">20251030_0317</a>
<a href="archive/20251029_0315.html">20251029_0315</a>
<a href="archive/20251028_0316.html">20251028_0316</a>
<a href="archive/20251027_0314.html">20251027_0314</a>
<a href="archive/20251026_0314.html">20251026_0314</a>
<a href="archive/20251025_0313.html">20251025_0313</a>
<a href="archive/20251024_0315.html">20251024_0315</a>
<a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
