<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-24 03:15</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251024_0315</div>
    <div class="row"><div class="card">
<div class="title">Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in   LLM</div>
<div class="meta-line">Authors: Xiaoyu Wu, Yifei Pang, Terrance Liu, Zhiwei Steven Wu</div>
<div class="meta-line">Venue: Neurips 2025</div>
<div class="meta-line">First: 2025-05-30T09:09:33+00:00 · Latest: 2025-10-22T17:51:21+00:00</div>
<div class="meta-line">Comments: Accepted by Neurips 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.24379v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.24379v3">PDF</a> · <a href="https://github.com/Nicholas0228/unlearned_data_extraction_llm">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are typically trained on datasets collected from the
web, which may inadvertently contain harmful or sensitive personal information.
To address growing privacy concerns, unlearning methods have been proposed to
remove the influence of specific data from trained models. Of these, exact
unlearning -- which retrains the model from scratch without the target data --
is widely regarded the gold standard for mitigating privacy risks in
deployment. In this paper, we revisit this assumption in a practical deployment
setting where both the pre- and post-unlearning logits API are exposed, such as
in open-weight scenarios. Targeting this setting, we introduce a novel data
extraction attack that leverages signals from the pre-unlearning model to guide
the post-unlearning model, uncovering patterns that reflect the removed data
distribution. Combining model guidance with a token filtering strategy, our
attack significantly improves extraction success rates -- doubling performance
in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.
Furthermore, we demonstrate our attack&#x27;s effectiveness on a simulated medical
diagnosis dataset to highlight real-world privacy risks associated with exact
unlearning. In light of our findings, which suggest that unlearning may, in a
contradictory way, increase the risk of privacy leakage during real-world
deployments, we advocate for evaluation of unlearning methods to consider
broader threat models that account not only for post-unlearning models but also
for adversarial access to prior checkpoints. Code is publicly available at:
https://github.com/Nicholas0228/unlearned_data_extraction_llm.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information.</div>
</details>
</div>
<div class="card">
<div class="title">Hubble: a Model Suite to Advance the Study of LLM Memorization</div>
<div class="meta-line">Authors: Johnny Tian-Zheng Wei, Ameya Godbole, Mohammad Aflah Khan, Ryan Wang, Xiaoyuan Zhu, James Flemings, Nitya Kashyap, Krishna P. Gummadi, Willie Neiswanger, Robin Jia</div>
<div class="meta-line">First: 2025-10-22T17:48:23+00:00 · Latest: 2025-10-22T17:48:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19811v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19811v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Hubble, a suite of fully open-source large language models (LLMs)
for the scientific study of LLM memorization. Hubble models come in standard
and perturbed variants: standard models are pretrained on a large English
corpus, and perturbed models are trained in the same way but with controlled
insertion of text (e.g., book passages, biographies, and test sets) designed to
emulate key memorization risks. Our core release includes 8 models -- standard
and perturbed models with 1B or 8B parameters, pretrained on 100B or 500B
tokens -- establishing that memorization risks are determined by the frequency
of sensitive data relative to size of the training corpus (i.e., a password
appearing once in a smaller corpus is memorized better than the same password
in a larger corpus). Our release also includes 6 perturbed models with text
inserted at different pretraining phases, showing that sensitive data without
continued exposure can be forgotten. These findings suggest two best practices
for addressing memorization risks: to dilute sensitive data by increasing the
size of the training corpus, and to order sensitive data to appear earlier in
training. Beyond these general empirical findings, Hubble enables a broad range
of memorization research; for example, analyzing the biographies reveals how
readily different types of private information are memorized. We also
demonstrate that the randomized insertions in Hubble make it an ideal testbed
for membership inference and machine unlearning, and invite the community to
further explore, benchmark, and build upon our work.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Hubble, a suite of fully open-source large language models (LLMs) for the scientific study of LLM memorization.</div>
</details>
</div>
<div class="card">
<div class="title">The Feasibility of Training Sovereign Language Models in the Global   South: A Study of Brazil and Mexico</div>
<div class="meta-line">Authors: Sandra Malagon, Monica A. Ulloa Ruiz, Tatiana Elizabeth Sandoval Plaza, Gabriel Rafael Rosario Bolívar, Valentina García Mesa, Ivanna Alvarado Morales</div>
<div class="meta-line">First: 2025-10-22T17:37:46+00:00 · Latest: 2025-10-22T17:37:46+00:00</div>
<div class="meta-line">Comments: 11 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19801v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19801v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid escalation of computational requirements for training large-scale
language models has reinforced structural asymmetries between high-capacity
jurisdictions and countries in the Global South. This paper examines the
technical and fiscal feasibility of sovereign-scale language model training in
Brazil and Mexico under conditions of constrained hardware access, energy
availability, and fiscal ceilings. Using a dual-axis design that varies
accelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150
days), we estimate compute demand, energy consumption, capital expenditures,
and regulatory compatibility for the training of a 10-trillion-token model. Our
findings show that while all configurations remain below export-control and
electrical infrastructure thresholds, fiscal viability is determined by
hardware efficiency. H100-based scenarios achieve training feasibility at a
total cost of 8-14 million USD, while A100 deployments require 19-32 million
USD due to higher energy and hardware demand. We argue that extending training
timelines should be treated as a policy lever to mitigate hardware constraints,
enabling the production of usable, auditable, and locally aligned models
without competing at the global frontier. This study contributes to the
discourse on AI compute governance and technological sovereignty by
highlighting context-sensitive strategies that allow middle-income countries to
establish sustainable and strategically sufficient AI capabilities.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid escalation of computational requirements for training large-scale language models has reinforced structural asymmetries between high-capacity jurisdictions and countries in the Global South.</div>
</details>
</div>
<div class="card">
<div class="title">Blackbox Model Provenance via Palimpsestic Membership Inference</div>
<div class="meta-line">Authors: Rohith Kuditipudi, Jing Huang, Sally Zhu, Diyi Yang, Christopher Potts, Percy Liang</div>
<div class="meta-line">First: 2025-10-22T17:30:39+00:00 · Latest: 2025-10-22T17:30:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19796v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19796v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Suppose Alice trains an open-weight language model and Bob uses a blackbox
derivative of Alice&#x27;s model to produce text. Can Alice prove that Bob is using
her model, either by querying Bob&#x27;s derivative model (query setting) or from
the text alone (observational setting)? We formulate this question as an
independence testing problem--in which the null hypothesis is that Bob&#x27;s model
or text is independent of Alice&#x27;s randomized training run--and investigate it
through the lens of palimpsestic memorization in language models: models are
more likely to memorize data seen later in training, so we can test whether Bob
is using Alice&#x27;s model using test statistics that capture correlation between
Bob&#x27;s model or text and the ordering of training examples in Alice&#x27;s training
run. If Alice has randomly shuffled her training data, then any significant
correlation amounts to exactly quantifiable statistical evidence against the
null hypothesis, regardless of the composition of Alice&#x27;s training data. In the
query setting, we directly estimate (via prompting) the likelihood Bob&#x27;s model
gives to Alice&#x27;s training examples and order; we correlate the likelihoods of
over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to
12B parameters with the base model&#x27;s training data order, achieving a p-value
on the order of at most 1e-8 in all but six cases. In the observational
setting, we try two approaches based on estimating 1) the likelihood of Bob&#x27;s
text overlapping with spans of Alice&#x27;s training examples and 2) the likelihood
of Bob&#x27;s text with respect to different versions of Alice&#x27;s model we obtain by
repeating the last phase (e.g., 1%) of her training run on reshuffled data. The
second approach can reliably distinguish Bob&#x27;s text from as little as a few
hundred tokens; the first does not involve any retraining but requires many
more tokens (several hundred thousand) to achieve high power.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Suppose Alice trains an open-weight language model and Bob uses a blackbox derivative of Alice&#x27;s model to produce text.</div>
</details>
</div>
<div class="card">
<div class="title">AdaSPEC: Selective Knowledge Distillation for Efficient Speculative   Decoders</div>
<div class="meta-line">Authors: Yuezhou Hu, Jiaxin Guo, Xinyu Feng, Tuo Zhao</div>
<div class="meta-line">First: 2025-10-22T17:13:00+00:00 · Latest: 2025-10-22T17:13:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19779v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19779v1">PDF</a> · <a href="https://github.com/yuezhouhu/adaspec">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speculative Decoding (SD) accelerates large language model inference by
employing a small draft model to generate predictions, which are then verified
by a larger target model. The effectiveness of SD hinges on the alignment
between these models, which is typically enhanced by Knowledge Distillation
(KD). However, conventional KD methods aim to minimize the KL divergence
between the draft and target models across all tokens, a goal that is
misaligned with the true objective of SD, which is to maximize token acceptance
rate. Therefore, draft models often struggle to fully assimilate the target
model&#x27;s knowledge due to capacity constraints, leading to suboptimal
performance. To address this challenge, we propose AdaSPEC, a novel method that
incorporates selective token filtering into the KD process. AdaSPEC utilizes a
reference model to identify and filter out difficult-to-fit tokens, enabling
the distillation of a draft model that better aligns with the target model on
simpler tokens. This approach improves the overall token acceptance rate
without compromising generation quality. We evaluate AdaSPEC across diverse
tasks, including arithmetic reasoning, instruction-following, coding, and
summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.
Our results demonstrate that AdaSPEC consistently outperforms the
state-of-the-art DistillSpec method, achieving higher acceptance rates across
all tasks (up to 15\%). The code is publicly available at
https://github.com/yuezhouhu/adaspec.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model.</div>
</details>
</div>
<div class="card">
<div class="title">SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via   Promoting Deeper Thought Exploration</div>
<div class="meta-line">Authors: Xichen Zhang, Sitong Wu, Haoru Tan, Shaozuo Yu, Yinghao Zhu, Ziyi He, Jiaya Jia</div>
<div class="meta-line">First: 2025-10-22T16:56:01+00:00 · Latest: 2025-10-22T16:56:01+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/dvlab-research/SmartSwitch</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19767v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19767v1">PDF</a> · <a href="https://github.com/dvlab-research/SmartSwitch">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The long chain-of-thought (LongCoT) capability is central to the recent
breakthroughs achieved by large language models in complex reasoning tasks.
However, the accompanying issue of &#x27;&#x27;underthinking&#x27;&#x27;, where models exhibit
shallow reasoning by frequently switching thoughts without sufficient
exploration, limits both performance and token efficiency. To address this
problem, we propose a simple yet effective reasoning strategy: the SmartSwitch
inference framework. This framework can be easily integrated into any large
language model as a plug-and-play solution, continuously monitoring the model&#x27;s
reasoning process to detect underthinking and guide it toward deeper
exploration of promising but overlooked thoughts. Specifically, the perception
module identifies points where thoughts switch and evaluates the potential of
the preceding thought using an off-the-shelf process reward model (PRM). If a
high-potential thought is found to be prematurely abandoned, the intervention
module interrupts the ongoing inference, backtracks to the point before the
switch, and inserts a &quot;deepening prompt&quot; to encourage further exploration along
that promising path. Extensive experiments on challenging mathematical
reasoning benchmarks demonstrate that our method significantly enhances the
performance of various large language models of different sizes.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The long chain-of-thought (LongCoT) capability is central to the recent breakthroughs achieved by large language models in complex reasoning tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Masked and Unmasked: Discrete Diffusion Models via Partial   Masking</div>
<div class="meta-line">Authors: Chen-Hao Chao, Wei-Fang Sun, Hanwen Liang, Chun-Yi Lee, Rahul G. Krishnan</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-24T04:16:40+00:00 · Latest: 2025-10-22T16:25:33+00:00</div>
<div class="meta-line">Comments: Published at NeurIPS 2025. Project Page:
  https://chen-hao-chao.github.io/mdm-prime</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.18495v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.18495v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://chen-hao-chao.github.io/mdm-prime">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked diffusion models (MDM) are powerful generative models for discrete
data that generate samples by progressively unmasking tokens in a sequence.
Each token can take one of two states: masked or unmasked. We observe that
token sequences often remain unchanged between consecutive sampling steps;
consequently, the model repeatedly processes identical inputs, leading to
redundant computation. To address this inefficiency, we propose the Partial
masking scheme (Prime), which augments MDM by allowing tokens to take
intermediate states interpolated between the masked and unmasked states. This
design enables the model to make predictions based on partially observed token
information, and facilitates a fine-grained denoising process. We derive a
variational training objective and introduce a simple architectural design to
accommodate intermediate-state inputs. Our method demonstrates superior
performance across a diverse set of generative modeling tasks. On text data, it
achieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM
(21.52), autoregressive models (17.54), and their hybrid variants (17.58),
without relying on an autoregressive formulation. On image data, it attains
competitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable
to leading continuous generative models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Masked diffusion models (MDM) are powerful generative models for discrete data that generate samples by progressively unmasking tokens in a sequence.</div>
</details>
</div>
<div class="card">
<div class="title">Memo: Training Memory-Efficient Embodied Agents with Reinforcement   Learning</div>
<div class="meta-line">Authors: Gunshi Gupta, Karmesh Yadav, Zsolt Kira, Yarin Gal, Rahaf Aljundi</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-22T16:24:47+00:00 · Latest: 2025-10-22T16:24:47+00:00</div>
<div class="meta-line">Comments: Accepted for Spotlight Presentation at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19732v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19732v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To enable embodied agents to operate effectively over extended timeframes, it
is crucial to develop models that form and access memories to stay
contextualized in their environment. In the current paradigm of training
transformer-based policies for embodied sequential decision-making tasks,
visual inputs often overwhelm the context limits of transformers, while humans
can maintain and utilize a lifetime of experience compressed as memories.
Significant compression is possible in principle, as much of the input is
irrelevant and can be abstracted. However, existing approaches predominantly
focus on either recurrent models with fixed-size memory or transformers with
full-context reliance. In this work, we propose Memo, a transformer-based
architecture and training recipe for reinforcement learning (RL) on
memory-intensive, long-horizon tasks. Memo incorporates the creation and
retrieval of memory by interleaving periodic summarization tokens with the
inputs of a model during training. We demonstrate Memo&#x27;s effectiveness on a
gridworld meta-RL benchmark and a multi-object navigation task in
photo-realistic indoor settings. Memo outperforms naive long-context
transformer baselines while being more compute and storage efficient.
Additionally, Memo generalizes better to longer contexts at inference time and
remains robust in streaming settings, where historical context must be
truncated to fit inference constraints.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment.</div>
</details>
</div>
<div class="card">
<div class="title">The Coverage Principle: How Pre-Training Enables Post-Training</div>
<div class="meta-line">Authors: Fan Chen, Audrey Huang, Noah Golowich, Sadhika Malladi, Adam Block, Jordan T. Ash, Akshay Krishnamurthy, Dylan J. Foster</div>
<div class="meta-line">First: 2025-10-16T17:53:50+00:00 · Latest: 2025-10-22T16:15:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.15020v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.15020v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models demonstrate remarkable abilities when pre-trained on large
text corpora and fine-tuned for specific tasks, but how and why pre-training
shapes the success of the final model remains poorly understood. Notably,
although pre-training success is often quantified by cross-entropy loss,
cross-entropy can be a poor predictor of downstream performance. Instead, we
provide a theoretical perspective on this relationship through the lens of
\emph{coverage}, which quantifies the probability mass the pre-trained model
places on high-quality responses and which is necessary and sufficient for
post-training and test-time scaling methods such as Best-of-N to succeed. Our
main results develop an understanding of \emph{the coverage principle}, a
phenomenon whereby next-token prediction (more generally, maximum likelihood)
implicitly optimizes toward a model with good coverage. In particular, we
uncover a mechanism that explains the power of coverage in predicting
downstream performance: \emph{coverage generalizes faster than cross-entropy},
avoiding spurious dependence on problem-dependent parameters such as the
sequence length. We also study practical algorithmic interventions with
provable benefits for improving coverage, including (i) model/checkpoint
selection procedures, (ii) gradient normalization schemes, and (iii) test-time
decoding strategies.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Language models demonstrate remarkable abilities when pre-trained on large text corpora and fine-tuned for specific tasks, but how and why pre-training shapes the success of the final model remains poorly understood.</div>
</details>
</div>
<div class="card">
<div class="title">GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks</div>
<div class="meta-line">Authors: Varvara Krechetova, Denis Kochedykov</div>
<div class="meta-line">First: 2025-03-23T16:20:14+00:00 · Latest: 2025-10-22T16:12:30+00:00</div>
<div class="meta-line">Comments: Github with code and benchmark set:
  https://github.com/Solirinai/GeoBenchX</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2503.18129v2">Abs</a> · <a href="http://arxiv.org/pdf/2503.18129v2">PDF</a> · <a href="https://github.com/Solirinai/GeoBenchX">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper establishes a benchmark for evaluating tool-calling capabilities
of large language models (LLMs) on multi-step geospatial tasks relevant to
commercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet
3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o,
GPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23
geospatial functions. Our benchmark comprises tasks in four categories of
increasing complexity, with both solvable and intentionally unsolvable tasks to
test rejection accuracy. We develop a LLM-as-Judge evaluation framework to
compare agent solutions against reference solutions. Results show o4-mini and
Claude 3.5 Sonnet achieve the best overall performance, OpenAI&#x27;s GPT-4.1,
GPT-4o and Google&#x27;s Gemini 2.5 Pro Preview do not fall far behind, but the last
two are more efficient in identifying unsolvable tasks. Claude Sonnet 4, due
its preference to provide any solution rather than reject a task, proved to be
less accurate. We observe significant differences in token usage, with
Anthropic models consuming more tokens than competitors. Common errors include
misunderstanding geometrical relationships, relying on outdated knowledge, and
inefficient data manipulation. The resulting benchmark set, evaluation
framework, and data generation pipeline are released as open-source resources
(available at https://github.com/Solirinai/GeoBenchX), providing one more
standardized method for the ongoing evaluation of LLMs for GeoAI.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper establishes a benchmark for evaluating tool-calling capabilities of large language models (LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners.</div>
</details>
</div>
<div class="card">
<div class="title">Base Models Know How to Reason, Thinking Models Learn When</div>
<div class="meta-line">Authors: Constantin Venhoff, Iván Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-08T17:58:28+00:00 · Latest: 2025-10-22T16:02:22+00:00</div>
<div class="meta-line">Comments: 10 pages, Accepted to the Mechanistic Interpretability Workshop at
  NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.07364v3">Abs</a> · <a href="http://arxiv.org/pdf/2510.07364v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Why do thinking language models like DeepSeek R1 outperform their base
counterparts? Despite consistent performance gains, it remains unclear to what
extent thinking models learn entirely new reasoning capabilities or repurpose
pre-existing base model ones. In this work, we propose a hybrid model where we
activate reasoning mechanisms in base models at the right time to elicit
thinking-model-level reasoning chains, implying that thinking models exploit
already existing capabilities. To ground our analysis, we introduce an
unsupervised, bottom-up approach for uncovering human-interpretable reasoning
behaviors in thinking models. This approach provides an unbiased method to
discover reasoning behaviors without imposing manual or LLM-derived
assumptions. Across three base and four thinking models, using GSM8K and
MATH500, our hybrid model recovers up to 91% of the performance gap to thinking
models without any weight updates while steering only 12% of tokens.
Concretely, our empirical setup provides a simple, causal way to test the
effectiveness of existing reasoning mechanisms in base models by invoking them
directly and measuring the resulting task performance. More broadly, these
results reframe our understanding of how thinking models are trained:
pre-training is when models acquire most of their reasoning mechanisms, and
post-training teaches efficient deployment of these mechanisms at the right
time, enabling efficient use of their inference-time compute.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Why do thinking language models like DeepSeek R1 outperform their base counterparts?</div>
</details>
</div>
<div class="card">
<div class="title">SEMPO: Lightweight Foundation Models for Time Series Forecasting</div>
<div class="meta-line">Authors: Hui He, Kun Yi, Yuanchi Ma, Qi Zhang, Zhendong Niu, Guansong Pang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-22T15:58:44+00:00 · Latest: 2025-10-22T15:58:44+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19710v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19710v1">PDF</a> · <a href="https://github.com/mala-lab/SEMPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent boom of large pre-trained models witnesses remarkable success in
developing foundation models (FMs) for time series forecasting. Despite
impressive performance across diverse downstream forecasting tasks, existing
time series FMs possess massive network architectures and require substantial
pre-training on large-scale datasets, which significantly hinders their
deployment in resource-constrained environments. In response to this growing
tension between versatility and affordability, we propose SEMPO, a novel
lightweight foundation model that requires pretraining on relatively
small-scale data, yet exhibits strong general time series forecasting.
Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctral
decomposition module, that substantially improves the utilization of
pre-training data by modeling not only the high-energy frequency signals but
also the low-energy yet informative frequency signals that are ignored in
current methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns
heterogeneous temporal patterns through small dataset-specific prompts and
adaptively routes time series tokens to prompt-based experts for
parameter-efficient model adaptation across different datasets and domains.
Equipped with these modules, SEMPO significantly reduces both pre-training data
scale and model size, while achieving strong generalization. Extensive
experiments on two large-scale benchmarks covering 16 datasets demonstrate the
superior performance of SEMPO in both zero-shot and few-shot forecasting
scenarios compared with state-of-the-art methods. Code and data are available
at https://github.com/mala-lab/SEMPO.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The recent boom of large pre-trained models witnesses remarkable success in developing foundation models (FMs) for time series forecasting.</div>
</details>
</div>
<div class="card">
<div class="title">Fast Inference via Hierarchical Speculative Decoding</div>
<div class="meta-line">Authors: Amir Globerson, Haim Kaplan, Yishay Mansour, Clara Mohri, Tal Schuster</div>
<div class="meta-line">First: 2025-10-22T15:56:19+00:00 · Latest: 2025-10-22T15:56:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19705v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer language models generate text autoregressively, making inference
latency proportional to the number of tokens generated. Speculative decoding
reduces this latency without sacrificing output quality, by leveraging a small
draft model to propose tokens that the larger target model verifies in
parallel. In practice, however, there may exist a set of potential draft
models- ranging from faster but less inaccurate, to slower yet more reliable.
We introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacks
these draft models into a hierarchy, where each model proposes tokens, and the
next larger model verifies them in a single forward pass, until finally the
target model verifies tokens. We derive an expression for the expected latency
of any such hierarchy and show that selecting the latency-optimal hierarchy can
be done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over the
best single-draft baseline, demonstrating the practicality of our algorithm in
reducing generation latency beyond previous techniques.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformer language models generate text autoregressively, making inference latency proportional to the number of tokens generated.</div>
</details>
</div>
<div class="card">
<div class="title">Masked Generative Priors Improve World Models Sequence Modelling   Capabilities</div>
<div class="meta-line">Authors: Cristian Meo, Mircea Lica, Zarif Ikram, Akihiro Nakano, Vedant Shah, Aniket Rajiv Didolkar, Dianbo Liu, Anirudh Goyal, Justin Dauwels</div>
<div class="meta-line">First: 2024-10-10T11:52:07+00:00 · Latest: 2025-10-22T15:16:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2410.07836v6">Abs</a> · <a href="http://arxiv.org/pdf/2410.07836v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (RL) has become the leading approach for creating
artificial agents in complex environments. Model-based approaches, which are RL
methods with world models that predict environment dynamics, are among the most
promising directions for improving data efficiency, forming a critical step
toward bridging the gap between research and real-world deployment. In
particular, world models enhance sample efficiency by learning in imagination,
which involves training a generative sequence model of the environment in a
self-supervised manner. Recently, Masked Generative Modelling has emerged as a
more efficient and superior inductive bias for modelling and generating token
sequences. Building on the Efficient Stochastic Transformer-based World Models
(STORM) architecture, we replace the traditional MLP prior with a Masked
Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our
model on two downstream tasks: reinforcement learning and video prediction.
GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari
100k benchmark. Moreover, we apply Transformer-based World Models to continuous
action environments for the first time, addressing a significant gap in prior
research. To achieve this, we employ a state mixer function that integrates
latent state representations with actions, enabling our model to handle
continuous control tasks. We validate this approach through qualitative and
quantitative analyses on the DeepMind Control Suite, showcasing the
effectiveness of Transformer-based World Models in this new domain. Our results
highlight the versatility and efficacy of the MaskGIT dynamics prior, paving
the way for more accurate world models and effective RL policies.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deep Reinforcement Learning (RL) has become the leading approach for creating artificial agents in complex environments.</div>
</details>
</div>
<div class="card">
<div class="title">BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level   Autoregression</div>
<div class="meta-line">Authors: Cristian Meo, Varun Sarathchandran, Avijit Majhi, Shao Hung, Carlo Saccardi, Ruben Imhoff, Roberto Deidda, Remko Uijlenhoet, Justin Dauwels</div>
<div class="meta-line">First: 2025-10-07T11:52:32+00:00 · Latest: 2025-10-22T15:14:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.06293v2">Abs</a> · <a href="http://arxiv.org/pdf/2510.06293v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Predicting precipitation maps is a highly complex spatiotemporal modeling
task, critical for mitigating the impacts of extreme weather events. Short-term
precipitation forecasting, or nowcasting, requires models that are not only
accurate but also computationally efficient for real-time applications. Current
methods, such as token-based autoregressive models, often suffer from flawed
inductive biases and slow inference, while diffusion models can be
computationally intensive. To address these limitations, we introduce BlockGPT,
a generative autoregressive transformer using batched tokenization (Block)
method that predicts full two-dimensional fields (frames) at each time step.
Conceived as a model-agnostic paradigm for video prediction, BlockGPT
factorizes space-time by using self-attention within each frame and causal
attention across frames; in this work, we instantiate it for precipitation
nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI
(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines
including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)
models. The results show that BlockGPT achieves superior accuracy, event
localization as measured by categorical metrics, and inference speeds up to 31x
faster than comparable baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Predicting precipitation maps is a highly complex spatiotemporal modeling task, critical for mitigating the impacts of extreme weather events.</div>
</details>
</div>
<div class="card">
<div class="title">From Forecasting to Planning: Policy World Model for Collaborative   State-Action Prediction</div>
<div class="meta-line">Authors: Zhida Zhao, Talas Fu, Yifan Wang, Lijun Wang, Huchuan Lu</div>
<div class="meta-line">First: 2025-10-22T14:57:51+00:00 · Latest: 2025-10-22T14:57:51+00:00</div>
<div class="meta-line">Comments: Accepted by NuerIPS 2025 (Poster)</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19654v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19654v1">PDF</a> · <a href="https://github.com/6550Zhao/Policy-World-Model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite remarkable progress in driving world models, their potential for
autonomous systems remains largely untapped: the world models are mostly
learned for world simulation and decoupled from trajectory planning. While
recent efforts aim to unify world modeling and planning in a single framework,
the synergistic facilitation mechanism of world modeling for planning still
requires further exploration. In this work, we introduce a new driving paradigm
named Policy World Model (PWM), which not only integrates world modeling and
trajectory planning within a unified architecture, but is also able to benefit
planning using the learned world knowledge through the proposed action-free
future state forecasting scheme. Through collaborative state-action prediction,
PWM can mimic the human-like anticipatory perception, yielding more reliable
planning performance. To facilitate the efficiency of video forecasting, we
further introduce a dynamically enhanced parallel token generation mechanism,
equipped with a context-guided tokenizer and an adaptive dynamic focal loss.
Despite utilizing only front camera input, our method matches or exceeds
state-of-the-art approaches that rely on multi-view and multi-modal inputs.
Code and model weights will be released at
https://github.com/6550Zhao/Policy-World-Model.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite remarkable progress in driving world models, their potential for autonomous systems remains largely untapped: the world models are mostly learned for world simulation and decoupled from trajectory planning.</div>
</details>
</div>
<div class="card">
<div class="title">Style Attack Disguise: When Fonts Become a Camouflage for Adversarial   Intent</div>
<div class="meta-line">Authors: Yangshijie Zhang, Xinda Wang, Jialin Liu, Wenqiang Wang, Zhicong Ma, Xingxing Jia</div>
<div class="meta-line">First: 2025-10-22T14:40:24+00:00 · Latest: 2025-10-22T14:40:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19641v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19641v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With social media growth, users employ stylistic fonts and font-like emoji to
express individuality, creating visually appealing text that remains
human-readable. However, these fonts introduce hidden vulnerabilities in NLP
models: while humans easily read stylistic text, models process these
characters as distinct tokens, causing interference. We identify this
human-model perception gap and propose a style-based attack, Style Attack
Disguise (SAD). We design two sizes: light for query efficiency and strong for
superior attack performance. Experiments on sentiment classification and
machine translation across traditional models, LLMs, and commercial services
demonstrate SAD&#x27;s strong attack performance. We also show SAD&#x27;s potential
threats to multimodal tasks including text-to-image and text-to-speech
generation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With social media growth, users employ stylistic fonts and font-like emoji to express individuality, creating visually appealing text that remains human-readable.</div>
</details>
</div>
<div class="card">
<div class="title">dInfer: An Efficient Inference Framework for Diffusion Language Models</div>
<div class="meta-line">Authors: Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, Xinyuan Zhang, Zhen Tao, Haibo Feng, Ziyun Jiang, Ying Xu, Zenan Huang, Yihong Zhuang, Haokai Xu, Jiaqi Hu, Zhenzhong Lan, Junbo Zhao, Jianguo Li, Da Zheng</div>
<div class="meta-line">First: 2025-10-09T16:19:42+00:00 · Latest: 2025-10-22T14:33:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.08666v3">Abs</a> · <a href="http://arxiv.org/pdf/2510.08666v3">PDF</a> · <a href="https://github.com/inclusionAI/dInfer">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based large language models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs, leveraging denoising-based generation
to enable inherent parallelism. Even more and more open-sourced dLLM models
emerge, yet their widespread adoption remains constrained by the lack of a
standardized and efficient inference framework. We present dInfer, an efficient
and extensible framework for dLLM inference. dInfer decomposes the inference
pipeline into four modular components--model, diffusion iteration manager,
decoding strategy, and KV-cache manager--and integrates novel algorithms for
each component alongside system-level optimizations. Through this combination
of algorithmic innovations and system enhancements, dInfer achieves substantial
efficiency gains without compromising output quality on LLaDA-MoE. At batch
size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800
tokens per second across six benchmarks on $8\times$ H800 GPUs. Compared to
prior systems, dInfer delivers a $10\times$ speedup over Fast-dLLM while
maintaining similar model performance. Even compared to the AR model (with a
comparable number of activation parameters and performance) QWen2.5-3B, which
is highly optimized with the latest vLLM inference engine, dInfer still
delivers a $2$-$3\times$ speedup. The implementation of dInfer is open-sourced
at https://github.com/inclusionAI/dInfer.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism.</div>
</details>
</div>
<div class="card">
<div class="title">Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision   Geometry Priors</div>
<div class="meta-line">Authors: Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-30T14:16:41+00:00 · Latest: 2025-10-22T14:04:01+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.24625v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.24625v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Previous research has investigated the application of Multimodal Large
Language Models (MLLMs) in understanding 3D scenes by interpreting them as
videos. These approaches generally depend on comprehensive 3D data inputs, such
as point clouds or reconstructed Bird&#x27;s-Eye View (BEV) maps. In our research,
we advance this field by enhancing the capability of MLLMs to understand and
reason in 3D spaces directly from video data, without the need for additional
3D input. We propose a novel and efficient method called the Video-3D Geometry
Large Language Model (VG LLM). Our approach utilizes a 3D visual geometry
encoder to extract 3D prior information from video sequences. This information
is then integrated with visual tokens and input into the MLLM. Extensive
experiments have shown that our method has achieved substantial improvements in
various tasks related to 3D scene understanding and spatial reasoning, all
directly learned from video sources. Impressively, our 4B model, which does not
rely on explicit 3D data inputs, achieves competitive results compared to
existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the
VSI-Bench evaluations.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos.</div>
</details>
</div>
<div class="card">
<div class="title">Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning   Segmentation</div>
<div class="meta-line">Authors: Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2025-10-22T13:42:59+00:00 · Latest: 2025-10-22T13:42:59+00:00</div>
<div class="meta-line">Comments: Project page: https://www.jshyun.me/projects/decaf</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19592v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19592v1">PDF</a> · <a href="https://github.com/HYUNJS/DecAF">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://www.jshyun.me/projects/decaf">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) demonstrate strong video
understanding by attending to visual tokens relevant to textual queries. To
directly adapt this for localization in a training-free manner, we cast video
reasoning segmentation as a video QA task and extract attention maps via
rollout mechanism. However, raw attention maps are noisy and poorly aligned
with object regions. We propose Decomposed Attention Fusion (DecAF), which
refines these maps through two mechanisms: (1) contrastive object-background
fusion and (2) complementary video-frame fusion. This method suppresses
irrelevant activations and enhances object-focused cues, enabling direct
conversion of attention maps into coarse segmentation masks. In addition, we
introduce attention-guided SAM2 prompting for obtaining fine-grained masks.
Unlike existing methods that jointly train MLLMs with SAM, our method operates
entirely without retraining. DecAF outperforms training-free methods and
achieves performance comparable to training-based methods on both referring and
reasoning VOS benchmarks. The code will be available at
https://github.com/HYUNJS/DecAF.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) demonstrate strong video understanding by attending to visual tokens relevant to textual queries.</div>
</details>
</div>
<div class="card">
<div class="title">VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view   Driving Reconstruction</div>
<div class="meta-line">Authors: Junhong Lin, Kangli Wang, Shunzhou Wang, Songlin Fan, Ge Li, Wei Gao</div>
<div class="meta-line">First: 2025-10-22T13:28:49+00:00 · Latest: 2025-10-22T13:28:49+00:00</div>
<div class="meta-line">Comments: 10 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19578v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Feed-forward surround-view autonomous driving scene reconstruction offers
fast, generalizable inference ability, which faces the core challenge of
ensuring generalization while elevating novel view quality. Due to the
surround-view with minimal overlap regions, existing methods typically fail to
ensure geometric consistency and reconstruction quality for novel views. To
tackle this tension, we claim that geometric information must be learned
explicitly, and the resulting features should be leveraged to guide the
elevating of semantic quality in novel views. In this paper, we introduce
\textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end
learning framework designed to address this challenge. To achieve generalizable
geometric estimation, we design a lightweight variant of the VGGT architecture
to efficiently distill its geometric priors from the pre-trained VGGT to the
geometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale
geometry tokens to predict Gaussian parameters for novel view rendering, which
shares the same patch backbone as the geometry branch. Finally, we integrate
multi-scale features from both geometry and Gaussian head branches to jointly
supervise a semantic refinement model, optimizing rendering quality through
feature-consistent learning. Experiments on nuScenes demonstrate that our
approach significantly outperforms state-of-the-art methods in both objective
metrics and subjective quality under various settings, which validates VGD&#x27;s
scalability and high-fidelity surround-view reconstruction.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Feed-forward surround-view autonomous driving scene reconstruction offers fast, generalizable inference ability, which faces the core challenge of ensuring generalization while elevating novel view quality.</div>
</details>
</div>
<div class="card">
<div class="title">CARES: Context-Aware Resolution Selector for VLMs</div>
<div class="meta-line">Authors: Moshe Kimhi, Nimrod Shabtay, Raja Giryes, Chaim Baskin, Eli Schwartz</div>
<div class="meta-line">First: 2025-10-22T11:44:31+00:00 · Latest: 2025-10-22T11:44:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19496v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19496v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) commonly process images at native or high
resolution to remain effective across tasks. This inflates visual tokens ofter
to 97-99% of total tokens, resulting in high compute and latency, even when
low-resolution images would suffice. We introduce \emph{CARES}-a
\textbf{C}ontext-\textbf{A}ware \textbf{R}esolution \textbf{S}elector, a
lightweight preprocessing module that, given an image-query pair, predicts the
\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to
extract features and predict when a target pretrained VLM&#x27;s response converges
to its peak ability to answer correctly. Though trained as a discrete
classifier over a set of optional resolutions, CARES interpolates continuous
resolutions at inference for fine-grained control. Across five multimodal
benchmarks spanning documents and natural images, as well as diverse target
VLMs, CARES preserves task performance while reducing compute by up to 80%.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large vision-language models (VLMs) commonly process images at native or high resolution to remain effective across tasks.</div>
</details>
</div>
<div class="card">
<div class="title">ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language   Models on Edge Devices</div>
<div class="meta-line">Authors: Xin Nie, Liang Dong, HaiCheng Zhang, JiaWang Xiao, G. Sun</div>
<div class="meta-line">First: 2025-10-22T11:20:47+00:00 · Latest: 2025-10-22T11:20:47+00:00</div>
<div class="meta-line">Comments: 19 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19482v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The deployment of Large Language Models (LLMs) on CPU-based edge devices is
crucial for enabling on-device intelligence and expanding AI accessibility.
However, it remains challenging due to limited memory and computational
resources. During edge inference, memory usage and latency are the primary
bottlenecks. Although weight quantization can effectively reduce memory
consumption, existing hardware-friendly approaches often rely on uniform
quantization, which poorly fits weight distributions and incurs high
dequantization overhead at low bit widths. To address these limitations, we
propose ELUTQ, an efficient quantization framework introducing a novel
quantization format, Hierarchical Linear Quantization (HLQ). HLQ better
captures the statistical characteristics of weights without increasing the
computational cost of Bit-serial LUT-based GEMM operations, thereby eliminating
dequantization overhead. It is orthogonal to existing quantization algorithms
and can be seamlessly integrated into various quantization pipelines. For
efficient on-device deployment, ELUTQ provides optimized CPU kernels for
end-to-end inference. Experiments show that for LLaMA3-8B, HLQ reduces
perplexity by about 8% at 3-bit and 85% at 2-bit precision under post-training
quantization, completing quantization within one hour. With efficient
finetuning, HLQ further improves 2-bit performance within two hours. In terms
of inference efficiency, our 2-bit LLaMA2-7B achieves over 25 tokens/s on an
Apple M2 chip (4 threads, batch size = 1).</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The deployment of Large Language Models (LLMs) on CPU-based edge devices is crucial for enabling on-device intelligence and expanding AI accessibility.</div>
</details>
</div>
<div class="card">
<div class="title">REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion   Transformers</div>
<div class="meta-line">Authors: Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, Liang Zheng</div>
<div class="meta-line">First: 2025-04-14T17:59:53+00:00 · Latest: 2025-10-22T10:52:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2504.10483v3">Abs</a> · <a href="http://arxiv.org/pdf/2504.10483v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://end2end-diffusion.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper we tackle a fundamental question: &quot;Can we train latent
diffusion models together with the variational auto-encoder (VAE) tokenizer in
an end-to-end manner?&quot; Traditional deep-learning wisdom dictates that
end-to-end training is often preferable when possible. However, for latent
diffusion transformers, it is observed that end-to-end training both VAE and
diffusion-model using standard diffusion-loss is ineffective, even causing a
degradation in final performance. We show that while diffusion loss is
ineffective, end-to-end training can be unlocked through the
representation-alignment (REPA) loss -- allowing both VAE and diffusion model
to be jointly tuned during the training process. Despite its simplicity, the
proposed training recipe (REPA-E) shows remarkable performance; speeding up
diffusion model training by over 17x and 45x over REPA and vanilla training
recipes, respectively. Interestingly, we observe that end-to-end tuning with
REPA-E also improves the VAE itself; leading to improved latent space structure
and downstream generation performance. In terms of final performance, our
approach sets a new state-of-the-art; achieving FID of 1.12 and 1.69 with and
without classifier-free guidance on ImageNet 256 x 256. Code is available at
https://end2end-diffusion.github.io.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper we tackle a fundamental question: &quot;Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?&quot; Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible.</div>
</details>
</div>
<div class="card">
<div class="title">LLM Unlearning with LLM Beliefs</div>
<div class="meta-line">Authors: Kemou Li, Qizhou Wang, Yue Wang, Fengpeng Li, Jun Liu, Bo Han, Jiantao Zhou</div>
<div class="meta-line">First: 2025-10-22T09:44:36+00:00 · Latest: 2025-10-22T09:44:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19422v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19422v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models trained on vast corpora inherently risk memorizing
sensitive or harmful content, which may later resurface in their outputs.
Prevailing unlearning methods generally rely on gradient ascent and its
variants to lower the probability of specific target responses. However, we
find that this strategy induces a critical side effect: probability mass is
redistributed into high-likelihood regions, often corresponding to semantically
related rephrasings of the targets. We refer to this as the squeezing effect,
which explains why many methods yield merely spurious unlearning, a problem
further obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport
actual success. To address this, we propose a bootstrapping (BS) framework that
explicitly links the squeezing effect with the model&#x27;s own high-confidence
generations, namely its model beliefs. Since model beliefs inherently capture
the very high-likelihood regions where probability mass is squeezed,
incorporating them into the unlearning objective directly counters the
squeezing effect. By jointly suppressing both target responses and model
beliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S
(sequence) removes entire high-confidence generations, together achieving more
thorough forgetting while preserving utility. Extensive experiments across
diverse benchmarks with various model families confirm the effectiveness of our
approach.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models trained on vast corpora inherently risk memorizing sensitive or harmful content, which may later resurface in their outputs.</div>
</details>
</div>
<div class="card">
<div class="title">Spiking Neural Networks Need High Frequency Information</div>
<div class="meta-line">Authors: Yuetong Fang, Deming Zhou, Ziqing Wang, Hongwei Ren, ZeCui Zeng, Lusong Li, Shibo Zhou, Renjing Xu</div>
<div class="meta-line">First: 2025-05-24T09:15:59+00:00 · Latest: 2025-10-22T09:17:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.18608v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.18608v3">PDF</a> · <a href="https://github.com/bic-L/MaxFormer">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spiking Neural Networks promise brain-inspired and energy-efficient
computation by transmitting information through binary (0/1) spikes. Yet, their
performance still lags behind that of artificial neural networks, often assumed
to result from information loss caused by sparse and binary activations. In
this work, we challenge this long-standing assumption and reveal a previously
overlooked frequency bias: spiking neurons inherently suppress high-frequency
components and preferentially propagate low-frequency information. This
frequency-domain imbalance, we argue, is the root cause of degraded feature
representation in SNNs. Empirically, on Spiking Transformers, adopting
Avg-Pooling (low-pass) for token mixing lowers performance to 76.73% on
Cifar-100, whereas replacing it with Max-Pool (high-pass) pushes the top-1
accuracy to 79.12%. Accordingly, we introduce Max-Former that restores
high-frequency signals through two frequency-enhancing operators: (1) extra
Max-Pool in patch embedding, and (2) Depth-Wise Convolution in place of
self-attention. Notably, Max-Former attains 82.39% top-1 accuracy on ImageNet
using only 63.99M parameters, surpassing Spikformer (74.81%, 66.34M) by +7.58%.
Extending our insight beyond transformers, our Max-ResNet-18 achieves
state-of-the-art performance on convolution-based benchmarks: 97.17% on
CIFAR-10 and 83.06\% on CIFAR-100. We hope this simple yet effective solution
inspires future research to explore the distinctive nature of spiking neural
networks. Code is available: https://github.com/bic-L/MaxFormer.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spiking Neural Networks promise brain-inspired and energy-efficient computation by transmitting information through binary (0/1) spikes.</div>
</details>
</div>
<div class="card">
<div class="title">AMAuT: A Flexible and Efficient Multiview Audio Transformer Framework   Trained from Scratch</div>
<div class="meta-line">Authors: Weichuang Shao, Iman Yi Liao, Tomas Henrique Bode Maul, Tissa Chandesa</div>
<div class="meta-line">First: 2025-10-22T08:41:59+00:00 · Latest: 2025-10-22T08:41:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19368v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19368v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent foundational models, SSAST, EAT, HuBERT, Qwen-Audio, and Audio
Flamingo, achieve top-tier results across standard audio benchmarks but are
limited by fixed input rates and durations, hindering their reusability. This
paper introduces the Augmentation-driven Multiview Audio Transformer (AMAuT), a
training-from-scratch framework that eliminates the dependency on pre-trained
weights while supporting arbitrary sample rates and audio lengths. AMAuT
integrates four key components: (1) augmentation-driven multiview learning for
robustness, (2) a conv1 + conv7 + conv1 one-dimensional CNN bottleneck for
stable temporal encoding, (3) dual CLS + TAL tokens for bidirectional context
representation, and (4) test-time adaptation/augmentation (TTA^2) to improve
inference reliability. Experiments on five public benchmarks, AudioMNIST,
SpeechCommands V1 &amp; V2, VocalSound, and CochlScene, show that AMAuT achieves
accuracies up to 99.8% while consuming less than 3% of the GPU hours required
by comparable pre-trained models. Thus, AMAuT presents a highly efficient and
flexible alternative to large pre-trained models, making state-of-the-art audio
classification accessible in computationally constrained settings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent foundational models, SSAST, EAT, HuBERT, Qwen-Audio, and Audio Flamingo, achieve top-tier results across standard audio benchmarks but are limited by fixed input rates and durations, hindering their reusability.</div>
</details>
</div>
<div class="card">
<div class="title">Visual Multi-Agent System: Mitigating Hallucination Snowballing via   Visual Flow</div>
<div class="meta-line">Authors: Xinlei Yu, Chengming Xu, Guibin Zhang, Yongbo He, Zhangquan Chen, Zhucun Xue, Jiangning Zhang, Yue Liao, Xiaobin Hu, Yu-Gang Jiang, Shuicheng Yan</div>
<div class="meta-line">First: 2025-09-26T02:43:24+00:00 · Latest: 2025-10-22T08:07:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2509.21789v2">Abs</a> · <a href="http://arxiv.org/pdf/2509.21789v2">PDF</a> · <a href="https://github.com/YU-deep/ViF.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables
challenging tasks but suffers from a novel failure term, multi-agent visual
hallucination snowballing, where hallucinations are seeded in a single agent
and amplified by following ones due to the over-reliance on textual flow to
relay visual information. Through turn-, layer-, and token-wise attention
analyses, we provide detailed insights into the essence of hallucination
snowballing regarding the reduction of visual attention allocation. It leads us
to identify a subset of vision tokens with a unimodal attention peak in middle
layers that best preserve visual evidence but gradually diminish in deeper
agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we
propose ViF, a lightweight, plug-and-play mitigation paradigm that relays
inter-agent messages with Visual Flow powered by the selected visual relay
tokens and applies attention reallocation to amplify this pattern. The
experiment results demonstrate that our method markedly reduces hallucination
snowballing, consistently improving the performance across eight benchmarks
based on four common MAS structures and ten base models. The source code is
publicly available at: https://github.com/YU-deep/ViF.git.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in a single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information.</div>
</details>
</div>
<div class="card">
<div class="title">Data Efficient Any Transformer-to-Mamba Distillation via Attention   Bridge</div>
<div class="meta-line">Authors: Penghao Wang, Yuhao Zhou, Mengxuan Wu, Panpan Zhang, Zhangyang Wang, Kai Wang</div>
<div class="meta-line">First: 2025-10-22T05:56:14+00:00 · Latest: 2025-10-22T05:56:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19266v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19266v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-space models (SSMs) have emerged as efficient alternatives to
Transformers for sequence modeling, offering superior scalability through
recurrent structures. However, their training remains costly and the ecosystem
around them is far less mature than that of Transformers. Moreover, the
structural heterogeneity between SSMs and Transformers makes it challenging to
efficiently distill knowledge from pretrained attention models. In this work,
we propose Cross-architecture distillation via Attention Bridge (CAB), a novel
data-efficient distillation framework that efficiently transfers attention
knowledge from Transformer teachers to state-space student models. Unlike
conventional knowledge distillation that transfers knowledge only at the output
level, CAB enables token-level supervision via a lightweight bridge and
flexible layer-wise alignment, improving both efficiency and transferability.
We further introduce flexible layer-wise alignment strategies to accommodate
architectural discrepancies between teacher and student. Extensive experiments
across vision and language domains demonstrate that our method consistently
improves the performance of state-space models, even under limited training
data, outperforming both standard and cross-architecture distillation methods.
Our findings suggest that attention-based knowledge can be efficiently
transferred to recurrent models, enabling rapid utilization of Transformer
expertise for building a stronger SSM community.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">State-space models (SSMs) have emerged as efficient alternatives to Transformers for sequence modeling, offering superior scalability through recurrent structures.</div>
</details>
</div>
<div class="card">
<div class="title">Mixing Configurations for Downstream Prediction</div>
<div class="meta-line">Authors: Juntang Wang, Hao Wu, Runkun Guo, Yihan Wang, Dongmian Zou, Shixin Xu</div>
<div class="meta-line">First: 2025-10-22T05:09:53+00:00 · Latest: 2025-10-22T05:09:53+00:00</div>
<div class="meta-line">Comments: 16 pages,13 figures, conference paper. Equal contribution: Juntang
  Wang and Hao Wu</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.19248v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.19248v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans possess an innate ability to group objects by similarity, a cognitive
mechanism that clustering algorithms aim to emulate. Recent advances in
community detection have enabled the discovery of configurations -- valid
hierarchical clusterings across multiple resolution scales -- without requiring
labeled data. In this paper, we formally characterize these configurations and
identify similar emergent structures in register tokens within Vision
Transformers. Unlike register tokens, configurations exhibit lower redundancy
and eliminate the need for ad hoc selection. They can be learned through
unsupervised or self-supervised methods, yet their selection or composition
remains specific to the downstream task and input. Building on these insights,
we introduce GraMixC, a plug-and-play module that extracts configurations,
aligns them using our Reverse Merge/Split (RMS) technique, and fuses them via
attention heads before forwarding them to any downstream predictor. On the DSN1
16S rRNA cultivation-media prediction task, GraMixC improves the R2 score from
0.6 to 0.9 across multiple methods, setting a new state of the art. We further
validate GraMixC on standard tabular benchmarks, where it consistently
outperforms single-resolution and static-feature baselines.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Humans possess an innate ability to group objects by similarity, a cognitive mechanism that clustering algorithms aim to emulate.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251023_0314.html">20251023_0314</a>
<a href="archive/20251022_0317.html">20251022_0317</a>
<a href="archive/20251021_0314.html">20251021_0314</a>
<a href="archive/20251020_0313.html">20251020_0313</a>
<a href="archive/20251019_0312.html">20251019_0312</a>
<a href="archive/20251018_0314.html">20251018_0314</a>
<a href="archive/20251017_0312.html">20251017_0312</a>
<a href="archive/20251016_0313.html">20251016_0313</a>
<a href="archive/20251015_0313.html">20251015_0313</a>
<a href="archive/20251014_0314.html">20251014_0314</a>
<a href="archive/20251013_2035.html">20251013_2035</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
